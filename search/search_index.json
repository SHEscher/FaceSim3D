{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"General overview","text":""},{"location":"#facesim3d","title":"FaceSim3D \ud83d\uddff","text":"<p>Testing the effect of dynamic 3D viewing conditions on face similarity perception</p> <p> </p>"},{"location":"#project-description","title":"Project description","text":"<p>To test the effect of space and time on face similarity judgments, we conducted an online experiment using a triplet odd-one-out task in a static 2D and a dynamic 3D condition. We then trained sparse and deep computational encoding models of human face similarity judgments to investigate the latent representations that underlie their predictions.</p>"},{"location":"#how-to-navigate-this-project-documentation","title":"How to navigate this project documentation?","text":"<p>\ud83d\uddff : A brief overview of the experiment can be found in the section Experiment.</p> <p> : Results are summarized in the form of a Jupyter notebook in the section Results.</p> <p> : The research code is available as a Python package. For its setup and API reference, see the section Research Code.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this code or data, please cite the following paper (Hofmann et al. Human-aligned deep and sparse encoding models of dynamic 3D face similarity perception. PsyArXiv. 2024):</p> <pre><code>@article{hofmannHumanalignedDeepSparse2024,\n    title={Human-aligned deep and sparse encoding models of dynamic {3D} face similarity perception},\n    author={Hofmann, Simon M. and Ciston, Anthony and Koushik Abhay and Klotzsche, Felix and Hebart, Martin N. and M\u00fcller, Klaus-Robert and Villringer, Arno and Scherf, Nico and Hilsmann, Anna and Nikulin, Vadim V. and Gaebler, Michael},\n    journal={PsyArXiv},\n    doi={10.31234/osf.io/f62pw},\n    year={2024},\n}\n</code></pre>"},{"location":"#contributorscollaborators","title":"Contributors/Collaborators","text":"<p>Simon M. Hofmann*, Anthony Ciston, Abhay Koushik, Felix Klotzsche, Martin N. Hebart, Klaus-Robert M\u00fcller, Arno Villringer, Nico Scherf, Anna Hilsmann, Vadim V. Nikulin, Michael Gaebler</p> <p>This study was conducted at the Max Planck Institute for Human Cognitive and Brain Sciences as part of the NEUROHUM project.</p> <p></p> <p>* corresponding author</p>"},{"location":"code/","title":"Code overview","text":""},{"location":"code/#facesim3d-code","title":"FaceSim3D \u2013 code","text":"Period Status Author Feb, 2022 - Sep, 2024 <code>in process</code> Simon M. Hofmann"},{"location":"code/#analysis-steps","title":"Analysis steps","text":""},{"location":"code/#representational-similarity-analysis-rsa","title":"Representational Similarity Analysis (RSA)","text":"<p>RSA is applied on similarity matrices of face-pairs in both viewing conditions (2D, 3D) that were computed on the following associated data:</p>"},{"location":"code/#behavioral-judgments","title":"Behavioral judgments","text":"<p>Human judgments of face similarity from the triplet odd-one-out task.</p>"},{"location":"code/#vgg-face-activation-maps","title":"<code>VGG-Face</code> activation maps","text":"<p>Face images both in original and 3D-reconstructed form are fed into a pre-trained <code>VGG-Face</code> network. The resulting activation maps (of all layers) were used to compute similarity matrices.</p>"},{"location":"code/#cognitive-models","title":"Cognitive models","text":"<p>Modeling human similarity judgments in both viewing conditions (2D &amp; 3D). To this end, sparse and deep encoding models were used to predict human similarity judgments.</p>"},{"location":"code/#human-aligned-vgg-face-activation-maps","title":"Human-aligned <code>VGG-Face</code> activation maps","text":"<p>An adaptation of the deep neural network VGG-Face was aligned to human choices in the face similarity judgment task. Also, for this embedding model, similarity matrices were computed based on feature maps of the model.</p>"},{"location":"code/#sparse-embedding-models-spose-vice","title":"Sparse embedding models: SPoSE &amp; VICE","text":"<p>Sparse models were developed for modeling human similarity judgments in similarity tasks, see Hebart et al. (2020), and Muttenthaler el al. (2022) for details. We used the SPoSE and VICE models to predict human similarity judgments and computed similarity matrices based on the model's predictions.</p>"},{"location":"code/#physical-face-features","title":"Physical face features","text":"<p>Original face stimuli are from the Chicago Face Database (CFD).</p> <p></p> <p>Each face is associated with a large set of physical face features (e.g., length of the nose). That is, each face can be described by a high-dimensional feature vector. These vectors have been used to compute face similarity matrices.</p>"},{"location":"code/#flame-and-deca-dimensions","title":"FLAME and DECA dimensions","text":"<p>The 3D-reconstructed faces have corresponding FLAME and DECA dimensions. These dimensions have been used to compute further similarity matrices.</p>"},{"location":"code/#code-structure","title":"Code structure","text":"<p>The analysis code can be installed as a Python package <code>facesim3d</code>.</p>"},{"location":"code/#how-to-set-up-the-research-code","title":"How to set up the research code","text":"<p>See the section Analysis package for how to install the research code as a  Python package.</p> <p>See the section API reference for more information on the analysis code of the study.</p>"},{"location":"experiment/","title":"Experiment overview","text":""},{"location":"experiment/#facesim3d-experiment","title":"FaceSim3D \u2013 experiment","text":"<pre><code>Period: November 2021 \u2014 December 2022\nStatus: [finalised]\n</code></pre> Authors Simon M. Hofmann Anthony Ciston Abhay Koushik Michael Gaebler Contact simon.hofmann[@]cbs.mpg.de GitHub @anfrimov GitHub @AbhayKoushik gaebler[@]cbs.mpg.de"},{"location":"experiment/#description-of-the-experiment","title":"Description of the experiment","text":"<p>The experiment consists of a triplet-odd-one-out task, where participants have to identify the most dissimilar face out of three faces. The experiment is run in two conditions: In the 2D condition, participants see static 2D images of faces. In the 3D condition, participants see dynamically moving faces, providing various perspectives.</p>"},{"location":"experiment/#experiment-setup-code","title":"Experiment setup &amp; code","text":"<p><code>UXF 2.0</code>, based on <code>Unity &gt;= 2018.4</code> has been used for the implementation of the face similarity judgment task (triplet odd-one-out task). The corresponding code can be found in <code>experiment/FaceSimExp/</code> in the repository. The experiment was launched as GitHub page. Participants were recruited via Prolific, and response data were temporarily stored on AWS servers.</p> <ol> <li> <p>Both Anthony Ciston and Abhay Koushik were central to the development of the experimental code.\u00a0\u21a9</p> </li> </ol>"},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2023 Simon M. Hofmann et al. (MPI CBS)</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"package/","title":"Analysis package","text":""},{"location":"package/#research-code-facesim3d-as-python-package","title":"Research code <code>facesim3d</code> as Python package","text":"<p>The code for data quality control, preprocessing, modeling and further analysis is available as the Python package <code>facesim3d</code>. The package has been implemented in <code>Python 3.10.4</code> (no other versions have been tested).</p> <p>Ideally, create a virtual environment (e.g., <code>conda</code>) to install the package:</p> <pre><code>conda create -n face_3.10 python=3.10.4\n</code></pre> <p>Activate the environment:</p> <pre><code>conda activate face_3.10\n</code></pre>"},{"location":"package/#install-the-package","title":"Install the package","text":"<pre><code># Go to root folder of FaceSim3D\ncd FaceSim3D/\npip install -e .\n</code></pre> <p>Two computational models (SPosE, VICE) that were used require additional packages that can be installed with the following command:</p> <pre><code>pip install -e \".[spose,vice]\"\n</code></pre> <p>See the <code>API reference</code> for more information on the analysis code of the study.</p>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#publications","title":"Publications","text":"<p>Results of the main study are published in the form of a preprint on PsyArXiv (Hofmann et al., 2024)<sup>1</sup> and are currently under review.</p> <p>Results on the pilot study were presented</p> <p>at CCN 2022 (San Francisco): (Hofmann et al., 2022)<sup>2</sup>.</p> <ol> <li> <p>Hofmann, S. M., Ciston, A., Koushik, A., Klotzsche, F., Hebart, M. N., M\u00fcller, K.-R., Villringer, A., Scherf, N., Hilsmann, A., Nikulin, V. V. and Gaebler, M. (2024). Human-aligned deep and sparse encoding models of dynamic 3D face similarity perception. PsyArXiv. https://doi.org/10.31234/osf.io/f62pw \u21a9</p> </li> <li> <p>Hofmann, S., Koushik, A., Klotzsche, F., Nikulin, V. V., Villringer, A. and Gaebler, M. (2022). Testing the effect of depth on the perception of faces in an online study. Proceedings of the 2022 Conference on Cognitive Computational Neuroscience. https://doi.org/10.32470/CCN.2022.1254-0 \u21a9</p> </li> </ol>"},{"location":"results/","title":"Results overview","text":""},{"location":"results/#facesim3d-results","title":"FaceSim3D results","text":"Access to the notebook <p>The notebook can also be found on the project's  GitHub repository in the folder <code>code/notebooks/</code>.</p> <p>An overview of the results is provided in the form of a <code>Jupyter notebook</code> on the next page.</p>"},{"location":"notebooks/facesim_results/","title":"FaceSim \u2013 Results","text":"<pre>\nNumber of participants that took part in session 2D: 1530 (n_female = 714, n_male = 716, n_unknown = 100)\nAge of participants in session 2D: range = 19 - 65, mean = 31.27 \u00b1 10.75 SD years\n\nNumber of participants that completed the task in session 2D: 1397\n\nNumber of participants that failed the attention checks in session 2D: 104\n\nNumber of participants that entered NO CODE in session 2D: 21\n\nNumber of participants that entered unknown code in session 2D: 8\n\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n\nNumber of participants that took part in session 3D: 1414 (n_female = 667, n_male = 662, n_unknown = 85)\nAge of participants in session 3D: range = 18 - 65, mean = 32.57 \u00b1 11.24 SD years\n\nNumber of participants that completed the task in session 3D: 1313\n\nNumber of participants that failed the attention checks in session 3D: 78\n\nNumber of participants that entered NO CODE in session 3D: 22\n\nNumber of participants that entered unknown code in session 3D: 1\n\n*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\nAge of all participants: range = 18 - 65, mean = 31.90 \u00b1 11.01 SD years\n\n</pre> <pre>\nPprobabity being caught &lt; 3 times across the whole experiment\nThe chance of having less than 3 catch(es) in 9 catch-trials is: 0.83%\n\nProbabity being not caught in one block\nThe chance of having less than 1 catch(es) in 3 catch-trials is: 3.70%\n</pre> <pre>\n2D Session:\nIn session 2D 18.5% of trials were removed.\n\n3D Session:\nIn session 3D 16.9% of trials were removed.\n</pre> <pre>\nResponse time for session 2D:\ncount    194261.000000\nmean          3.479793\nstd           1.498887\nmin           0.651300\n25%           2.374800\n50%           3.098500\n75%           4.214500\nmax           9.999999\nName: response_time, dtype: float64\n\nResponse time for session 3D:\ncount    185473.000000\nmean          3.543740\nstd           1.496115\nmin           0.633200\n25%           2.433000\n50%           3.169599\n75%           4.299999\nmax           9.999999\nName: response_time, dtype: float64\n\nttest(rt_2d, rt_3d): t-statistic: -13.15, p-value &lt;= 1.66e-39\nCohen's d: -0.043\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:          response_time   No. Observations:               379734\nModel:                            GLM   Df Residuals:                   379732\nModel Family:         InverseGaussian   Df Model:                            1\nLink Function:               identity   Scale:                        0.051843\nMethod:                          IRLS   Log-Likelihood:            -6.3403e+05\nDate:                Thu, 21 Nov 2024   Deviance:                       18458.\nTime:                        15:01:50   Pearson chi2:                 1.97e+04\nNo. Iterations:                     5   Pseudo R-squ. (CS):          0.0004551\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept            3.4798      0.003   1037.700      0.000       3.473       3.486\nC(session)[T.3D]     0.0639      0.005     13.140      0.000       0.054       0.073\n====================================================================================\n</pre> <pre>pearsonr(sim ~ RT) (order=1): PearsonRResult(statistic=0.23160084101455494, pvalue=2.8385370557883092e-61)\npearsonr(sim ~ RT) (order=2): PearsonRResult(statistic=0.22948643784766498, pvalue=3.658911721046632e-60)\npearsonr(sim ~ RT) (order=1): PearsonRResult(statistic=0.26796668659180756, pvalue=3.6611021617578353e-82)\npearsonr(sim ~ RT) (order=2): PearsonRResult(statistic=0.2649071451455536, pvalue=2.845448723742933e-80)\n2D\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          response_time   R-squared:                       0.054\nModel:                            OLS   Adj. R-squared:                  0.053\nMethod:                 Least Squares   F-statistic:                     280.4\nDate:                Thu, 21 Nov 2024   Prob (F-statistic):           2.84e-61\nTime:                        15:02:07   Log-Likelihood:                 1586.2\nNo. Observations:                4950   AIC:                            -3168.\nDf Residuals:                    4948   BIC:                            -3155.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.3997      0.005    627.696      0.000       3.389       3.410\nx1             0.2415      0.014     16.747      0.000       0.213       0.270\n==============================================================================\nOmnibus:                       10.620   Durbin-Watson:                   1.836\nProb(Omnibus):                  0.005   Jarque-Bera (JB):               12.597\nSkew:                          -0.031   Prob(JB):                      0.00184\nKurtosis:                       3.239   Cond. No.                         6.43\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n(2D-session: 1-order polynomial fit: R2=0.054; b0=3.400, SE0=0.005, t0=627.696, p0&lt;0.000; b1=0.241, SE1=0.014, t1=16.747, p1&lt;0.000; AIC=-3168, BIC=-3155).\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          response_time   R-squared:                       0.229\nModel:                            OLS   Adj. R-squared:                  0.229\nMethod:                 Least Squares   F-statistic:                     735.5\nDate:                Thu, 21 Nov 2024   Prob (F-statistic):          2.21e-280\nTime:                        15:02:07   Log-Likelihood:                 2094.0\nNo. Observations:                4950   AIC:                            -4182.\nDf Residuals:                    4947   BIC:                            -4163.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.1478      0.009    351.479      0.000       3.130       3.165\nx1             1.9309      0.052     37.144      0.000       1.829       2.033\nx2            -2.2061      0.066    -33.568      0.000      -2.335      -2.077\n==============================================================================\nOmnibus:                       27.794   Durbin-Watson:                   1.913\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               28.107\nSkew:                           0.183   Prob(JB):                     7.88e-07\nKurtosis:                       3.049   Cond. No.                         39.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n(2D-session: 2-order polynomial fit: R2=0.229; b0=3.148, SE0=0.009, t0=351.479, p0&lt;0.000; b1=1.931, SE1=0.052, t1=37.144, p1&lt;0.000; b2=-2.206, SE2=0.066, t2=-33.568, p2&lt;0.000; AIC=-4182, BIC=-4163).\n\n3D\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          response_time   R-squared:                       0.072\nModel:                            OLS   Adj. R-squared:                  0.072\nMethod:                 Least Squares   F-statistic:                     382.8\nDate:                Thu, 21 Nov 2024   Prob (F-statistic):           3.66e-82\nTime:                        15:02:07   Log-Likelihood:                 1423.8\nNo. Observations:                4950   AIC:                            -2844.\nDf Residuals:                    4948   BIC:                            -2831.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.4475      0.006    622.442      0.000       3.437       3.458\nx1             0.2877      0.015     19.565      0.000       0.259       0.316\n==============================================================================\nOmnibus:                       38.559   Durbin-Watson:                   1.788\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               41.096\nSkew:                          -0.186   Prob(JB):                     1.19e-09\nKurtosis:                       3.245   Cond. No.                         6.35\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n(3D-session: 1-order polynomial fit: R2=0.072; b0=3.448, SE0=0.006, t0=622.442, p0&lt;0.000; b1=0.288, SE1=0.015, t1=19.565, p1&lt;0.000; AIC=-2844, BIC=-2831).\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          response_time   R-squared:                       0.255\nModel:                            OLS   Adj. R-squared:                  0.255\nMethod:                 Least Squares   F-statistic:                     846.9\nDate:                Thu, 21 Nov 2024   Prob (F-statistic):          4.75e-317\nTime:                        15:02:07   Log-Likelihood:                 1968.1\nNo. Observations:                4950   AIC:                            -3930.\nDf Residuals:                    4947   BIC:                            -3911.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.1860      0.009    354.354      0.000       3.168       3.204\nx1             2.0575      0.052     39.255      0.000       1.955       2.160\nx2            -2.3142      0.066    -34.886      0.000      -2.444      -2.184\n==============================================================================\nOmnibus:                        0.521   Durbin-Watson:                   1.854\nProb(Omnibus):                  0.771   Jarque-Bera (JB):                0.473\nSkew:                           0.016   Prob(JB):                        0.790\nKurtosis:                       3.036   Cond. No.                         38.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n(3D-session: 2-order polynomial fit: R2=0.255; b0=3.186, SE0=0.009, t0=354.354, p0&lt;0.000; b1=2.057, SE1=0.052, t1=39.255, p1&lt;0.000; b2=-2.314, SE2=0.066, t2=-34.886, p2&lt;0.000; AIC=-3930, BIC=-3911).\n\n</pre> Out[12]: sample_type max_acc min_n_samples session 2D multi-sub-sample 0.6634 5.0 2D multi-sub-sample_female 0.6483 5.0 2D multi-sub-sample_male 0.6433 5.0 3D multi-sub-sample 0.6867 5.0 3D multi-sub-sample_female 0.6767 5.0 3D multi-sub-sample_male 0.7100 5.0 <pre>\n2D Session:\n\nAccuracy of best performing SPoSE model 2D: 89.32% (59.26%)\n\n3D Session:\n\nAccuracy of best performing SPoSE model 3D: 86.01% (59.06%)\n</pre> <pre>\n2D Session:\n\nAccuracy of best performing VICE model 2D: 89.34% (59.27%)\n\n3D Session:\n\nAccuracy of best performing VICE model 3D: 86.32% (59.27%)\n</pre> <pre>\n2D Session:\n\nAccuracy of best performing VGGFace model '2023-11-15_04-44_VGGFaceHumanjudgmentFrozenCore' 2D: 85.92% (57.00%)\n\n3D Session:\n\nAccuracy of best performing VGGFace model '2023-12-11_19-46_VGGFaceHumanjudgmentFrozenCore' 3D: 84.17% (57.80%)\n</pre> <pre>Maximal emperical accuracy across viewing conditions is: 66.05%.\n</pre> <p>Now we take the same computation but across viewing conditions of the full-sample (i.e., data from the main study).</p> <pre>\nPredicting human choices in one viewing condition with the choices in the other condition\nleads to an accuracy of 72.76% (48.06%).\n\n</pre> <pre>The theoretical highest accuracy when predicting only mixed gender trials correctly,\n\twould be at: 75.76%\n\nThe theoretical highest accuracy when predicting mixed gender trials correctly and one-gender trials on chance level, \n\twould be at: 83.84%\n</pre> <p>For now, this is just of theoretical interest, and might be reported in the appendix of the main paper.</p> Out[22]: sample_type max_acc min_n_samples session 2D multi-sub-sample_female 0.6483 5.0 2D multi-sub-sample_male 0.6433 5.0 3D multi-sub-sample_female 0.6767 5.0 3D multi-sub-sample_male 0.7100 5.0 <pre>\n2D Session:\n\tfemale:\n\nAccuracy of best performing SPoSE model 2D within exclusive 'female' trials: 87.75% (56.89%)\n\tmale:\n\nAccuracy of best performing SPoSE model 2D within exclusive 'male' trials: 90.88% (58.46%)\n\n3D Session:\n\tfemale:\n\nAccuracy of best performing SPoSE model 3D within exclusive 'female' trials: 85.43% (57.81%)\n\tmale:\n\nAccuracy of best performing SPoSE model 3D within exclusive 'male' trials: 81.30% (57.72%)\n</pre> <pre>\n2D Session:\n\tfemale:\n\nAccuracy of best performing VICE model 2D within exclusive 'female' trials: 87.52% (56.74%)\n\tmale:\n\nAccuracy of best performing VICE model 2D within exclusive 'male' trials: 89.80% (57.77%)\n\n3D Session:\n\tfemale:\n\nAccuracy of best performing VICE model 3D within exclusive 'female' trials: 86.01% (58.20%)\n\tmale:\n\nAccuracy of best performing VICE model 3D within exclusive 'male' trials: 81.59% (57.93%)\n</pre> <pre>\n2D Session:\n\tfemale:\n\nAccuracy of best performing VGGFace model '2023-11-24_14-57_VGGFaceHumanjudgmentFrozenCore' 2D within exclusive 'female' trials: 82.52% (53.50%)\n\tmale:\n\nAccuracy of best performing VGGFace model '2023-11-26_09-05_VGGFaceHumanjudgmentFrozenCore' 2D within exclusive 'male' trials: 85.34% (54.90%)\n\n3D Session:\n\tfemale:\n\nAccuracy of best performing VGGFace model '2023-11-25_10-17_VGGFaceHumanjudgmentFrozenCore' 3D within exclusive 'female' trials: 79.21% (53.60%)\n\tmale:\n\nAccuracy of best performing VGGFace model '2023-11-26_22-57_VGGFaceHumanjudgmentFrozenCore' 3D within exclusive 'male' trials: 78.45% (55.70%)\n</pre> In\u00a0[27]: Copied! <pre>rsa_corr_df = get_corr_df_rsm(corr_name=CORR_NAME, metric=METRIC)\n# get_mer_table() # noqa: ERA001\n</pre> rsa_corr_df = get_corr_df_rsm(corr_name=CORR_NAME, metric=METRIC) # get_mer_table() # noqa: ERA001 In\u00a0[28]: Copied! <pre># Set N highest correlating VGG layers\nn_highest_vgg: int = 5\n</pre> # Set N highest correlating VGG layers n_highest_vgg: int = 5 <pre>\n2D_BSM\n\nSpearman correlation 2D_BSM ~ 3D_BSM_r: r=0.933, p&lt;=0\n\nSpearman correlation 2D_BSM ~ CFD_PFF_r: r=0.258, p&lt;=5.2e-76\n\nSpearman correlation 2D_BSM ~ DECA_SHAPE_r: r=0.259, p&lt;=1e-76\n\nSpearman correlation 2D_BSM ~ DECA_EXP_r: r=0.305, p&lt;=2.3e-107\n\nSpearman correlation 2D_BSM ~ DECA_POSE_r: r=0.089, p&lt;=3.1e-10\n\nSpearman correlation 2D_BSM ~ DECA_CAM_r: r=0.030, p&lt;=0.038\n\nSpearman correlation 2D_BSM ~ DECA_TEX_r: r=0.468, p&lt;=2.2e-267\n\nSpearman correlation 2D_BSM ~ DECA_DETAIL_r: r=0.319, p&lt;=1.9e-117\n\nSpearman correlation 2D_BSM ~ SPoSE_2D_r: r=0.927, p&lt;=0\n\nSpearman correlation 2D_BSM ~ VICE_2D_r: r=0.935, p&lt;=0\n\nSpearman correlation 2D_BSM ~ SPoSE_3D_r: r=0.914, p&lt;=0\n\nSpearman correlation 2D_BSM ~ VICE_3D_r: r=0.916, p&lt;=0\n\nSpearman correlation 2D_BSM ~ 2023-11-15_04-44_VGGFaceHumanjudgmentFrozenCore_2D_embedding_r: r=0.776, p&lt;=0\n\nSpearman correlation 2D_BSM ~ 2023-11-15_04-44_VGGFaceHumanjudgmentFrozenCore_2D_decision_r: r=0.876, p&lt;=0\n\n</pre> <pre>\nCorrelation VGG layers\n\nSpearman correlation 2D_BSM ~ VGG_3D-recon_MAXP_5_3_r: r=0.489, p&lt;=4.8e-296\n\nSpearman correlation 2D_BSM ~ VGG_3D-recon_RELU_5_3_r: r=0.482, p&lt;=2.4e-286\n\nSpearman correlation 2D_BSM ~ VGG_3D-recon_CONV_5_3_r: r=0.482, p&lt;=2.4e-286\n\nSpearman correlation 2D_BSM ~ VGG_3D-recon_FC6_r: r=0.432, p&lt;=1.8e-224\n\nSpearman correlation 2D_BSM ~ VGG_3D-recon_FC6-DROPOUT_r: r=0.432, p&lt;=1.8e-224\n\n</pre> <pre>\n********************************************************************************\n\n3D_BSM\n\nSpearman correlation 3D_BSM ~ 2D_BSM_r: r=0.933, p&lt;=0\n\nSpearman correlation 3D_BSM ~ CFD_PFF_r: r=0.246, p&lt;=2.2e-69\n\nSpearman correlation 3D_BSM ~ DECA_SHAPE_r: r=0.249, p&lt;=4.7e-71\n\nSpearman correlation 3D_BSM ~ DECA_EXP_r: r=0.299, p&lt;=5.9e-103\n\nSpearman correlation 3D_BSM ~ DECA_POSE_r: r=0.093, p&lt;=6.9e-11\n\nSpearman correlation 3D_BSM ~ DECA_CAM_r: r=0.041, p&lt;=0.0042\n\nSpearman correlation 3D_BSM ~ DECA_TEX_r: r=0.478, p&lt;=4.8e-281\n\nSpearman correlation 3D_BSM ~ DECA_DETAIL_r: r=0.326, p&lt;=1.4e-122\n\nSpearman correlation 3D_BSM ~ SPoSE_2D_r: r=0.912, p&lt;=0\n\nSpearman correlation 3D_BSM ~ VICE_2D_r: r=0.916, p&lt;=0\n\nSpearman correlation 3D_BSM ~ SPoSE_3D_r: r=0.936, p&lt;=0\n\nSpearman correlation 3D_BSM ~ VICE_3D_r: r=0.939, p&lt;=0\n\nSpearman correlation 3D_BSM ~ 2023-12-11_19-46_VGGFaceHumanjudgmentFrozenCore_3D_embedding_r: r=0.721, p&lt;=0\n\nSpearman correlation 3D_BSM ~ 2023-12-11_19-46_VGGFaceHumanjudgmentFrozenCore_3D_decision_r: r=0.879, p&lt;=0\n\n</pre> <pre>\nCorrelation VGG layers\n\nSpearman correlation 3D_BSM ~ VGG_3D-recon_MAXP_5_3_r: r=0.490, p&lt;=5.4e-297\n\nSpearman correlation 3D_BSM ~ VGG_3D-recon_RELU_5_3_r: r=0.483, p&lt;=3e-287\n\nSpearman correlation 3D_BSM ~ VGG_3D-recon_CONV_5_3_r: r=0.483, p&lt;=3e-287\n\nSpearman correlation 3D_BSM ~ VGG_3D-recon_FC6_r: r=0.435, p&lt;=4.8e-228\n\nSpearman correlation 3D_BSM ~ VGG_3D-recon_FC6-DROPOUT_r: r=0.435, p&lt;=4.8e-228\n\n</pre> <pre>\n********************************************************************************\n</pre> <pre>\n2D_BSM_female_only\n\nSpearman correlation 2D_BSM_female_only ~ 3D_BSM_female_only_r: r=0.839, p&lt;=0\n\nSpearman correlation 2D_BSM_female_only ~ 3D_BSM_male_only_r: r=0.036, p&lt;=0.21\n\nSpearman correlation 2D_BSM_female_only ~ 2D_BSM_male_only_r: r=0.046, p&lt;=0.11\n\nSpearman correlation 2D_BSM_female_only ~ CFD_PFF_female_only_r: r=0.216, p&lt;=2e-14\n\nSpearman correlation 2D_BSM_female_only ~ DECA_SHAPE_female_only_r: r=0.155, p&lt;=4.8e-08\n\nSpearman correlation 2D_BSM_female_only ~ DECA_EXP_female_only_r: r=0.150, p&lt;=1.3e-07\n\nSpearman correlation 2D_BSM_female_only ~ DECA_POSE_female_only_r: r=0.109, p&lt;=0.00013\n\nSpearman correlation 2D_BSM_female_only ~ DECA_CAM_female_only_r: r=0.095, p&lt;=0.00085\n\nSpearman correlation 2D_BSM_female_only ~ DECA_TEX_female_only_r: r=0.154, p&lt;=5.6e-08\n\nSpearman correlation 2D_BSM_female_only ~ DECA_DETAIL_female_only_r: r=0.064, p&lt;=0.024\n\nSpearman correlation 2D_BSM_female_only ~ SPoSE_2D_female_only_r: r=0.923, p&lt;=0\n\nSpearman correlation 2D_BSM_female_only ~ VICE_2D_female_only_r: r=0.873, p&lt;=0\n\nSpearman correlation 2D_BSM_female_only ~ SPoSE_3D_female_only_r: r=0.814, p&lt;=2.2e-291\n\nSpearman correlation 2D_BSM_female_only ~ VICE_3D_female_only_r: r=0.824, p&lt;=6.4e-304\n\nSpearman correlation 2D_BSM_female_only ~ 2023-11-24_14-57_VGGFaceHumanjudgmentFrozenCore_2D_female_only_r: r=0.543, p&lt;=6.8e-95\n\nSpearman correlation 2D_BSM_female_only ~ 2023-11-15_04-44_VGGFaceHumanjudgmentFrozenCore_2D_decision_female_only_r: r=0.741, p&lt;=7.3e-214\n\n</pre> <pre>\nCorrelation VGG layers\n\nSpearman correlation 2D_BSM_female_only ~ VGG_3D-recon_RELU_5_3_female_only_r: r=0.436, p&lt;=6.3e-58\n\nSpearman correlation 2D_BSM_female_only ~ VGG_3D-recon_CONV_5_3_female_only_r: r=0.436, p&lt;=6.3e-58\n\nSpearman correlation 2D_BSM_female_only ~ VGG_3D-recon_MAXP_5_3_female_only_r: r=0.431, p&lt;=1.9e-56\n\nSpearman correlation 2D_BSM_female_only ~ VGG_3D-recon_RELU_5_2_female_only_r: r=0.398, p&lt;=7.4e-48\n\nSpearman correlation 2D_BSM_female_only ~ VGG_3D-recon_CONV_5_2_female_only_r: r=0.398, p&lt;=7.4e-48\n\n</pre> <pre>\n--------------------------------------------------------------------------------\n\n2D_BSM_male_only\n\nSpearman correlation 2D_BSM_male_only ~ 3D_BSM_female_only_r: r=0.055, p&lt;=0.057\n\nSpearman correlation 2D_BSM_male_only ~ 2D_BSM_female_only_r: r=0.046, p&lt;=0.11\n\nSpearman correlation 2D_BSM_male_only ~ 3D_BSM_male_only_r: r=0.881, p&lt;=0\n\nSpearman correlation 2D_BSM_male_only ~ CFD_PFF_male_only_r: r=0.196, p&lt;=5e-12\n\nSpearman correlation 2D_BSM_male_only ~ DECA_SHAPE_male_only_r: r=0.228, p&lt;=5.8e-16\n\nSpearman correlation 2D_BSM_male_only ~ DECA_EXP_male_only_r: r=0.245, p&lt;=3.6e-18\n\nSpearman correlation 2D_BSM_male_only ~ DECA_POSE_male_only_r: r=0.064, p&lt;=0.026\n\nSpearman correlation 2D_BSM_male_only ~ DECA_CAM_male_only_r: r=-0.004, p&lt;=0.9\n\nSpearman correlation 2D_BSM_male_only ~ DECA_TEX_male_only_r: r=0.255, p&lt;=1.2e-19\n\nSpearman correlation 2D_BSM_male_only ~ DECA_DETAIL_male_only_r: r=0.126, p&lt;=1e-05\n\nSpearman correlation 2D_BSM_male_only ~ SPoSE_2D_male_only_r: r=0.917, p&lt;=0\n\nSpearman correlation 2D_BSM_male_only ~ VICE_2D_male_only_r: r=0.877, p&lt;=0\n\nSpearman correlation 2D_BSM_male_only ~ SPoSE_3D_male_only_r: r=0.863, p&lt;=0\n\nSpearman correlation 2D_BSM_male_only ~ VICE_3D_male_only_r: r=0.834, p&lt;=1.6e-318\n\nSpearman correlation 2D_BSM_male_only ~ 2023-11-26_09-05_VGGFaceHumanjudgmentFrozenCore_2D_male_only_r: r=0.715, p&lt;=2.3e-192\n\nSpearman correlation 2D_BSM_male_only ~ 2023-11-15_04-44_VGGFaceHumanjudgmentFrozenCore_2D_decision_male_only_r: r=0.821, p&lt;=1.1e-299\n\n</pre> <pre>\nCorrelation VGG layers\n\nSpearman correlation 2D_BSM_male_only ~ VGG_3D-recon_FC6-DROPOUT_male_only_r: r=0.445, p&lt;=1.1e-60\n\nSpearman correlation 2D_BSM_male_only ~ VGG_3D-recon_FC6-RELU_male_only_r: r=0.445, p&lt;=1.1e-60\n\nSpearman correlation 2D_BSM_male_only ~ VGG_3D-recon_FC6_male_only_r: r=0.445, p&lt;=1.1e-60\n\nSpearman correlation 2D_BSM_male_only ~ VGG_3D-recon_MAXP_5_3_male_only_r: r=0.441, p&lt;=1.7e-59\n\nSpearman correlation 2D_BSM_male_only ~ VGG_3D-recon_RELU_5_3_male_only_r: r=0.432, p&lt;=9.2e-57\n\n</pre> <pre>\n--------------------------------------------------------------------------------\n\n3D_BSM_female_only\n\nSpearman correlation 3D_BSM_female_only ~ 2D_BSM_female_only_r: r=0.839, p&lt;=0\n\nSpearman correlation 3D_BSM_female_only ~ 3D_BSM_male_only_r: r=0.045, p&lt;=0.11\n\nSpearman correlation 3D_BSM_female_only ~ 2D_BSM_male_only_r: r=0.055, p&lt;=0.057\n\nSpearman correlation 3D_BSM_female_only ~ CFD_PFF_female_only_r: r=0.222, p&lt;=4e-15\n\nSpearman correlation 3D_BSM_female_only ~ DECA_SHAPE_female_only_r: r=0.128, p&lt;=7e-06\n\nSpearman correlation 3D_BSM_female_only ~ DECA_EXP_female_only_r: r=0.115, p&lt;=5.9e-05\n\nSpearman correlation 3D_BSM_female_only ~ DECA_POSE_female_only_r: r=0.112, p&lt;=8.4e-05\n\nSpearman correlation 3D_BSM_female_only ~ DECA_CAM_female_only_r: r=0.104, p&lt;=0.00028\n\nSpearman correlation 3D_BSM_female_only ~ DECA_TEX_female_only_r: r=0.169, p&lt;=2.6e-09\n\nSpearman correlation 3D_BSM_female_only ~ DECA_DETAIL_female_only_r: r=0.074, p&lt;=0.01\n\nSpearman correlation 3D_BSM_female_only ~ SPoSE_2D_female_only_r: r=0.827, p&lt;=2.4e-308\n\nSpearman correlation 3D_BSM_female_only ~ VICE_2D_female_only_r: r=0.820, p&lt;=5.4e-298\n\nSpearman correlation 3D_BSM_female_only ~ SPoSE_3D_female_only_r: r=0.928, p&lt;=0\n\nSpearman correlation 3D_BSM_female_only ~ VICE_3D_female_only_r: r=0.886, p&lt;=0\n\nSpearman correlation 3D_BSM_female_only ~ 2023-11-25_10-17_VGGFaceHumanjudgmentFrozenCore_3D_female_only_r: r=0.630, p&lt;=1.6e-136\n\nSpearman correlation 3D_BSM_female_only ~ 2023-12-11_19-46_VGGFaceHumanjudgmentFrozenCore_3D_decision_female_only_r: r=0.796, p&lt;=2.7e-269\n\n</pre> <pre>\nCorrelation VGG layers\n\nSpearman correlation 3D_BSM_female_only ~ VGG_3D-recon_RELU_5_3_female_only_r: r=0.410, p&lt;=8e-51\n\nSpearman correlation 3D_BSM_female_only ~ VGG_3D-recon_CONV_5_3_female_only_r: r=0.410, p&lt;=8e-51\n\nSpearman correlation 3D_BSM_female_only ~ VGG_3D-recon_MAXP_5_3_female_only_r: r=0.404, p&lt;=2.1e-49\n\nSpearman correlation 3D_BSM_female_only ~ VGG_3D-recon_RELU_5_2_female_only_r: r=0.377, p&lt;=1.1e-42\n\nSpearman correlation 3D_BSM_female_only ~ VGG_3D-recon_CONV_5_2_female_only_r: r=0.377, p&lt;=1.1e-42\n\n</pre> <pre>\n--------------------------------------------------------------------------------\n\n3D_BSM_male_only\n\nSpearman correlation 3D_BSM_male_only ~ 3D_BSM_female_only_r: r=0.045, p&lt;=0.11\n\nSpearman correlation 3D_BSM_male_only ~ 2D_BSM_female_only_r: r=0.036, p&lt;=0.21\n\nSpearman correlation 3D_BSM_male_only ~ 2D_BSM_male_only_r: r=0.881, p&lt;=0\n\nSpearman correlation 3D_BSM_male_only ~ CFD_PFF_male_only_r: r=0.176, p&lt;=5e-10\n\nSpearman correlation 3D_BSM_male_only ~ DECA_SHAPE_male_only_r: r=0.202, p&lt;=9e-13\n\nSpearman correlation 3D_BSM_male_only ~ DECA_EXP_male_only_r: r=0.242, p&lt;=9.1e-18\n\nSpearman correlation 3D_BSM_male_only ~ DECA_POSE_male_only_r: r=0.069, p&lt;=0.016\n\nSpearman correlation 3D_BSM_male_only ~ DECA_CAM_male_only_r: r=0.014, p&lt;=0.63\n\nSpearman correlation 3D_BSM_male_only ~ DECA_TEX_male_only_r: r=0.276, p&lt;=6.4e-23\n\nSpearman correlation 3D_BSM_male_only ~ DECA_DETAIL_male_only_r: r=0.132, p&lt;=3.7e-06\n\nSpearman correlation 3D_BSM_male_only ~ SPoSE_2D_male_only_r: r=0.846, p&lt;=0\n\nSpearman correlation 3D_BSM_male_only ~ VICE_2D_male_only_r: r=0.838, p&lt;=0\n\nSpearman correlation 3D_BSM_male_only ~ SPoSE_3D_male_only_r: r=0.945, p&lt;=0\n\nSpearman correlation 3D_BSM_male_only ~ VICE_3D_male_only_r: r=0.883, p&lt;=0\n\nSpearman correlation 3D_BSM_male_only ~ 2023-11-26_22-57_VGGFaceHumanjudgmentFrozenCore_3D_male_only_r: r=0.671, p&lt;=7.7e-161\n\nSpearman correlation 3D_BSM_male_only ~ 2023-12-11_19-46_VGGFaceHumanjudgmentFrozenCore_3D_decision_male_only_r: r=0.832, p&lt;=2.5e-315\n\n</pre> <pre>\nCorrelation VGG layers\n\nSpearman correlation 3D_BSM_male_only ~ VGG_3D-recon_FC6_male_only_r: r=0.449, p&lt;=6.9e-62\n\nSpearman correlation 3D_BSM_male_only ~ VGG_3D-recon_FC6-DROPOUT_male_only_r: r=0.449, p&lt;=6.9e-62\n\nSpearman correlation 3D_BSM_male_only ~ VGG_3D-recon_FC6-RELU_male_only_r: r=0.449, p&lt;=6.9e-62\n\nSpearman correlation 3D_BSM_male_only ~ VGG_3D-recon_MAXP_5_3_male_only_r: r=0.441, p&lt;=1.8e-59\n\nSpearman correlation 3D_BSM_male_only ~ VGG_3D-recon_RELU_5_3_male_only_r: r=0.433, p&lt;=3.4e-57\n\n</pre> <pre>\n--------------------------------------------------------------------------------\n</pre> <pre>After VICE optimization:\n\n\t* in the 2D-session, 28 dimensions remained relevant.\n\t* in the 3D-session, 26 dimensions remained relevant.\n</pre> <pre>3 strongest correlations between first 3 VICE dimensions and PFAs\n\n2D-VICE dimension 1:\n\nPFA labels\t\t\tMidcheekChinR\t\tMidcheekChinL\t\tCheeksAvg\nR\t\t\t\t\t0.31\t\t\t0.31\t\t\t0.31\n\n2D-VICE dimension 2:\n\nPFA labels\t\t\tEyeSize\t\tCheeksAvg\t\tMidcheekChinL\nR\t\t\t\t\t-0.46\t\t\t0.48\t\t\t0.48\n\n2D-VICE dimension 3:\n\nPFA labels\t\t\tNoseWidth\t\tBottomLipChin\t\tNoseShape\nR\t\t\t\t\t-0.47\t\t\t-0.48\t\t\t-0.58\n\n *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n\n3D-VICE dimension 1:\n\nPFA labels\t\t\tCheeksAvg\t\tMidcheekChinL\t\tBottomLipChin\nR\t\t\t\t\t0.25\t\t\t0.26\t\t\t0.3\n\n3D-VICE dimension 2:\n\nPFA labels\t\t\tUpperFaceLength2\t\tfWHR2\t\tNoseShape\nR\t\t\t\t\t0.57\t\t\t-0.6\t\t\t-0.61\n\n3D-VICE dimension 3:\n\nPFA labels\t\t\tEyeSize\t\tChinLength\t\tBottomLipChin\nR\t\t\t\t\t0.35\t\t\t-0.41\t\t\t-0.48\n\n *-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n</pre> <pre>Female eye-size:\n count    50.000000\nmean      0.054131\nstd       0.006044\nmin       0.039763\n25%       0.051509\n50%       0.052956\n75%       0.057007\nmax       0.071462\nName: P056, dtype: float64\n\nMale eye-size:\n count    50.000000\nmean      0.048736\nstd       0.007077\nmin       0.029462\n25%       0.043373\n50%       0.049134\n75%       0.053614\nmax       0.061582\nName: P056, dtype: float64\n\nttest_ind(a=female_feat_tabel[eye_size_code], b=male_feat_tabel[eye_size_code]) = Ttest_indResult(statistic=4.099633006037346, pvalue=8.543611806806524e-05)\n</pre>"},{"location":"notebooks/facesim_results/#facesim-results","title":"FaceSim \u2013 Results\u00b6","text":"<p>Notebook to summarize results of the <code>FaceSim</code> study.</p> <pre><code>Author: Simon M. Hofmann\nYears: 2023-2024</code></pre>"},{"location":"notebooks/facesim_results/#results","title":"Results\u00b6","text":""},{"location":"notebooks/facesim_results/#general-stats","title":"General stats\u00b6","text":""},{"location":"notebooks/facesim_results/#participants","title":"Participants\u00b6","text":""},{"location":"notebooks/facesim_results/#trials-excluded","title":"Trials excluded\u00b6","text":""},{"location":"notebooks/facesim_results/#response-times","title":"Response times\u00b6","text":""},{"location":"notebooks/facesim_results/#correlate-response-times-of-triplet-ids-between-sessions","title":"Correlate response times of triplet ID's between sessions\u00b6","text":""},{"location":"notebooks/facesim_results/#response-times-as-function-of-similarity","title":"Response times as function of similarity\u00b6","text":""},{"location":"notebooks/facesim_results/#predicting-human-similarity-judgments","title":"Predicting human similarity judgments\u00b6","text":""},{"location":"notebooks/facesim_results/#across-all-samples","title":"Across all samples\u00b6","text":""},{"location":"notebooks/facesim_results/#maximal-empirical-accuracy-noise-ceiling","title":"Maximal empirical accuracy (noise ceiling)\u00b6","text":"<p>Maximal empirical accuracy is the upper bound for model performance based on the human choice variance within the same face triplets (often called the noise ceiling).</p> <p>In the following we report the accuracy adjusted for the maximal empirical accuracy, followed by the bare accuracy in brackets.</p>"},{"location":"notebooks/facesim_results/#sparse-encoding-models","title":"Sparse encoding models\u00b6","text":""},{"location":"notebooks/facesim_results/#spose","title":"SPoSE\u00b6","text":""},{"location":"notebooks/facesim_results/#vice","title":"VICE\u00b6","text":""},{"location":"notebooks/facesim_results/#deep-encoding-models","title":"Deep encoding models\u00b6","text":""},{"location":"notebooks/facesim_results/#vgg-face-frozencore","title":"VGG-Face FrozenCore\u00b6","text":"<p>This variant of VGG Face uses the pre-trained VGG-Face weights up until the layer <code>XX</code>[FILL]. The newly attached upper part of the model consists of further convolutional layers, i.e., the decision block is fully convolutional.</p>"},{"location":"notebooks/facesim_results/#predicting-one-viewing-condition-from-the-other","title":"Predicting one viewing condition from the other\u00b6","text":""},{"location":"notebooks/facesim_results/#extract-maximal-emperical-accuracy-across-viewing-conditions-in-the-multi-sub-sample","title":"Extract maximal emperical accuracy across viewing conditions (in the multi-sub-sample).\u00b6","text":""},{"location":"notebooks/facesim_results/#accuracy-when-gender-matches","title":"Accuracy when gender matches\u00b6","text":"<p>Calculate the probabilty when participants would only choose the odd-one-out based on gender. This would be a theoretical upper bound for an accuracy based on such a simply choice-behavior. That is, when there is a triplet with, e.g., <code>M</code>-<code>M</code>-<code>F</code>, they choose <code>F</code>. For triplets with one gender only, they guess randomly.</p>"},{"location":"notebooks/facesim_results/#predictions-within-gender-trials","title":"Predictions within gender trials\u00b6","text":""},{"location":"notebooks/facesim_results/#maximial-empirical-accuracy-within-gender-exclusive-trials","title":"Maximial empirical accuracy within gender-exclusive-trials\u00b6","text":""},{"location":"notebooks/facesim_results/#sparse-encoding-models","title":"Sparse encoding models\u00b6","text":""},{"location":"notebooks/facesim_results/#spose","title":"SPoSE\u00b6","text":""},{"location":"notebooks/facesim_results/#vice","title":"VICE\u00b6","text":""},{"location":"notebooks/facesim_results/#deep-encoding-models","title":"Deep encoding models\u00b6","text":""},{"location":"notebooks/facesim_results/#vgg-face-frozencore","title":"VGG-Face FrozenCore\u00b6","text":""},{"location":"notebooks/facesim_results/#representational-differences-between-viewing-conditions-2d-3d","title":"Representational differences between viewing conditions (2D &amp; 3D)\u00b6","text":"<p>Differences between the static 2D and the dynamic 3D condition</p>"},{"location":"notebooks/facesim_results/#correlations-of-similarity-matrices-rsa","title":"Correlations of similarity matrices (RSA)\u00b6","text":""},{"location":"notebooks/facesim_results/#todo-add-info-about-maximal-empirical-correlation-coefficient","title":"TODO: add info about maximal empirical correlation coefficient.\u00b6","text":""},{"location":"notebooks/facesim_results/#correlations-within-exclusive-gender-trials-rsa","title":"Correlations within exclusive gender trials (RSA)\u00b6","text":""},{"location":"notebooks/facesim_results/#behaviorally-relevant-face-features","title":"Behaviorally relevant face features\u00b6","text":"<p>Correlations of VICE weights (dimensions) with PFA (CFD).</p> <p>Also computed in <code>computational_choice_model.py</code>; and, see plots above.</p>"},{"location":"notebooks/facesim_results/#face-attributes","title":"Face attributes\u00b6","text":""},{"location":"notebooks/facesim_results/#gender-specific-face-attributes-how-do-pfas-in-cfd-differ-between-genders","title":"Gender-specific face attributes: How do PFAs (in CFD) differ between genders?\u00b6","text":""},{"location":"reference/","title":"Index","text":""},{"location":"reference/#facesim3d","title":"facesim3d","text":"<p><code>facesim3d</code>: A Python package to reproduce the <code>FaceSim3D</code> study.</p> <p>Modules:</p> Name Description <code>configs</code> <p>Configuration for FaceSim3D project.</p> <code>datachecks</code> <p>Check data from the triplet-odd-one out task.</p> <code>facesim3d</code> <p><code>facesim3d</code>: A Python package to reproduce the <code>FaceSim3D</code> study.</p> <code>modeling</code> <p>Init facesim3d.modeling submodule.</p> <code>read_data</code> <p>Read &amp; prepare data files.</p>"},{"location":"reference/configs/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> configs","text":""},{"location":"reference/configs/#facesim3d.configs","title":"configs","text":"<p>Configuration for FaceSim3D project.</p> <p>Note: store private configs in the same folder as <code>config.toml</code>, namely: <code>./[PRIVATE_PREFIX]_configs.toml</code></p> <p>Author: Simon M. Hofmann | [firstname].[lastname]@cbs.mpg.de | 2022</p> <p>Classes:</p> Name Description <code>CONFIG</code> <p>Configuration object.</p> <p>Functions:</p> Name Description <code>update_logger_configs</code> <p>Update logger name and filename.</p>"},{"location":"reference/configs/#facesim3d.configs.CONFIG","title":"CONFIG","text":"<pre><code>CONFIG(config_dict: dict | None = None)\n</code></pre> <p>Configuration object.</p> <p>Initialise CONFIG class object.</p> <p>Methods:</p> Name Description <code>asdict</code> <p>Convert the config object to dict.</p> <code>show</code> <p>Display the nested configuration information.</p> <code>update</code> <p>Update the config object with new entries.</p> <code>update_paths</code> <p>Update relative paths to PROJECT_ROOT dir.</p> Source code in <code>code/facesim3d/configs.py</code> <pre><code>def __init__(self, config_dict: dict | None = None) -&gt; None:\n    \"\"\"Initialise CONFIG class object.\"\"\"\n    if config_dict is not None:\n        self.update(config_dict)\n</code></pre>"},{"location":"reference/configs/#facesim3d.configs.CONFIG.asdict","title":"asdict","text":"<pre><code>asdict()\n</code></pre> <p>Convert the config object to dict.</p> Source code in <code>code/facesim3d/configs.py</code> <pre><code>def asdict(self):\n    \"\"\"Convert the config object to dict.\"\"\"\n    dict_out = {}\n    for key, val in self.__dict__.items():\n        if isinstance(val, CONFIG):\n            dict_out.update({key: val.asdict()})\n        else:\n            dict_out.update({key: val})\n    return dict_out\n</code></pre>"},{"location":"reference/configs/#facesim3d.configs.CONFIG.show","title":"show","text":"<pre><code>show(indent: int = 0)\n</code></pre> <p>Display the nested configuration information.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>int</code> <p>The number of tabs to use for indentation (default: 0)</p> <code>0</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>code/facesim3d/configs.py</code> <pre><code>def show(self, indent: int = 0):\n    \"\"\"\n    Display the nested configuration information.\n\n    :param indent: The number of tabs to use for indentation (default: 0)\n    :return: None\n    \"\"\"\n    for key, val in self.__dict__.items():\n        if isinstance(val, CONFIG):\n            print(\"\\t\" * indent + f\"{key}:\")\n            val.show(indent=indent + 1)\n        else:\n            _val = val.replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\") if isinstance(val, str) else val\n            print(\"\\t\" * indent + f\"{key}: \" + (f\"'{_val}'\" if isinstance(val, str) else f\"{val}\"))\n</code></pre>"},{"location":"reference/configs/#facesim3d.configs.CONFIG.update","title":"update","text":"<pre><code>update(new_configs: dict[str, Any]) -&gt; None\n</code></pre> <p>Update the config object with new entries.</p> Source code in <code>code/facesim3d/configs.py</code> <pre><code>def update(self, new_configs: dict[str, Any]) -&gt; None:\n    \"\"\"Update the config object with new entries.\"\"\"\n    for k, val in new_configs.items():\n        if isinstance(val, list | tuple):\n            setattr(self, k, [CONFIG(x) if isinstance(x, dict) else x for x in val])\n        else:\n            setattr(self, k, CONFIG(val) if isinstance(val, dict) else val)\n</code></pre>"},{"location":"reference/configs/#facesim3d.configs.CONFIG.update_paths","title":"update_paths","text":"<pre><code>update_paths(\n    parent_path: str | None = None,\n    for_logging: bool = False,\n)\n</code></pre> <p>Update relative paths to PROJECT_ROOT dir.</p> Source code in <code>code/facesim3d/configs.py</code> <pre><code>def update_paths(self, parent_path: str | None = None, for_logging: bool = False):\n    \"\"\"Update relative paths to PROJECT_ROOT dir.\"\"\"\n    # Use project root dir as the parent path if it is not specified\n    parent_path = self.PROJECT_ROOT if hasattr(self, \"PROJECT_ROOT\") else parent_path\n\n    if parent_path is not None:\n        parent_path = str(Path(parent_path).absolute())\n\n        for key, path in self.__dict__.items():\n            if isinstance(path, str) and not Path(path).is_absolute():\n                if for_logging and key != \"filename\":\n                    # In the case of logging configs, apply only on filename\n                    continue\n                self.__dict__.update({key: str(Path(parent_path).joinpath(path))})\n\n            elif isinstance(path, CONFIG):\n                path.update_paths(parent_path=parent_path, for_logging=for_logging)\n\n    else:\n        print(\"Paths can't be converted to absolute paths, since no PROJECT_ROOT is found!\")\n</code></pre>"},{"location":"reference/configs/#facesim3d.configs.update_logger_configs","title":"update_logger_configs","text":"<pre><code>update_logger_configs(\n    new_logger_name: str,\n    new_logger_filename: str | Path,\n    logger: Logger,\n) -&gt; Logger\n</code></pre> <p>Update logger name and filename.</p> <p>Parameters:</p> Name Type Description Default <code>new_logger_name</code> <code>str</code> <p>new logger name</p> required <code>new_logger_filename</code> <code>str | Path</code> <p>new logger filename</p> required <code>logger</code> <code>Logger</code> <p>updated logger object</p> required Source code in <code>code/facesim3d/configs.py</code> <pre><code>def update_logger_configs(\n    new_logger_name: str, new_logger_filename: str | Path, logger: logging.Logger\n) -&gt; logging.Logger:\n    \"\"\"\n    Update logger name and filename.\n\n    :param new_logger_name: new logger name\n    :param new_logger_filename: new logger filename\n    :param logger: updated logger object\n    \"\"\"\n    # Set new logger name\n    logger.name = new_logger_name\n\n    # Check filename\n    if not str(new_logger_filename).endswith(\".log\"):\n        msg = f\"Given filename '{new_logger_filename}' does not end with '.log'.\"\n        raise ValueError(msg)\n\n    # Create parent dirs\n    Path(new_logger_filename).parent.mkdir(parents=True, exist_ok=True)\n\n    # Overwrite logger\n    for handler in logger.handlers:\n        if isinstance(handler, logging.FileHandler):\n            new_file_handler = logging.FileHandler(filename=new_logger_filename)\n            new_file_handler.setFormatter(handler.formatter)\n            new_file_handler.setLevel(handler.level)\n            logger.removeHandler(handler)\n            logger.addHandler(new_file_handler)\n    return logger\n</code></pre>"},{"location":"reference/datachecks/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> datachecks","text":""},{"location":"reference/datachecks/#facesim3d.datachecks","title":"datachecks","text":"<p>Check data from the triplet-odd-one out task.</p> <p>This script can be run via the command line interface (CLI).</p> <p>What arguments can be passed in CLI?</p> <pre><code>python -m facesim3d.datachecks --help\n</code></pre> <p>Functions:</p> Name Description <code>check_catch_trials</code> <p>Check catch trials (i.e., those trials, which participants missed).</p> <code>check_monotonous_choices</code> <p>Check monotonous choices (i.e., participant chooses repeatedly same side (left, middle, right)).</p> <code>check_response_times</code> <p>Check for rapid response times.</p> <code>check_timeouts</code> <p>Check timeouts (i.e., participants do not make any choice in a trial).</p> <code>determine_optimal_set_of_triplets_for_resampling</code> <p>Determine an optimal set of triplets for resampling.</p> <code>determine_threshold_for_minimal_response_time</code> <p>Determine the minimum response time for the given session.</p> <code>display_choice_sides</code> <p>Display choice sides of a single participant.</p> <code>display_faces_of_catch_trials</code> <p>Display the faces of catch trials for given participant.</p> <code>display_faces_of_trial</code> <p>Display the faces of a single trial.</p> <code>estimate_remaining_costs</code> <p>Estimate the remaining costs for the experiment.</p> <code>explore_quality_score_of_single_participant</code> <p>Explore quality score of single participant.</p> <code>get_choice_side</code> <p>Get side of choice in a trial (left, middle, right).</p> <code>get_quality_score_table</code> <p>Compute participant bad quality score (BQS).</p> <code>get_response_time_stats</code> <p>Get response time statistics.</p> <code>main</code> <p>Run the main function to check response data.</p> <code>percentage_missed_n_catch_trials</code> <p>Compute the percentage of participants who missed at least <code>n</code> catch trials.</p> <code>plot_catch_counts</code> <p>Plot catch trial counts.</p> <code>plot_response_time_distribution</code> <p>Plot the response time distribution of a given set.</p> <code>print_ppid_list_for_bonus</code> <p>Print a line-separated list of ppid's to provide bonus payments on Prolific.</p> <code>print_ppid_list_to_accept</code> <p>Print a comma-separated list of ppid's, who are to be accepted on Prolific.</p> <code>print_ppid_list_to_block</code> <p>Print a comma separated list of ppid to block on Prolific.</p> <code>prob_catch_trials</code> <p>Compute the probability of catches equal or lower than r-times (assuming random choice behavior) -&gt; <code>P(X &lt;= r)</code>.</p> <code>review_prolific_participants</code> <p>Review participants for low quality data and make a decision for Prolific.</p> <code>update_block_list</code> <p>Update the blocklist.</p>"},{"location":"reference/datachecks/#facesim3d.datachecks.check_catch_trials","title":"check_catch_trials","text":"<pre><code>check_catch_trials(\n    trial_results_table: DataFrame, verbose: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Check catch trials (i.e., those trials, which participants missed).</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def check_catch_trials(trial_results_table: pd.DataFrame, verbose: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Check catch trials (i.e., those trials, which participants missed).\"\"\"\n    caught_table = trial_results_table[~trial_results_table.caught.isna()]\n    caught_table = caught_table[caught_table.caught == 1]\n\n    if verbose:\n        cprint(string=\"\\nCheck catch trials:\", fm=\"ul\")\n        print(caught_table[[\"ppid\", \"trial_num\", \"head_odd\", \"triplet\", \"caught\"]])\n\n    return caught_table\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.check_monotonous_choices","title":"check_monotonous_choices","text":"<pre><code>check_monotonous_choices(\n    trial_results_table: DataFrame,\n    set_nr: str,\n    verbose: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Check monotonous choices (i.e., participant chooses repeatedly same side (left, middle, right)).</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def check_monotonous_choices(trial_results_table: pd.DataFrame, set_nr: str, verbose: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Check monotonous choices (i.e., participant chooses repeatedly same side (left, middle, right)).\"\"\"\n    global __monoton_table, __prev_set_nr  # noqa: PLW0603\n    # Note: this assumes that each FLAGS.set_nr is run separately (i.e., starting the script freshly)\n    try:\n        # Check if table is cached as global variable (for multiple calls)\n        __monoton_table  # noqa: B018\n        if __prev_set_nr != set_nr:\n            msg = \"Previous set number is different from current set number.\"\n            raise NameError(msg)\n    except NameError as e:\n        # Iterate through all trials and save indices of monotonous choices (w.r.t. side)\n        print(e)\n        monotonous_indices = []  # init list of monotonous indices\n        cs_cnt = 0  # init choice side (cs) counter\n        prev_cs = None  # init previous choice side\n        prev_ppid = None  # init previous participant id (ppid)\n\n        for _i, row in tqdm(\n            iterable=trial_results_table.iterrows(), desc=\"Check monotonous choices\", total=len(trial_results_table)\n        ):\n            cs = get_choice_side(table_row=row, verbose=verbose)\n            ppid = row.ppid\n\n            if prev_cs is None or cs is None:\n                prev_cs = cs\n                prev_ppid = ppid\n                continue\n\n            # Check whether the previous choice side is equal to current choice side of same participant\n            if cs == prev_cs and ppid == prev_ppid and row.response_time &lt;= TH_MONO_RT:\n                cs_cnt += 1\n            else:\n                # Otherwise reset counter\n                cs_cnt = 0\n\n            # If there is monotonous choice behavior, add corresponding indices to the list\n            if cs_cnt == TH_MONO:\n                # Response time could be checked here as well, as an average over the previous trials fast\n                # responses with low variance could be a sign of an inattentive participant\n                monotonous_indices += list(range(_i + 1 - cs_cnt, _i + 1))\n            if cs_cnt &gt; TH_MONO:\n                monotonous_indices.append(_i)\n\n            prev_cs = cs\n            prev_ppid = ppid\n\n        # Extract monotonous trials\n        __monoton_table = trial_results_table.iloc[monotonous_indices]\n        __prev_set_nr = set_nr\n\n        if verbose:\n            cprint(\n                string=f\"\\nCheck monotonous choices (&gt;={TH_MONO}x same side, response time&lt;={TH_MONO_RT} sec):\",\n                fm=\"ul\",\n            )\n            print(__monoton_table[[\"ppid\", \"trial_num\", \"head_odd\", \"triplet\", \"response_time\"]])\n\n    return __monoton_table\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.check_response_times","title":"check_response_times","text":"<pre><code>check_response_times(\n    trial_results_table: DataFrame, verbose: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Check for rapid response times.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def check_response_times(trial_results_table: pd.DataFrame, verbose: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Check for rapid response times.\"\"\"\n    rt_table = trial_results_table[trial_results_table.response_time &lt;= TH_RT]\n\n    if verbose:\n        cprint(string=f\"\\nCheck response times (&lt;={TH_RT}sec):\", fm=\"ul\")\n        print(rt_table[[\"ppid\", \"trial_num\", \"response_time\"]])\n\n    return rt_table\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.check_timeouts","title":"check_timeouts","text":"<pre><code>check_timeouts(\n    trial_results_table: DataFrame, verbose: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Check timeouts (i.e., participants do not make any choice in a trial).</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def check_timeouts(trial_results_table: pd.DataFrame, verbose: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Check timeouts (i.e., participants do not make any choice in a trial).\"\"\"\n    rt_table = trial_results_table[trial_results_table.head_odd == 0]\n\n    if verbose:\n        cprint(string=\"\\nCheck timeouts:\", fm=\"ul\")\n        print(rt_table[[\"ppid\", \"trial_num\", \"head_odd\", \"response_time\"]])  # response_time &gt;= 10.\n\n    return rt_table\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.determine_optimal_set_of_triplets_for_resampling","title":"determine_optimal_set_of_triplets_for_resampling","text":"<pre><code>determine_optimal_set_of_triplets_for_resampling() -&gt; None\n</code></pre> <p>Determine an optimal set of triplets for resampling.</p> <p>The following points are desired: 1. to sample some heads / triplets multiple times (e.g., 10 times) to get an estimate of the noise in    terms of human similarity judgments 2. to employ triplets that were sampled already multiple times.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def determine_optimal_set_of_triplets_for_resampling() -&gt; None:\n    \"\"\"\n    Determine an optimal set of triplets for resampling.\n\n    The following points are desired:\n    1. to sample some heads / triplets multiple times (e.g., 10 times) to get an estimate of the noise in\n       terms of human similarity judgments\n    2. to employ triplets that were sampled already multiple times.\n    \"\"\"\n    n_triplets = np.math.comb(params.main.n_faces, 3)\n    triplet_tab = get_triplet_ids_and_heads(pilot=False)\n    assert len(triplet_tab) == n_triplets, f\"Expected {n_triplets} triplets, but got {len(triplet_tab)}\"  # noqa: S101\n\n    triplets_dict = {}\n    for sess in params.SESSIONS:\n        tr_table = read_trial_results_of_session(session=sess, clean_trials=True, verbose=False)[\n            [\n                \"triplet_id\",\n                # 'head1', 'head2', 'head3',\n                \"triplet\",\n                \"head_odd\",\n            ]\n        ]\n        tr_table = tr_table.astype({\"triplet_id\": int, \"head_odd\": int})\n        # tr_table = tr_table.astype({'triplet_id': int, 'head1': int, 'head2': int, 'head3': int})  # noqa: ERA001\n\n        tripl_val_ctn_tab = (\n            tr_table.value_counts()\n            .rename_axis([\"triplet_id\", \"triplet\", \"head_odd\"])\n            .reset_index(name=\"counts\")[[\"triplet_id\", \"triplet\", \"counts\"]]\n        )\n\n        # Compute the rate of consistency in the human similarity judgments within triplets\n        for _i, multi_triplet_tr in tr_table[\n            tr_table.triplet_id.isin(\n                tripl_val_ctn_tab[  # take triplets sampled multiple times\n                    tripl_val_ctn_tab.counts &gt; 2  # noqa: PLR2004\n                ].triplet_id\n            )\n        ].groupby(\"triplet_id\"):\n            print(multi_triplet_tr)\n            multi_triplet_tr.head_odd.nunique()\n            # TODO: how to compute? continue here ...  # noqa: FIX002\n            break\n\n        # Check whether all triplets were sampled at least once\n        assert (  # noqa: S101\n            len(tripl_val_ctn_tab) == n_triplets\n        ), f\"In {sess} session not all triplets were sampled at least once!\"\n\n        triplets_dict[sess] = tripl_val_ctn_tab\n\n    # Check which heads were sampled the most\n    # TODO: continue here  # noqa: FIX002\n    tv = triplets_dict[\"2D\"]\n    tv[\"heads\"] = tv.triplet.map(lambda x: x.split(\"_\"))\n    heads = []\n    for _i, row in tv.iterrows():\n        heads.extend(row.heads * row.counts)\n    heads = np.array(heads).astype(int)\n    print(pd.value_counts(heads).describe()[[\"min\", \"max\"]])\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.determine_threshold_for_minimal_response_time","title":"determine_threshold_for_minimal_response_time","text":"<pre><code>determine_threshold_for_minimal_response_time(\n    session: str, plot: bool = True, verbose: bool = True\n) -&gt; None\n</code></pre> <p>Determine the minimum response time for the given session.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def determine_threshold_for_minimal_response_time(session: str, plot: bool = True, verbose: bool = True) -&gt; None:\n    \"\"\"Determine the minimum response time for the given session.\"\"\"\n    tr_table = read_trial_results_of_session(session=session, clean_trials=True, verbose=verbose)\n\n    if verbose:\n        print(tr_table.response_time.describe())\n\n    rt_min = tr_table.response_time.min()\n    rt_max = tr_table.response_time.max()\n    rt_mean = tr_table.response_time.mean()\n    rt_median = tr_table.response_time.median()\n    rt_std = tr_table.response_time.std()\n    rt_quant_0_001 = tr_table.response_time.quantile(0.001)\n    rt_quant_0_002 = tr_table.response_time.quantile(0.002)\n\n    if plot:\n        _, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 8), num=f\"Response time distribution {session}\")  # fig = _\n        h = tr_table.response_time.hist(bins=10 * 10, ax=ax)\n        y_max = h.get_ylim()[1]\n        plt.vlines(x=rt_min, ymin=0, ymax=y_max, color=\"r\")\n        plt.text(x=rt_min, y=y_max, s=f\"min: {rt_min:.2f}s\", color=\"r\", rotation=45)\n\n        plt.vlines(x=rt_mean, ymin=0, ymax=y_max, color=\"r\")\n        plt.text(x=rt_mean, y=y_max, s=f\"mean: {rt_mean:.2f}s\", color=\"r\", rotation=45)\n\n        plt.vlines(x=rt_median, ymin=0, ymax=y_max, color=\"orange\")\n        plt.text(x=rt_median, y=y_max, s=f\"median: {rt_median:.2f}s\", color=\"orange\", rotation=45)\n\n        plt.vlines(x=rt_max, ymin=0, ymax=y_max, color=\"r\")\n        plt.text(x=rt_max, y=y_max, s=f\"max: {rt_max:.2f}s\", color=\"r\", rotation=45)\n\n        plt.vlines(x=rt_mean - rt_std, ymin=0, ymax=y_max, color=\"r\", linestyle=\"--\", alpha=0.5)\n        plt.text(x=rt_mean - rt_std, y=y_max, s=f\"mean-std: {rt_mean - rt_std:.2f}s\", color=\"r\", rotation=45)\n\n        plt.vlines(x=rt_mean + rt_std, ymin=0, ymax=y_max, color=\"r\", linestyle=\"--\", alpha=0.5)\n        plt.text(x=rt_mean + rt_std, y=y_max, s=f\"mean+std: {rt_mean + rt_std:.2f}s\", color=\"r\", rotation=45)\n\n        plt.vlines(x=rt_quant_0_001, ymin=0, ymax=y_max, color=\"pink\", linestyle=\"--\", alpha=1.0)\n        plt.text(x=rt_quant_0_001, y=y_max, s=f\"0.1% quantile: {rt_quant_0_001:.2f}s\", color=\"pink\", rotation=45)\n\n        plt.vlines(x=rt_quant_0_002, ymin=0, ymax=y_max, color=\"orchid\", linestyle=\"--\", alpha=1.0)\n        plt.text(x=rt_quant_0_002, y=y_max, s=f\"0.2% quantile: {rt_quant_0_002:.2f}s\", color=\"orchid\", rotation=45)\n        h.set(title=f\"Response time distribution for session {session} (cleaned data)\")\n        h.get_figure().tight_layout()\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.display_choice_sides","title":"display_choice_sides","text":"<pre><code>display_choice_sides(\n    ppid: str, trial_results_table: DataFrame\n) -&gt; ndarray\n</code></pre> <p>Display choice sides of a single participant.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def display_choice_sides(ppid: str, trial_results_table: pd.DataFrame) -&gt; np.ndarray:\n    \"\"\"Display choice sides of a single participant.\"\"\"\n    ppid_tab = trial_results_table.loc[trial_results_table.ppid == ppid]\n    ppid_tab = ppid_tab[~ppid_tab.keyPress.isna()].reset_index(drop=True)\n\n    ohe = OneHotEncoder(categories=[[\"LeftArrow\", \"DownArrow\", \"RightArrow\"]])\n    # Some participants used \"UpArrow\" instead of \"DownArrow\"\n    ppid_tab.keyPress = ppid_tab.keyPress.replace(\"UpArrow\", \"DownArrow\")\n\n    ohe.fit(ppid_tab.keyPress.to_numpy().reshape(-1, 1))\n\n    # Transform choices to one-hot encoding\n    return ohe.transform(ppid_tab.keyPress.to_numpy().reshape(-1, 1)).toarray()\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.display_faces_of_catch_trials","title":"display_faces_of_catch_trials","text":"<pre><code>display_faces_of_catch_trials(\n    ppid: str,\n    set_nr: str,\n    misses_only: bool = True,\n    verbose: bool = False,\n) -&gt; None\n</code></pre> <p>Display the faces of catch trials for given participant.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def display_faces_of_catch_trials(ppid: str, set_nr: str, misses_only: bool = True, verbose: bool = False) -&gt; None:\n    \"\"\"Display the faces of catch trials for given participant.\"\"\"\n    tr_table = read_trial_results_of_participant(ppid=ppid, clean_trials=False, verbose=verbose)\n    tr_table = tr_table[~tr_table.caught.isna()]  # remove trials w/o data\n    keep_session = f\"{ppid}_{set_infix(set_nr)}_trial_results\"  # keep data of Set-session only\n    tr_table = tr_table.loc[tr_table.ppid_session_dataname == keep_session]\n\n    catch_head_trial = 0.0\n    assert (  # noqa: S101\n        tr_table.catch_head != catch_head_trial\n    ).sum() &gt; 1, \"No catch trials found. Do not clean the trial results table before passing it to this function!\"\n\n    # Filter for catch trials\n    tr_table = tr_table.loc[tr_table.catch_head != catch_head_trial]\n\n    if misses_only:\n        tr_table = tr_table.loc[tr_table.caught]\n\n    # Display catch trials\n    for _i, table_row in tr_table.iterrows():\n        display_faces_of_trial(table_row=table_row, verbose=verbose)\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.display_faces_of_trial","title":"display_faces_of_trial","text":"<pre><code>display_faces_of_trial(\n    table_row: Series, verbose: bool = False\n) -&gt; None\n</code></pre> <p>Display the faces of a single trial.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def display_faces_of_trial(table_row: pd.Series, verbose: bool = False) -&gt; None:\n    \"\"\"Display the faces of a single trial.\"\"\"\n    if table_row[[\"head1\", \"head2\", \"head3\"]].isna().any():\n        cprint(string=f\"Invalid trial of ppid '{table_row.ppid}' (missing face(s)).\", col=\"r\")\n        return\n\n    faces = [f\"Head{int(h)}\" for h in table_row[[\"head1\", \"head2\", \"head3\"]]]\n    choice_side = get_choice_side(table_row=table_row, verbose=verbose)\n    catch_head_trial = 0.0\n    is_catch_trial = table_row.catch_head != catch_head_trial\n    rt = table_row.response_time\n\n    # Display faces\n    title = (\n        f\"PPID '{table_row.ppid}' | Triplet-ID: {int(table_row.triplet_id)} | RT = {rt:.2f} sec | \"\n        f\"trial: {table_row.trial_num}\"\n    )\n\n    color = \"green\"\n    if is_catch_trial and table_row.caught:\n        color = \"indianred\"  # in case of missed catch trial\n        title += \" | Caught\"\n\n    fig, axs = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True, num=title, figsize=(12, 4))\n    for i, ax in enumerate(axs.flatten()):\n        ax.imshow(\n            display_face(\n                head_id=faces[i], data_mode=\"3d-reconstructions\", interactive=False, show=False, verbose=verbose\n            )\n        )\n        ax.set_title(faces[i], color=color if i == choice_side else \"black\")\n        if i == choice_side:\n            for spine in ax.spines.values():\n                spine.set_edgecolor(color)\n                spine.set_linewidth(2)\n            ax.set_xticks([])\n            ax.set_yticks([])\n        else:\n            ax.axis(\"off\")\n    fig.suptitle(title)\n    fig.tight_layout()\n    fig.show()\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.estimate_remaining_costs","title":"estimate_remaining_costs","text":"<pre><code>estimate_remaining_costs(\n    in_euro: bool, verbose: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Estimate the remaining costs for the experiment.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def estimate_remaining_costs(in_euro: bool, verbose: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Estimate the remaining costs for the experiment.\"\"\"\n    # Prepare the path for the cost table\n    path_to_save = Path(\n        paths.data.MAIN,\n        \"costs\",\n        f\"{datetime.now().date()}_expected_remaining_costs{'_in_euro' if in_euro else ''}.csv\",\n    )\n    path_to_save.parent.mkdir(parents=True, exist_ok=True)\n    if path_to_save.exists():\n        cprint(string=f\"Cost table '{path_to_save}' already computed!\", col=\"g\")\n        return pd.read_csv(path_to_save)\n\n    cprint(string=\"\\nCompute expected remaining costs ...\\n\", col=\"b\", fm=\"bo\")\n\n    # Provide variables\n    # data_loss_per_catch = 1 / 3  # approximately 33%  # noqa: ERA001\n    n_trials_per_ppid = 180 - 3 * 3  # 180 trials - (3 blocks * 3 catch trials per block)\n    prolific_service_fee = 1 / 3  # 33 %\n    cost_per_ppid = 2.25 if in_euro else 1.94  # in \u20ac or in \u00a3\n    perc_bonus_payment = params.BONUS\n    non_bonus_sets = [2.0, 2.1, 2.2]\n\n    n_triplets = 0\n    n_unseen_triplets = 0\n    session_df = pd.DataFrame(\n        columns=[\n            \"n_triplets\",\n            \"n_seen_triplets\",\n            \"n_unseen_triplets\",\n            \"expected_n_trials\",\n            \"data_loss_triplets\",\n            \"ideal_n_ppid\",\n            \"total_expected_n_ppid_triplets\",\n            \"total_expected_n_ppid_trials\",\n            \"n_ppids_approved\",\n            \"data_loss_trials\",\n            \"expected_remaining_n_ppids_triplets\",\n            \"expected_remaining_n_ppids_trials\",\n            \"mean_costs_per_ppid\",\n            \"total_paid\",\n            \"total_expected_costs_triplets\",\n            \"expected_remaining_costs_triplets\",\n            \"total_expected_costs_trials\",\n            \"expected_remaining_costs_trials\",\n        ],\n        index=params.SESSIONS,\n    )  # init\n\n    for sess in params.SESSIONS:  # == [\"2D\", \"3D\"]\n        triplet_tab = get_current_state_of_triplets(session=sess, pilot=False, plot=True)\n        sess_n_triplets = len(triplet_tab)  # == np.math.comb(params.main.n_faces, 3) [assert]\n        sess_n_unseen_triplets = len(triplet_tab[triplet_tab.status != \"G\"])\n        session_df.loc[sess, [\"n_triplets\", \"n_seen_triplets\", \"n_unseen_triplets\"]] = [\n            sess_n_triplets,\n            sess_n_triplets - sess_n_unseen_triplets,\n            sess_n_unseen_triplets,\n        ]\n        n_triplets += sess_n_triplets\n        n_unseen_triplets += sess_n_unseen_triplets\n\n    # Ideal number of participants that are needed to complete the sampling\n    ideal_n_ppid = np.math.ceil(n_triplets / n_trials_per_ppid)\n    session_df.loc[:, \"ideal_n_ppid\"] = session_df.n_triplets.map(lambda x: np.math.ceil(x / n_trials_per_ppid))\n\n    # Estimate cost per participant\n    list_of_acquired_sets = get_list_of_acquired_sets()\n    n_all_accepted_ppid = 0\n    n_ppid_wo_data = 0  # accepted but no data\n    df_mean_catch = pd.DataFrame(\n        columns=[\"ppid\", \"set_nr\", \"n_blocks\", \"n_catches\", \"mean_catch_per_block\", \"lost_data\"]\n    )\n    df_cost_ppid = pd.DataFrame(columns=[\"ppid\", \"set_nr\", \"cost\"])\n    for set_nr in list_of_acquired_sets:\n        # Extract number of accepted/approved participants\n        ppid_table = read_prolific_participant_data(set_nr=set_nr)\n        ppid_approved = ppid_table[(ppid_table.Status == \"APPROVED\") | (ppid_table.decision == \"accept\")][\n            \"Participant id\"\n        ]\n        n_all_accepted_ppid += len(ppid_approved)\n\n        # Load trial results table\n        tr_table = read_trial_results_of_set(set_nr=set_nr, clean_trials=False, verbose=False)\n        tr_table = tr_table[~tr_table.caught.isna()]  # remove trials w/o data\n        tr_table = tr_table[tr_table.block_num &gt; 1]  # remove training trials\n\n        # Extract number of participants without data\n        ppid_wo_data = ppid_approved[~ppid_approved.isin(tr_table.ppid.unique())]\n        n_ppid_wo_data += len(ppid_wo_data)\n\n        # Filter rejected participants\n        tr_table = tr_table[tr_table.ppid.isin(ppid_approved)]\n\n        # Compute the average number of catches per block\n        for ppid, ppid_tr_table in tr_table.groupby(\"ppid\"):\n            n_blocks = ppid_tr_table.block_num.max() - 1\n            n_catches = np.minimum(ppid_tr_table.caught.sum(), 3)\n            loss = (ppid_tr_table.groupby(\"block_num\").agg({\"caught\": \"sum\"}) &gt; 0).mean().values[0]  # noqa: PD011\n            # this handles also cases where two catches are in one block and none in the other\n            m_catch_per_block = n_catches / n_blocks\n            df_mean_catch.loc[len(df_mean_catch), :] = [ppid, set_nr, n_blocks, n_catches, m_catch_per_block, loss]\n\n            ppid_cost = cost_per_ppid\n            if set_nr not in non_bonus_sets and n_catches == 0:\n                ppid_cost += cost_per_ppid * perc_bonus_payment\n            ppid_cost += ppid_cost * prolific_service_fee  # add service fee of Prolific\n            df_cost_ppid.loc[len(df_cost_ppid), :] = [ppid, set_nr, round(ppid_cost, 2)]\n\n        # Add data-loss &amp; costs of approved participants w/o trial data\n        min_cost_in_set = df_cost_ppid.cost.min()\n        for ppid in ppid_wo_data:\n            # Add data-loss in catch df\n            df_mean_catch.loc[len(df_mean_catch), :] = [ppid, set_nr, np.nan, np.nan, np.nan, 1.0]\n            # Add costs\n            df_cost_ppid.loc[len(df_cost_ppid), :] = [ppid, set_nr, min_cost_in_set]\n\n        # Compute data-loss based on the expected number of unique triplets in the set\n        tr_table = read_trial_results_of_set(set_nr=set_nr, clean_trials=True, verbose=False)\n        assert tr_table.equals(  # noqa: S101\n            tr_table[tr_table.ppid.isin(ppid_approved)]\n        ), f\"Trials results are not clean in Set{set_nr}\"\n        expected_n_triplets_in_set = len(ppid_approved) * n_trials_per_ppid\n        n_triplets_in_set = tr_table.triplet_id.nunique()\n        data_loss_in_set = 1 - n_triplets_in_set / expected_n_triplets_in_set\n        data_loss_in_set_wo_bad_trials = 1 - n_triplets_in_set / len(tr_table)\n\n        if verbose:\n            cprint(string=f\"\\nSet{set_nr}:\", fm=\"ul\")\n            print(f\"Data lost (on trials): {df_mean_catch[df_mean_catch.set_nr == set_nr].lost_data.mean():.1%}\")\n            print(f\"Data lost (on triplets): {data_loss_in_set:.1%}\")\n            print(f\"Data lost (on triplets) w/o bad trials: {data_loss_in_set_wo_bad_trials:.1%}\")\n            print(f\"Costs ({'\u20ac' if in_euro else '\u00a3'}): {df_cost_ppid[df_cost_ppid.set_nr == set_nr].cost.sum():.2f}\")\n            print(\n                f\"Number of approved participants: {len(ppid_approved)} \"\n                f\"({len(ppid_approved) / len(ppid_table):.0%} of ppids on Prolific)\"\n            )\n            print(\n                f\"Number of approved participants w/o data: {len(ppid_wo_data)} \"\n                f\"({len(ppid_wo_data) / len(ppid_approved):.0%} of approved ppids)\"\n            )\n            print(\n                f\"Average cost per ppid ({'\u20ac' if in_euro else '\u00a3'}): \"\n                f\"{df_cost_ppid[df_cost_ppid.set_nr == set_nr].cost.mean():.2f}\"\n            )\n\n    # Print summary over sessions\n    print()\n    print(\"o*+*\" * 20)\n    for sess in params.SESSIONS:\n        print(sess.center(80 // len(params.SESSIONS)), end=\"\")\n    print()\n    print(\"o*+*\" * 20, \"\\n\")\n\n    cprint(string=f\"\\nNumber of approved ppids: {n_all_accepted_ppid}\", col=\"g\")  # == len(df_cost_ppid)\n    df_cost_ppid[\"session\"] = df_cost_ppid.set_nr.apply(lambda x: x.split(\".\")[0] + \"D\")\n    for sess in params.SESSIONS:\n        session_df.loc[sess, \"n_ppids_approved\"] = len(df_cost_ppid[df_cost_ppid.session == sess])\n        cprint(string=f\"\u2022 {sess}: {int(session_df.loc[sess, 'n_ppids_approved'])}\", col=\"g\")\n\n    # Data loss on triplets\n    expected_n_trials = n_all_accepted_ppid * n_trials_per_ppid\n    n_seen_triplets = n_triplets - n_unseen_triplets\n    perc_data_loss_triplets = 1 - (n_seen_triplets / expected_n_trials)\n    print(f\"Percentage of data loss (on triplets): {perc_data_loss_triplets:.2%}\")\n    for sess in params.SESSIONS:\n        session_df.loc[sess, \"expected_n_trials\"] = session_df.loc[sess, \"n_ppids_approved\"] * n_trials_per_ppid\n        sess_perc_data_loss_triplets = 1 - (\n            session_df.loc[sess, \"n_seen_triplets\"] / session_df.loc[sess, \"expected_n_trials\"]\n        )\n        session_df.loc[sess, \"data_loss_triplets\"] = sess_perc_data_loss_triplets\n        print(f\"\u2022 in {sess}: {sess_perc_data_loss_triplets:.2%}\")\n    # perc_data_loss_triplets \u2248 session_df.data_loss_triplets.mean()\n\n    # Data loss based on trials\n    perc_data_loss_trials = df_mean_catch.lost_data.mean()\n    print(f\"Percentage of data loss (on trials): {perc_data_loss_trials:.2%}\")\n    df_mean_catch[\"session\"] = df_mean_catch.set_nr.apply(lambda x: x.split(\".\")[0] + \"D\")\n    for sess in params.SESSIONS:\n        sess_perc_data_loss_trials = df_mean_catch[df_mean_catch.session == sess].lost_data.mean()\n        session_df.loc[sess, \"data_loss_trials\"] = sess_perc_data_loss_trials\n        print(f\"\u2022 in {sess}: {sess_perc_data_loss_trials:.2%}\")\n\n    # Compute the required number of participants to finalize sampling based on DynamoDB triplet table\n    total_expected_n_ppid_triplets = np.math.ceil(ideal_n_ppid / (1 - perc_data_loss_triplets))\n    expected_remaining_n_ppids_triplets = np.math.ceil(\n        (n_unseen_triplets / n_trials_per_ppid) / (1 - perc_data_loss_triplets)\n    )\n    for sess in params.SESSIONS:\n        session_df.loc[sess, \"total_expected_n_ppid_triplets\"] = np.math.ceil(\n            session_df.loc[sess, \"ideal_n_ppid\"] / (1 - session_df.loc[sess, \"data_loss_triplets\"])\n        )\n        session_df.loc[sess, \"expected_remaining_n_ppids_triplets\"] = np.math.ceil(\n            (session_df.loc[sess, \"n_unseen_triplets\"] / n_trials_per_ppid)\n            / (1 - session_df.loc[sess, \"data_loss_triplets\"])\n        )\n\n    # Compute the required number of participants to finalize sampling\n    total_expected_n_ppid_trials = np.math.ceil(ideal_n_ppid / (1 - perc_data_loss_trials))\n    for sess in params.SESSIONS:\n        session_df.loc[sess, \"total_expected_n_ppid_trials\"] = np.math.ceil(\n            session_df.loc[sess, \"ideal_n_ppid\"] / (1 - session_df.loc[sess, \"data_loss_trials\"])\n        )\n\n    print(f\"Total number of ppids needed (adjusted for data loss on triplets): {total_expected_n_ppid_triplets}\")\n    for sess in params.SESSIONS:\n        print(f\"\u2022 {sess}: {int(session_df.loc[sess, 'total_expected_n_ppid_triplets'])}\")\n    print(f\"Total number of ppids needed (adjusted for data loss on trials): {total_expected_n_ppid_trials}\")\n    for sess in params.SESSIONS:\n        print(f\"\u2022 {sess}: {int(session_df.loc[sess, 'total_expected_n_ppid_trials'])}\")\n\n    cprint(\n        string=f\"Expected remaining number of ppids needed (based on triplets): {expected_remaining_n_ppids_triplets}\",\n        col=\"r\",\n        fm=\"bo\",\n    )\n    cprint(\n        string=f\"Expected remaining number of ppids needed (based on trials): \"\n        f\"{total_expected_n_ppid_trials - n_all_accepted_ppid}\",\n        col=\"y\",\n        fm=\"bo\",\n    )\n    # total_expected_n_ppid_triplets - n_all_accepted_ppid\n    for sess in params.SESSIONS:\n        cprint(string=f\"\u2022 {sess}: {int(session_df.loc[sess, 'expected_remaining_n_ppids_triplets'])}\", col=\"r\")\n        session_df.loc[sess, \"expected_remaining_n_ppids_trials\"] = (\n            session_df.loc[sess, \"total_expected_n_ppid_trials\"] - session_df.loc[sess, \"n_ppids_approved\"]\n        )\n        cprint(string=f\"\u2022 {sess}: {int(session_df.loc[sess, 'expected_remaining_n_ppids_trials'])}\", col=\"y\")\n\n    # Compute costs\n    mean_cost_per_ppid = df_cost_ppid.cost.mean()\n    print(f\"\\nAverage cost per ppid ({'\u20ac' if in_euro else '\u00a3'}): {mean_cost_per_ppid:.2f}\")\n    for sess in params.SESSIONS:\n        session_df.loc[sess, \"mean_costs_per_ppid\"] = df_cost_ppid[df_cost_ppid.session == sess].cost.mean()\n        print(f\"\u2022 {sess}: {session_df.loc[sess, 'mean_costs_per_ppid']:.2f}\")\n\n    if verbose:\n        # Percentage of participants that get bonus\n        perc_all_pass = len(df_mean_catch[df_mean_catch.n_catches == 0]) / len(df_mean_catch)\n        print(f\"Participants who get bonus (no-catch): {perc_all_pass:.2%}\")\n        for sess in params.SESSIONS:\n            sess_perc_all_pass = len(\n                df_mean_catch[(df_mean_catch.session == sess) &amp; (df_mean_catch.n_catches == 0)]\n            ) / len(df_mean_catch[df_mean_catch.session == sess])\n            print(f\" \u2022 {sess}: {sess_perc_all_pass:.2%}\")\n\n    paid_costs = df_cost_ppid.cost.sum()\n    cprint(string=f\"Total paid costs ({'\u20ac' if in_euro else '\u00a3'}): {paid_costs:.2f}\", col=\"g\")\n    for sess in params.SESSIONS:\n        session_df.loc[sess, \"total_paid\"] = df_cost_ppid[df_cost_ppid.session == sess].cost.sum()\n        # == session_df.n_ppids_approved * session_df.mean_costs_per_ppid\n        cprint(string=f\"\u2022 {sess}: {session_df.loc[sess, 'total_paid']:.2f}\", col=\"g\")\n\n    # Compute expected total remaining costs\n    total_expected_costs_triplets = mean_cost_per_ppid * total_expected_n_ppid_triplets\n    total_expected_costs_trials = mean_cost_per_ppid * total_expected_n_ppid_trials\n    print(\n        f\"Total expected costs (based on triplets) ({'\u20ac' if in_euro else '\u00a3'}): \"\n        f\"{total_expected_costs_triplets:.2f}\"\n    )\n    for sess in params.SESSIONS:\n        session_df.loc[sess, \"total_expected_costs_triplets\"] = (\n            session_df.loc[sess, \"total_expected_n_ppid_triplets\"] * session_df.loc[sess, \"mean_costs_per_ppid\"]\n        )\n        print(f\"\u2022 {sess}: {session_df.loc[sess, 'total_expected_costs_triplets']:.2f}\")\n\n    print(f\"Total expected costs (based on trials) ({'\u20ac' if in_euro else '\u00a3'}): {total_expected_costs_trials:.2f}\")\n    for sess in params.SESSIONS:\n        session_df.loc[sess, \"total_expected_costs_trials\"] = (\n            session_df.loc[sess, \"total_expected_n_ppid_trials\"] * session_df.loc[sess, \"mean_costs_per_ppid\"]\n        )\n        print(f\"\u2022 {sess}: {session_df.loc[sess, 'total_expected_costs_trials']:.2f}\")\n\n    cprint(\n        string=f\"Remaining expected costs (based on triplets) ({'\u20ac' if in_euro else '\u00a3'}): \"\n        f\"{expected_remaining_n_ppids_triplets * mean_cost_per_ppid:.2f}\",\n        col=\"r\",\n        fm=\"bo\",\n    )\n    cprint(\n        string=f\"Remaining expected costs (based on trials) ({'\u20ac' if in_euro else '\u00a3'}): \"\n        f\"{total_expected_costs_trials - paid_costs:.2f}\",\n        col=\"y\",\n        fm=\"bo\",\n    )\n    for sess in params.SESSIONS:\n        expected_remaining_costs_triplets = round(\n            session_df.loc[sess, \"expected_remaining_n_ppids_triplets\"] * session_df.loc[sess, \"mean_costs_per_ppid\"],\n            2,\n        )\n        session_df.loc[sess, \"expected_remaining_costs_triplets\"] = expected_remaining_costs_triplets\n        cprint(string=f\"\u2022 {sess}: {expected_remaining_costs_triplets:.2f}\", col=\"r\")\n\n        expected_remaining_costs_trials = round(\n            session_df.loc[sess, \"expected_remaining_n_ppids_trials\"] * session_df.loc[sess, \"mean_costs_per_ppid\"], 2\n        )\n        session_df.loc[sess, \"expected_remaining_costs_trials\"] = expected_remaining_costs_trials\n        cprint(string=f\"\u2022 {sess}: {expected_remaining_costs_trials:.2f}\", col=\"y\")\n\n    print()\n    print(\"o*+*\" * 20)\n    print(\"-\" * 80)\n    print(\"o*+*\" * 20)\n\n    # Save\n    session_df.to_csv(path_to_save)\n\n    return session_df\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.explore_quality_score_of_single_participant","title":"explore_quality_score_of_single_participant","text":"<pre><code>explore_quality_score_of_single_participant(\n    ppid: str, set_nr: str, trial_results_table: DataFrame\n) -&gt; None\n</code></pre> <p>Explore quality score of single participant.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def explore_quality_score_of_single_participant(ppid: str, set_nr: str, trial_results_table: pd.DataFrame) -&gt; None:\n    \"\"\"Explore quality score of single participant.\"\"\"\n    qs_table = get_quality_score_table(set_nr=set_nr, save=False, verbose=False)\n\n    if not qs_table.loc[qs_table.ppid == ppid].empty:\n        print(\"\\n\" + \"-*o*\" * 15)\n        cprint(string=f\"\\nQuality scores of ppid '{ppid}':\", col=\"g\", fm=\"ul\")\n        print(qs_table.loc[qs_table.ppid == ppid, qs_table.columns[1:]])\n    else:\n        cprint(string=f\"Participant '{ppid}' not found in quality score table.\", col=\"r\")\n        return\n\n    tab = check_catch_trials(trial_results_table=trial_results_table, verbose=False)\n    if not tab.loc[tab.ppid == ppid].empty:\n        print(\n            \"\\nMissed catch trials:\\n\",\n            tab.loc[tab.ppid == ppid][[\"trial_num\", \"response_time\", \"keyPress\", \"caught\"]],\n        )\n\n    tab = check_monotonous_choices(trial_results_table=trial_results_table, set_nr=set_nr, verbose=False)\n    if not tab.loc[tab.ppid == ppid].empty:\n        print(\"\\nMonotonous choices:\\n\", tab.loc[tab.ppid == ppid][[\"trial_num\", \"keyPress\"]])\n\n    tab = check_timeouts(trial_results_table=trial_results_table, verbose=False)\n    if not tab.loc[tab.ppid == ppid].empty:\n        tab = tab.loc[tab.ppid == ppid].loc[tab.trial_num &gt; params.n_train_trials]\n        print(\"\\nTimeouts:\\n\", tab[[\"trial_num\", \"response_time\"]])\n\n    tab = check_response_times(trial_results_table=trial_results_table, verbose=False)\n    if not tab.loc[tab.ppid == ppid].empty:\n        print(\"\\nRapid responses:\\n\", tab.loc[tab.ppid == ppid][[\"trial_num\", \"response_time\", \"keyPress\"]])\n\n    print(\"\\n\" + \"-*o*\" * 15)\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.get_choice_side","title":"get_choice_side","text":"<pre><code>get_choice_side(\n    table_row: Series, verbose: bool = True\n) -&gt; int | None\n</code></pre> <p>Get side of choice in a trial (left, middle, right).</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def get_choice_side(table_row: pd.Series, verbose: bool = True) -&gt; int | None:\n    \"\"\"Get side of choice in a trial (left, middle, right).\"\"\"\n    # Could include new keypress column in TrialResults table\n    head_cols = [\"head1\", \"head2\", \"head3\", \"head_odd\"]\n    keypress_dict = {\"LeftArrow\": 0, \"DownArrow\": 1, \"UpArrow\": 1, \"RightArrow\": 2}\n    if table_row.keyPress in keypress_dict:\n        # keyPress information present\n        return keypress_dict[table_row.keyPress]\n\n    if not table_row[head_cols].isna().all():\n        side = 0  # init\n        max_n_sides = 3\n        while side &lt; max_n_sides:\n            if table_row[head_cols][side] == table_row[head_cols][-1]:\n                return side  # 0: left, 1: middle, 2: right\n            side += 1\n\n    if (\n        verbose\n        and table_row.head_odd != 0\n        and not np.isnan(table_row.head_odd)\n        and str(int(table_row.head_odd)) not in table_row.triplet\n    ):\n        # print only if choice was made and triplet does not contain head_odd\n        cprint(string=f\"Something is wrong in given table row:\\n{table_row[head_cols]}\", col=\"r\")\n        print(table_row)\n    return None\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.get_quality_score_table","title":"get_quality_score_table","text":"<pre><code>get_quality_score_table(\n    set_nr: str | None = None,\n    trial_results_table: DataFrame | None = None,\n    save: bool = False,\n    verbose: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Compute participant bad quality score (BQS).</p> <p>This is an aggregate score of the (bad) quality in behavioral responses of single participants.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def get_quality_score_table(\n    set_nr: str | None = None,\n    trial_results_table: pd.DataFrame | None = None,\n    save: bool = False,\n    verbose: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute participant bad quality score (BQS).\n\n    This is an aggregate score of the (bad) quality in behavioral responses of single participants.\n    \"\"\"\n    # Read the main results table\n    save_path = Path(paths.data.main.qc, f\"bad_quality_scores_Set{set_nr}.csv\")\n    if trial_results_table is None:\n        trial_results_table = read_trial_results_of_set(set_nr=set_nr, clean_trials=False, verbose=verbose)\n\n        # Check for existing table\n        if not save and save_path.exists():  # if save is True, (re-)compute the table\n            bqs_tab = pd.read_csv(save_path)\n            if set(bqs_tab.ppid) == set(trial_results_table.ppid):\n                # Only return table if it contains all participants (otherwise compute scores below)\n                if verbose:\n                    print(\"Found existing table.\")\n                return bqs_tab\n\n    else:\n        save = False\n        set_nr = f\"{np.random.randint(low=4, high=10)}.{np.random.randint(low=4, high=10)}\"\n        # placeholder set number\n\n    # Get tables of suspicious trials\n    c_tab = check_catch_trials(trial_results_table=trial_results_table, verbose=verbose)\n    m_tab = check_monotonous_choices(trial_results_table=trial_results_table, set_nr=set_nr, verbose=verbose)\n    t_tab = check_timeouts(trial_results_table=trial_results_table, verbose=verbose)\n    r_tab = check_response_times(trial_results_table=trial_results_table, verbose=verbose)\n\n    # Compute quality score per participant\n    bqs_tab = pd.DataFrame(  # init table w/ aggregate bad quality scores (bqs)\n        columns=[\"ppid\", \"catch_bqs\", \"mono_bqs\", \"timeout_bqs\", \"response_time_bqs\", \"BQS\"]\n    )\n    # Add bqs per criterion in table\n    for idx, ppid in enumerate(trial_results_table.ppid.unique()):\n        bqs_tab.loc[idx, :] = ppid, *[0] * (len(bqs_tab.columns) - 1)  # init participant row\n\n        # Check catch trials\n        if c_tab.ppid.str.contains(ppid).any():\n            # The higher number of missed catch trials leads to a higher bad quality score (bqs)\n            p_c_tab = c_tab.loc[c_tab.ppid == ppid]\n            p_c_tab = p_c_tab[p_c_tab.trial_num &gt; params.n_train_trials]  # exclude training trials\n            p_bqs = len(p_c_tab)\n            bqs_tab.loc[idx, \"catch_bqs\"] += p_bqs * W_CATCH  # each criterion is weighted by its corresponding weight\n\n        # Check monotonous choices\n        if m_tab.ppid.str.contains(ppid).any():\n            # The longer the monotonous choice behavior, the higher the bad quality score is\n            p_m_tab = m_tab.loc[m_tab.ppid == ppid]\n            p_bqs = len(p_m_tab) * W_MONO\n\n            # Add additional bqs for **fast** monotonic responses\n            prev_tn = None  # init previous trial number\n            sj = p_m_tab.index[0]  # init start index of monotonic choice behavior\n            for j, tn in p_m_tab.trial_num.iteritems():\n                if tn - 1 != prev_tn and prev_tn is not None:\n                    if p_m_tab.loc[sj : j - 1, \"response_time\"].mean() &lt; TH_MONO_FAST_RT:\n                        # Participants choose the same side relatively quickly\n                        p_bqs += 1 * W_MONO_RT\n                    # this measure could include standard deviation of response times\n                    # if p_m_tab.loc[sj:j, \"response_time\"].std() &lt; .25:  # define threshold\n                    #     p_bqs += 1  # noqa: ERA001\n\n                    sj = j  # update start index\n                prev_tn = tn  # update previous trial number\n\n            # Add bad quality score to table\n            bqs_tab.loc[idx, \"mono_bqs\"] += p_bqs\n\n        # Check timeouts\n        if t_tab.ppid.str.contains(ppid).any():\n            # Higher number of timeouts leads to a higher bad quality score (bqs)\n            p_t_tab = t_tab.loc[t_tab.ppid == ppid]\n            p_t_tab = p_t_tab[p_t_tab.trial_num &gt; params.n_train_trials]  # exclude training trials\n\n            # Check how many timeouts\n            p_bqs = len(p_t_tab) * W_TO\n\n            # Could do a measure of whether timeouts happen in consecutive trials?\n            pass\n\n            bqs_tab.loc[idx, \"timeout_bqs\"] += p_bqs\n\n        # Check response times\n        if r_tab.ppid.str.contains(ppid).any():\n            # High numbers of rapid responses could indicate inattentiveness or sloppy behavior\n            p_r_tab = r_tab.loc[r_tab.ppid == ppid]\n\n            # Check how often this happens for this ppid\n            p_bqs = len(p_r_tab.response_time) * W_RT\n\n            # Check whether this happens in consecutive trials?\n            prev_tn = None  # init previous trial number\n            sj = p_r_tab.index[0]  # init start index of rapid choice behavior\n            for j, tn in p_r_tab.trial_num.iteritems():\n                if tn - 1 != prev_tn and prev_tn is not None:\n                    if len(p_r_tab.loc[sj : j - 1]) &gt;= TH_CONSEC_RT:\n                        # Participants made consecutively rapid choices\n                        p_bqs += len(p_r_tab.loc[sj : j - 1]) * W_CONSEC_RT\n                    sj = j  # update start index\n                prev_tn = tn  # update previous trial number\n\n            bqs_tab.loc[idx, \"response_time_bqs\"] += p_bqs\n\n    # Compute aggregate BQS\n    bqs_tab.loc[:, \"BQS\"] = bqs_tab.loc[:, bqs_tab.columns[1:-1]].sum(axis=1)\n    for col in bqs_tab.columns:\n        if col != \"ppid\":\n            bqs_tab[col] = bqs_tab[col].map(\"{:,.2f}\".format).astype(float)\n\n    if verbose:\n        # The lower the quality score is, the better the participant data.\n        print(\"Participants bad quality score (BQS):\\n\", bqs_tab)\n\n    if save:\n        # Save table\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        bqs_tab.to_csv(save_path, index=False)\n\n    return bqs_tab\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.get_response_time_stats","title":"get_response_time_stats","text":"<pre><code>get_response_time_stats(\n    trial_results_table: DataFrame,\n    include_pilot: bool = True,\n) -&gt; Series\n</code></pre> <p>Get response time statistics.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def get_response_time_stats(trial_results_table: pd.DataFrame, include_pilot: bool = True) -&gt; pd.Series:\n    \"\"\"Get response time statistics.\"\"\"\n    if include_pilot:\n        # Aggregate RT stats\n        tr_table_pilot = read_pilot_data(clean_trials=True, verbose=False)  # for some comparisons\n        return trial_results_table.response_time.append(tr_table_pilot.response_time).describe()\n\n    return trial_results_table.response_time.describe()\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Run the main function to check response data.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def main():\n    \"\"\"Run the main function to check response data.\"\"\"\n    # Check data of given Set\n    cprint(string=\"\\n\" + \"*\" * 26 + f\"\\nChecking data of 'Set{FLAGS.set_nr}':\\n\" + \"*\" * 26 + \"\\n\", col=\"b\", fm=\"ul\")\n\n    # Identify potential low quality data\n    # read trial results table\n    tr_table = read_trial_results_of_set(set_nr=FLAGS.set_nr, clean_trials=False, verbose=True)\n    # p_table, p2_p_table = read_prolific_participant_data(set_nr=FLAGS.set_nr, return_path=True)  # noqa: ERA001\n\n    # Check how many subjects have missed at least one catch trial\n    percentage_missed_n_catch_trials(trial_results_table=tr_table, n=1)\n\n    if FLAGS.plot_catch:\n        plot_catch_counts(trial_results_table=tr_table, set_nr=FLAGS.set_nr, save=True)\n\n    if FLAGS.plot_rt:\n        plot_response_time_distribution(trial_results_table=tr_table, compare_pilot=True, set_nr=FLAGS.set_nr)\n\n    # Review participants\n    if FLAGS.review:\n        # Compute quality checks and save them in a table\n        cprint(string=\"Participants bad quality score (BQS):\", col=\"y\", fm=\"ul\")\n        print(\n            bqs_table := get_quality_score_table(\n                set_nr=FLAGS.set_nr, save=FLAGS.save_bqs_table, verbose=FLAGS.save_bqs_table\n            )\n        )\n\n        cprint(\n            string=f\"\\nExplore {len(bqs_table.loc[bqs_table.BQS &gt;= BQS_TH].ppid)} single participants with \"\n            f\"very high 'bad quality scores' (BQS):\",\n            col=\"b\",\n            fm=\"bo\",\n        )\n\n        # Review participants with high BQS\n        review_prolific_participants(\n            set_nr=FLAGS.set_nr,\n            ppids_to_review=bqs_table.loc[bqs_table.BQS &gt;= BQS_TH].ppid.to_list(),\n            plot_rt=FLAGS.plot_rt,\n            accept_at_bqs=None,\n        )\n\n        # Review single participants with low BQS (i.e., good quality)\n        cprint(\n            string=f\"\\nExplore {len(bqs_table.loc[bqs_table.BQS &lt; BQS_TH].ppid)} single participants with \"\n            f\"low quality scores (BQS):\",\n            col=\"b\",\n            fm=\"bo\",\n        )\n        review_prolific_participants(\n            set_nr=FLAGS.set_nr,\n            ppids_to_review=bqs_table.loc[bqs_table.BQS &lt; BQS_TH].ppid.to_list(),\n            plot_rt=False,\n            accept_at_bqs=ACCEPT_AT_BQS,\n        )\n\n        # Review hand selected participants\n        review_prolific_participants(set_nr=FLAGS.set_nr, plot_rt=True, force_review=True, ppids_to_review=[])\n\n        # Review participants which are left over (usually ppids w/o trial data)\n        p_table = read_prolific_participant_data(set_nr=FLAGS.set_nr)\n\n        ppids_wo_review = p_table[(p_table.Status == \"AWAITING REVIEW\") &amp; (p_table.decision.isna())][\"Participant id\"]\n        cprint(string=f\"\\nFollowing {len(ppids_wo_review)} participant(s) are left over to review:\", col=\"b\", fm=\"bo\")\n        print(ppids_wo_review, \"\\n\")\n\n        review_prolific_participants(\n            set_nr=FLAGS.set_nr, plot_rt=True, force_review=True, ppids_to_review=ppids_wo_review.to_list()\n        )\n\n        # Compute comma separated (or new line) list of ppid to accept on Prolific\n        print_ppid_list_to_accept(set_nr=FLAGS.set_nr)\n\n        # Compile list for bonus payment\n        min_bonus = 0.0\n        if min_bonus &lt; params.BONUS:\n            print_ppid_list_for_bonus(trial_results_table=tr_table, set_nr=FLAGS.set_nr, bonus=params.BONUS)\n\n        # Compile blocklist\n        print_ppid_list_to_block(set_nr=FLAGS.set_nr)\n\n    # Compute expected costs\n    if FLAGS.costs:\n        estimate_remaining_costs(in_euro=False, verbose=True)\n        if not FLAGS.triplet_table:\n            finalized_triplets(session=FLAGS.set_nr[0] + \"D\")\n\n    if FLAGS.triplet_table:\n        current_session = FLAGS.set_nr[0] + \"D\"\n        update_triplet_table_on_dynamodb(\n            session=current_session,\n            set_finalised_triplets_to_g=False,  # toggle manually if requested\n            delete_done_triplets=False,\n        )\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.percentage_missed_n_catch_trials","title":"percentage_missed_n_catch_trials","text":"<pre><code>percentage_missed_n_catch_trials(\n    trial_results_table: DataFrame, n: int = 1\n) -&gt; None\n</code></pre> <p>Compute the percentage of participants who missed at least <code>n</code> catch trials.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def percentage_missed_n_catch_trials(trial_results_table: pd.DataFrame, n: int = 1) -&gt; None:\n    \"\"\"Compute the percentage of participants who missed at least `n` catch trials.\"\"\"\n    catch_tab = check_catch_trials(trial_results_table=trial_results_table, verbose=False)\n\n    m_ppid_catch = catch_tab[catch_tab.block_num &gt; n].ppid.nunique()\n    cprint(\n        string=f\"{m_ppid_catch / trial_results_table.ppid.nunique():.2%} ({m_ppid_catch} out of \"\n        f\"{trial_results_table.ppid.nunique()}) participants have missed at least {n} catch trial \"\n        f\"in the main phase of the experiment.\",\n        col=\"y\",\n    )\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.plot_catch_counts","title":"plot_catch_counts","text":"<pre><code>plot_catch_counts(\n    trial_results_table: DataFrame,\n    set_nr: str,\n    exclude_training: bool = True,\n    save: bool = False,\n) -&gt; None\n</code></pre> <p>Plot catch trial counts.</p> <p>Parameters:</p> Name Type Description Default <code>trial_results_table</code> <code>DataFrame</code> <p>Table with trial results from the main study</p> required <code>set_nr</code> <code>str</code> <p>Set number (2D: 2.0, 2.1, ... | 3D: 3.0, 3.1, ...) [str]</p> required <code>exclude_training</code> <code>bool</code> <p>Exclude training trials</p> <code>True</code> <code>save</code> <code>bool</code> <p>whether to save plot</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def plot_catch_counts(\n    trial_results_table: pd.DataFrame, set_nr: str, exclude_training: bool = True, save: bool = False\n) -&gt; None:\n    \"\"\"\n    Plot catch trial counts.\n\n    :param trial_results_table: Table with trial results from the main study\n    :param set_nr: Set number (2D: 2.0, 2.1, ... | 3D: 3.0, 3.1, ...) [str]\n    :param exclude_training: Exclude training trials\n    :param save: whether to save plot\n    :return: None\n    \"\"\"\n    plt.figure(\n        num=f\"Number of missed catch trials in Set{set_nr}\" + (\" excluding training\" if exclude_training else \"\"),\n        figsize=(8, 6),\n    )\n    catch_table = trial_results_table[trial_results_table.block_num &gt; exclude_training]\n    catch_table = catch_table[~catch_table.caught.isna()]  # Remove trials w/o data\n    catch_table = catch_table.astype({\"caught\": bool})\n    n_catches = catch_table.groupby(\"ppid\").sum().caught\n    if exclude_training:\n        n_catches = n_catches.map(lambda x: np.minimum(x, 3))  # cannot be more than 3\n    h = sns.histplot(\n        x=n_catches,\n        discrete=True,\n        bins=n_catches.nunique(),\n        shrink=0.6,\n        color=\"salmon\",\n        binrange=(n_catches.min(), n_catches.max()),\n    )\n\n    # The following subjects have more than three catch trials missed:\n    # catch_table.groupby(\"ppid\").sum()[catch_table.groupby(\"ppid\").sum().caught&gt;3][[\"correct\", \"caught\"]]  # noqa: E501, ERA001\n    # Probably, after 3rd catch and an automatic stop of the experiment, the following catch trials were missed, too,\n    # automatically.\n\n    h.get_figure().tight_layout()\n\n    if save:\n        h.get_figure().savefig(Path(paths.data.main.qc, f\"Set{set_nr}_missed_catch_trials.png\"), dpi=300)\n        plt.close()\n    else:\n        plt.show()\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.plot_response_time_distribution","title":"plot_response_time_distribution","text":"<pre><code>plot_response_time_distribution(\n    trial_results_table: DataFrame,\n    set_nr: str,\n    compare_pilot: bool = True,\n) -&gt; None\n</code></pre> <p>Plot the response time distribution of a given set.</p> <p>Parameters:</p> Name Type Description Default <code>trial_results_table</code> <code>DataFrame</code> <p>table with trial results from the main study</p> required <code>set_nr</code> <code>str</code> <p>Set number (2D: 2.0, 2.1, ... | 3D: 3.0, 3.1, ...) [str]</p> required <code>compare_pilot</code> <code>bool</code> <p>Add pilot data in the (aggregated) response time stats, and in the distribution plot</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>response time statistics</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def plot_response_time_distribution(\n    trial_results_table: pd.DataFrame, set_nr: str, compare_pilot: bool = True\n) -&gt; None:\n    \"\"\"\n    Plot the response time distribution of a given set.\n\n    :param trial_results_table: table with trial results from the main study\n    :param set_nr: Set number (2D: 2.0, 2.1, ... | 3D: 3.0, 3.1, ...) [str]\n    :param compare_pilot: Add pilot data in the (aggregated) response time stats, and in the distribution plot\n    :return: response time statistics\n    \"\"\"\n    # TODO: create session (2D, 3D) specific plots (for both pilot and main data that are compared)  # noqa: FIX002\n    #  Session can be inferred from set_nr\n    # Get path specifics for given Set\n    _, p2_p_table = read_prolific_participant_data(set_nr=set_nr, return_path=True)\n\n    # Plot RT distribution\n    p2_rt_dist = Path(\n        paths.data.main.qc, f\"{p2_p_table.split('/')[-1].split('_')[0]}_Set{set_nr}_response_time_distributions.png\"\n    )\n    if p2_rt_dist.exists():\n        # Open image\n        Image.open(p2_rt_dist).show()\n\n    else:\n        h = None\n        # TODO: reduce to participant in given Set  # noqa: FIX002\n        plt.figure(num=\"Response time distributions\", figsize=(12, 8))\n        for i, (tr_tab, name) in enumerate(\n            zip(\n                [trial_results_table, read_pilot_data(clean_trials=True)][: compare_pilot + 1],\n                [\"Main\", \"Pilot\"][: compare_pilot + 1],\n                strict=True,\n            )\n        ):\n            # Response time can be max 10 seconds\n            tr_tab.response_time = tr_tab.response_time.map(lambda x: np.minimum(x, 10))\n            stat = tr_tab.response_time.describe()\n\n            h = sns.kdeplot(\n                data=tr_tab,\n                x=\"response_time\",\n                color=[\"orange\", \"blue\"][i],\n                ax=h,\n                label=f\"{name} (mean={tr_tab.response_time.mean():.2f}\u00b1{tr_tab.response_time.std():.2f}s)\",\n            )\n\n            for fac in [-1, 0, 1]:  # for 3SD: [-3, -2, -1, 0, 1, 2, 3]\n                l_value = stat[\"mean\"] - stat[\"std\"] * fac\n                if l_value &gt;= 0:\n                    h.axvline(\n                        x=l_value,\n                        color=[\"orange\", \"blue\"][i],\n                        linestyle=\"-\" if fac == 0 else \"--\",\n                        alpha=0.8 if fac == 0 else 1 / (2 * abs(fac)),\n                    )\n            print(name, \"\\n\", stat)\n        h.set(xlabel=\"Response time [s]\", ylabel=\"Density\", xlim=(0, 11.5))\n        h.legend()\n        h.get_figure().tight_layout()\n        h.get_figure().savefig(p2_rt_dist, dpi=300)\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.print_ppid_list_for_bonus","title":"print_ppid_list_for_bonus","text":"<pre><code>print_ppid_list_for_bonus(\n    trial_results_table: DataFrame,\n    set_nr: str,\n    bonus: float = BONUS,\n) -&gt; None\n</code></pre> <p>Print a line-separated list of ppid's to provide bonus payments on Prolific.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def print_ppid_list_for_bonus(trial_results_table: pd.DataFrame, set_nr: str, bonus: float = params.BONUS) -&gt; None:\n    \"\"\"Print a line-separated list of ppid's to provide bonus payments on Prolific.\"\"\"\n    tr_table_wo_training = trial_results_table[trial_results_table.block_num &gt; 1]  # rm training trials\n    tr_table_wo_training = tr_table_wo_training[~tr_table_wo_training.caught.isna()]  # Remove trials w/o data\n    tr_table_wo_training = tr_table_wo_training.astype({\"caught\": bool})\n    ppids_catches = tr_table_wo_training.groupby(\"ppid\").sum().caught\n\n    # Participants who have no catches (i.e., passed all attention checks) will get a bonus payment\n    ppid_bonus_list = ppids_catches[ppids_catches == 0].index.to_list()\n\n    # Append amount of bonus payment to ppid\n    payment_for_15_min = 1.94  # (1.94\u00a3 == 2.25\u20ac)\n    bonus_payment = bonus * payment_for_15_min  # 10%\n    ppid_bonus_list = [f\"{ppid},{bonus_payment:.2f}\" for ppid in ppid_bonus_list]\n\n    cprint(\n        string=f\"Following {len(ppid_bonus_list)} participants in Set{set_nr} will get a bonus payment:\\n\",\n        col=\"y\",\n        fm=\"ul\",\n    )\n    print(*ppid_bonus_list, sep=\"\\n\")  # copy past from console to Prolific\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.print_ppid_list_to_accept","title":"print_ppid_list_to_accept","text":"<pre><code>print_ppid_list_to_accept(set_nr: str) -&gt; None\n</code></pre> <p>Print a comma-separated list of ppid's, who are to be accepted on Prolific.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def print_ppid_list_to_accept(set_nr: str) -&gt; None:\n    \"\"\"Print a comma-separated list of ppid's, who are to be accepted on Prolific.\"\"\"\n    prolific_ppid_table = read_prolific_participant_data(set_nr=set_nr)\n    if \"decision\" in prolific_ppid_table.columns:\n        ppid_accept_list = prolific_ppid_table.loc[\n            (prolific_ppid_table[\"decision\"] == \"accept\") &amp; (prolific_ppid_table[\"Status\"] == \"AWAITING REVIEW\")\n        ][\"Participant id\"].to_list()\n        cprint(string=\"\\nFollowing participants have been accepted &amp; await review on Prolific:\\n\", col=\"g\", fm=\"ul\")\n        print(*ppid_accept_list, sep=\",\")  # copy past from console to Prolific\n\n    else:\n        cprint(\n            string=\"No 'decision' column in Prolific participant table found. Remaining \"\n            \"{len(prolific_ppid_table.loc[prolific_ppid_table['Status'] == 'AWAITING REVIEW'])} \"\n            \"Participants must be reviewed before!\",\n            col=\"r\",\n        )\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.print_ppid_list_to_block","title":"print_ppid_list_to_block","text":"<pre><code>print_ppid_list_to_block(\n    set_nr: str, per_session: bool = True\n) -&gt; None\n</code></pre> <p>Print a comma separated list of ppid to block on Prolific.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def print_ppid_list_to_block(set_nr: str, per_session: bool = True) -&gt; None:\n    \"\"\"Print a comma separated list of ppid to block on Prolific.\"\"\"\n    # Update list first\n    update_block_list(verbose=False)\n\n    # Print all blocklists as one\n    session = set_nr[0]  # extract session (2D, 3D)\n    str_session = f\"{session}D \" if per_session else \"\"\n    cprint(\n        string=f\"\\nFollowing participants have been rejected in all acquired {str_session}Sets or \"\n        f\"had no or low quality data &amp; should be excluded in subsequent experiments on \"\n        f\"Prolific:\\n\",\n        col=\"y\",\n        fm=\"ul\",\n    )\n    if per_session:\n        print(\n            *[\n                ppid\n                for _set_nr in block_list_dict\n                if _set_nr.split(\".\")[0] == session[0]\n                for ppid in block_list_dict[_set_nr]\n            ],\n            sep=\",\",\n        )\n    else:\n        print(*[ppid for _set_nr in block_list_dict for ppid in block_list_dict[_set_nr]], sep=\",\")\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.prob_catch_trials","title":"prob_catch_trials","text":"<pre><code>prob_catch_trials(\n    r: int, n: int = n_catch_trials, verbose: bool = False\n) -&gt; None\n</code></pre> <p>Compute the probability of catches equal or lower than r-times (assuming random choice behavior) -&gt; <code>P(X &lt;= r)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>r</code> <code>int</code> <p>max number of catches</p> required <code>n</code> <code>int</code> <p>number of catch trials</p> <code>n_catch_trials</code> <code>verbose</code> <code>bool</code> <p>add extra explanation</p> <code>False</code> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def prob_catch_trials(r: int, n: int = params.n_catch_trials, verbose: bool = False) -&gt; None:\n    \"\"\"\n    Compute the probability of catches equal or lower than r-times (assuming random choice behavior) -&gt; `P(X &lt;= r)`.\n\n    :param r: max number of catches\n    :param n: number of catch trials\n    :param verbose: add extra explanation\n    \"\"\"\n    s = 3  # number of faces in a (catch) trial\n    n_o = 1  # number of odd-one-out faces in a catch trial\n    p = n_o / s  # probability of choosing the odd-one-out face (i.e., passing the catch trial)\n    p_c = (s - n_o) / s  # == 1 - p  # probability of being caught\n\n    # Compute probability of the number of catches being equal or lower than r-times\n    px = 0\n    for r_i in range(r + 1):\n        ncr = np.math.comb(n, r_i)  # n choose r (nCr)  == ncr_p = np.math.comb(n, n-r)\n        px += ncr * p_c**r_i * p ** (n - r_i)  # binomial probability P(X=r_i)\n    print(f\"The chance of having less than {r + 1} catch(es) in {n} catch-trials is: {px:.2%}\")\n\n    # Compute probability of number of passed-catch-trials being equal or higher than n-r-times\n    if verbose:\n        px2 = 0  # px == px2 !\n        for r_j in range(n - r, n + 1):\n            ncr = np.math.comb(n, r_j)\n            px2 += ncr * p**r_j * p_c ** (n - r_j)\n        print(\"\\t\\t (*** which is equivalent to ***)\")\n        print(f\"The chance of having {n - r} or more pass(es) in {n} catch-trials is: {px2:.2%}\")\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.review_prolific_participants","title":"review_prolific_participants","text":"<pre><code>review_prolific_participants(\n    set_nr: str,\n    ppids_to_review: list[str],\n    plot_rt: bool = False,\n    accept_at_bqs: float | None = None,\n    force_review: bool = False,\n) -&gt; None\n</code></pre> <p>Review participants for low quality data and make a decision for Prolific.</p> <p>Parameters:</p> Name Type Description Default <code>set_nr</code> <code>str</code> <p>Set number (2D: 2.0, 2.1, ... | 3D: 3.0, 3.1, ...) [str]</p> required <code>ppids_to_review</code> <code>list[str]</code> <p>list of Prolific participant IDs to review</p> required <code>plot_rt</code> <code>bool</code> <p>whether to plot response time distribution</p> <code>False</code> <code>accept_at_bqs</code> <code>float | None</code> <p>auto accept participants with BQS below this threshold</p> <code>None</code> <code>force_review</code> <code>bool</code> <p>if True: force review of participants even if they have already been reviewed</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>table with participants with decision based on BQS and individual factors (behaviour)</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def review_prolific_participants(\n    set_nr: str,\n    ppids_to_review: list[str],\n    plot_rt: bool = False,\n    accept_at_bqs: float | None = None,\n    force_review: bool = False,\n) -&gt; None:\n    \"\"\"\n    Review participants for low quality data and make a decision for Prolific.\n\n    :param set_nr: Set number (2D: 2.0, 2.1, ... | 3D: 3.0, 3.1, ...) [str]\n    :param ppids_to_review: list of Prolific participant IDs to review\n    :param plot_rt: whether to plot response time distribution\n    :param accept_at_bqs: auto accept participants with BQS below this threshold\n    :param force_review: if True: force review of participants even if they have already been reviewed\n    :return: table with participants with decision based on BQS and individual factors (behaviour)\n    \"\"\"\n    # Load Prolific participant data for given Set\n    p_table, p2_p_table = read_prolific_participant_data(set_nr=set_nr, return_path=True)\n    ppid_data_table = read_participant_data(process=False)\n    trial_results_table = read_trial_results_of_set(set_nr=set_nr, clean_trials=False, verbose=False)\n    rt_stat = get_response_time_stats(trial_results_table=trial_results_table, include_pilot=True)\n    bqs_tab = get_quality_score_table(set_nr=set_nr, save=False, verbose=False)\n\n    # Define decision categories\n    decision_dict = {\"r\": \"reject\", \"o\": \"open\", \"a\": \"accept\", \"d\": \"done\"}\n\n    # Attach decision column to Prolific participant table\n    d_col = \"decision\"\n    if d_col not in p_table.columns:\n        p_table[d_col] = np.nan\n\n    # Review participant by participant\n    for ppid in tqdm(ppids_to_review):\n        cprint(string=f\"\\nProcessing participant '{ppid}':\", col=\"b\", fm=\"ul\")\n\n        ppid_info = p_table.loc[p_table[\"Participant id\"] == ppid].copy()\n\n        if ppid_info.empty:\n            # Participant not in Prolific table, probably due to returned/aborted experiment\n            cprint(string=f\"No participant information of '{ppid}' in Set{set_nr}.\", col=\"g\")\n            continue\n\n        current_decision = ppid_info[d_col].item()\n\n        # Check whether participant has to be reviewed\n        if not force_review and (\n            (current_decision != decision_dict[\"o\"] and not pd.isna(current_decision))\n            or ppid_info.Status.item() != \"AWAITING REVIEW\"\n        ):\n            # Participant has a decision already\n            cprint(string=f\"No processing necessary for participant '{ppid}'.\", col=\"g\")\n            continue\n        # else ['REJECTED', 'RETURNED', 'APPROVED']\n\n        # Provide overview of data quality of participant\n        explore_quality_score_of_single_participant(ppid=ppid, set_nr=set_nr, trial_results_table=trial_results_table)\n\n        # A comparison the participant's response times to the population mean\n        ppid_rt_stat = trial_results_table[trial_results_table.ppid == ppid].response_time.describe()\n        print(f\"\\nParticipant mean RT: {ppid_rt_stat['mean']:.2f} \u00b1 {ppid_rt_stat['std']:.2f} sec\")\n        cprint(string=f\"Population  mean RT: {rt_stat['mean']:.2f} \u00b1 {rt_stat['std']:.2f} sec\", col=\"g\", fm=\"ul\")\n        cprint(string=f\"Difference  mean RT:{ppid_rt_stat['mean'] - rt_stat['mean']:.2f} sec\", col=\"y\", fm=\"bo\")\n\n        # Decide on single participant\n        if plot_rt:\n            plt.figure(num=f\"Response time histogram of '{ppid}'\", figsize=(12, 8))\n            h = sns.histplot(\n                data=trial_results_table[trial_results_table.ppid == ppid],\n                x=\"response_time\",\n                bins=10 * 10,\n                binrange=(0, 10),\n            )\n            for fac in [-1, 0, 1]:  # for 3SD: [-3, -2, -1, 0, 1, 2, 3]\n                l_value = ppid_rt_stat[\"mean\"] - ppid_rt_stat[\"std\"] * fac\n                if l_value &gt;= 0:\n                    h.axvline(\n                        x=l_value, linestyle=\"-\" if fac == 0 else \"--\", alpha=0.8 if fac == 0 else 1 / (2 * abs(fac))\n                    )\n            h.set(xlim=(0, 10.3))\n            h.get_figure().tight_layout()\n            h.get_figure().show()\n\n        # Provide general information about the participant\n        if not pd.isna(ppid_info[\"Time taken\"]).values:  # noqa: PD011\n            ppid_info[\"Time taken\"] = ppid_info[\"Time taken\"].map(\n                lambda x: str(timedelta(seconds=x))\n            )  # convert seconds to hh:mm:ss\n        print(\"\\n\", ppid_info[[\"Time taken\", \"Total approvals\", \"Age\", \"Sex\", \"Nationality\", \"Status\"]])\n\n        # Print the self-reported ability to recognize faces if available\n        ppid_face_recognition = ppid_data_table[ppid_data_table.ppid == ppid].face_recognition\n        if len(ppid_face_recognition) &gt;= 2:  # noqa: PLR2004\n            # Find matching Set-Nr in participant data\n            infix = f\"_{set_infix(set_nr)}_\"  # e.g., \"_s005_\"\n            idx = [\n                i\n                for i, s in ppid_data_table[ppid_data_table.ppid == ppid].iterrows()\n                if infix in s.ppid_session_dataname\n            ]\n            ppid_face_recognition = ppid_face_recognition[idx]\n\n        if len(ppid_face_recognition) != 0 and not ppid_face_recognition.isna().item():\n            print(\n                f\"\\nSelf-reported ability to recognise faces [1-7]: \\033[95m{ppid_face_recognition.item()}\\033[0m\"\n            )  # print in light magenta\n\n        # Request decision\n        if not pd.isna(current_decision):\n            cprint(string=f\"\\nPrevious decision: {current_decision}\", col=\"b\")\n\n        # Automatic accept at the given threshold for BQS acceptance\n        if accept_at_bqs is not None and bqs_tab.loc[bqs_tab.ppid == ppid].BQS.item() &lt; accept_at_bqs:\n            cprint(\n                string=f\"Automatically accept participant '{ppid}' due to their low \"\n                f\"BQS {bqs_tab.loc[bqs_tab.ppid == ppid].BQS.item():.2f} &lt; {accept_at_bqs:.2f}.\",\n                col=\"g\",\n            )\n            decision = \"a\"\n        else:\n            try:\n                ppid_bqs = bqs_tab.loc[bqs_tab.ppid == ppid].BQS.item()\n            except ValueError:\n                ppid_bqs = np.nan\n            decision = cinput(\n                string=f\"\\nChoose action for participant '{ppid}' \"\n                f\"(BQS={ppid_bqs:.2f}):\\n\"\n                f\"\\t'[r]eject',\\t\\t'[o]pen',\\t\\t'[a]ccept'\\n\"\n                f\"Choose__: \",\n                col=\"y\",\n            ).lower()\n\n        if decision not in decision_dict and decision == \"d\":\n            msg = \"Invalid input.\"\n            raise ValueError(msg)\n\n        if plot_rt:\n            plt.close()\n\n        # Save decision\n        p_table.loc[p_table[\"Participant id\"] == ppid, d_col] = decision_dict[decision]\n        p_table.to_csv(p2_p_table, index=False)\n</code></pre>"},{"location":"reference/datachecks/#facesim3d.datachecks.update_block_list","title":"update_block_list","text":"<pre><code>update_block_list(verbose: bool = False) -&gt; None\n</code></pre> <p>Update the blocklist.</p> Source code in <code>code/facesim3d/datachecks.py</code> <pre><code>def update_block_list(verbose: bool = False) -&gt; None:\n    \"\"\"Update the blocklist.\"\"\"\n    # Add all blocked participants from all sets\n    all_block_ppids = []\n    ppids_participated = []\n    for set_nr in get_list_of_acquired_sets():\n        # Add Set number if missing\n        if set_nr not in block_list_dict:\n            block_list_dict.update({set_nr: []})\n        if set_nr not in exception_list_dict:\n            exception_list_dict.update({set_nr: []})\n\n        # Load prolific participant table\n        prolific_ppid_table = read_prolific_participant_data(set_nr=set_nr)\n\n        # Add blocked participants\n        block_list_dict[set_nr].extend(\n            prolific_ppid_table.loc[\n                (prolific_ppid_table.Status == \"REJECTED\") | (prolific_ppid_table.decision == \"open\")\n            ][\"Participant id\"].to_list()\n        )\n\n        # Add high BQS participants (BQS &gt;= 3)\n        bqs_table = get_quality_score_table(set_nr=set_nr, save=False)\n        block_list_dict[set_nr].extend(bqs_table[BQS_TH &lt;= bqs_table.BQS].ppid.to_list())\n\n        # Add with more than two catches\n        tr_table = read_trial_results_of_set(set_nr=set_nr, clean_trials=False, verbose=False)\n        tr_table = tr_table[tr_table.block_num &gt; 1]  # remove training trials\n        tr_table = tr_table[~tr_table.caught.isna()]  # remove trials w/o data\n        ppids_n_catches = tr_table.groupby(\"ppid\").sum().caught\n        block_list_dict[set_nr].extend(ppids_n_catches[ppids_n_catches &gt;= 2].index.to_list())  # noqa: PLR2004\n\n        # Add participants which (passed but) have no data\n        ppids_wo_data = prolific_ppid_table[~prolific_ppid_table[\"Participant id\"].isin(tr_table.ppid.unique())][\n            \"Participant id\"\n        ]\n        block_list_dict[set_nr].extend(ppids_wo_data.to_list())\n\n        # Add participants who did the experiment 3 times\n        ppids_participated.extend(prolific_ppid_table[\"Participant id\"].to_list())\n        ctn_ppids_accepted = pd.value_counts(ppids_participated)\n        ppids_with_3_sets = ctn_ppids_accepted[ctn_ppids_accepted &gt;= 3].index.to_list()  # noqa: PLR2004\n        block_list_dict[set_nr].extend(ppids_with_3_sets)\n        ppids_participated = [ppid for ppid in ppids_participated if ppid not in ppids_with_3_sets]\n\n        # Remove exceptions from blocklist\n        for e in exception_list_dict[set_nr]:\n            if e in block_list_dict[set_nr]:\n                block_list_dict[set_nr].remove(e)\n\n        # Clean up the blocklist: remove duplicates\n        block_list_dict[set_nr] = list(set(block_list_dict[set_nr]))  # within sets\n\n        # across sets\n        if len(all_block_ppids) == 0:\n            all_block_ppids.extend(block_list_dict[set_nr])\n        else:\n            temp_list = deepcopy(block_list_dict[set_nr])\n            duplicates = [ppid for ppid in all_block_ppids if ppid in temp_list]\n            for d in duplicates:\n                block_list_dict[set_nr].remove(d)\n                other_set = next(k for k in block_list_dict if d in block_list_dict[k])\n                if verbose:\n                    print(f\"\\t(removing duplicate '{d}' from Set{set_nr}, was already blocked in Set{other_set}.)\")\n\n            all_block_ppids.extend(block_list_dict[set_nr])\n        if verbose:\n            print(f\"Set{set_nr} has {len(block_list_dict[set_nr])} blocked participants.\")\n\n    if verbose:\n        cprint(string=f\"Total number of blocked participants: {len(all_block_ppids)}\", col=\"y\", fm=\"bo\")\n</code></pre>"},{"location":"reference/read_data/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> read_data","text":""},{"location":"reference/read_data/#facesim3d.read_data","title":"read_data","text":"<p>Read &amp; prepare data files.</p> <p>This module can be run from the command line interface (CLI) to read and prepare data for further analysis.</p> <p>CLI Usage</p> <pre><code>python -m facesim3d.read_data --help\n</code></pre> <p>With this, one can also delete the processed data from the remote (<code>DynamoDB</code>).</p> <p>Functions:</p> Name Description <code>archive_former_tables</code> <p>Move former tables to archive.</p> <code>convert_date_time</code> <p>Convert date-time string to format <code>YYYY-MM-DD HH:MM:SS:MS</code>.</p> <code>convert_dynamodb_json_in_row_of_df</code> <p>Convert a row in a pandas Dataframe (df), which is in the 'DynamoDB<code></code>json`-format, into a normal df format.</p> <code>create_all_triple_combinations</code> <p>Create all triplet combinations.</p> <code>delete_all_items_in_table_on_dynamodb</code> <p>Delete all items in table on <code>DynamoDB</code> (e.g., <code>'UXFData.FaceSim.TrialResults'</code>).</p> <code>finalized_triplets</code> <p>Provide an overview of the finalized triplets.</p> <code>finalized_triplets_multi_sub_sample</code> <p>Provide an overview of finalized triplets.</p> <code>get_current_state_of_triplets</code> <p>Get the current state of triplets (e.g., which triplet is currently in the experiment).</p> <code>get_list_of_acquired_sets</code> <p>Get the list of all sets that have been acquired.</p> <code>get_participant_session</code> <p>Get session (2D, 3D) of the given participant ID (<code>ppid</code>).</p> <code>get_participant_set_numbers</code> <p>Get the Set-number(s) of a given participant.</p> <code>get_triplet_ids_and_heads</code> <p>Get all <code>triplet_id</code>'s and heads corresponding to <code>UXFData.FaceSim.TripletsIDB.*D[.Pilot]</code>.</p> <code>load_local_table</code> <p>Load a <code>UXFData.FaceSim.*</code> table from the local storage system.</p> <code>load_table_from_dynamodb</code> <p>Load a <code>UXFData.FaceSim.*</code> table from DynamoDB.</p> <code>load_trial_results_from_dynamodb</code> <p>Load all trial-results from <code>DynamoDB</code>.</p> <code>main</code> <p>Run the main function of <code>read_data.py</code>.</p> <code>merge_tables</code> <p>Merge a given table (<code>df</code>) with an existing table of the given table name.</p> <code>plot_triplet_matrix</code> <p>Plot matrix of triplets.</p> <code>read_and_convert_s3_results_json_data</code> <p>Get the full trial table of the main study from memory.</p> <code>read_logs_of_set</code> <p>Read all log tables of a given Set number.</p> <code>read_participant_data</code> <p>Get the full participant table of the main study.</p> <code>read_pilot_data</code> <p>Get the full trial table of the pilot study (version 2).</p> <code>read_pilot_participant_data</code> <p>Get the full participant table of the pilot study (version 2).</p> <code>read_prolific_participant_data</code> <p>Read the participant table of a given Set downloaded from Prolific.</p> <code>read_trial_results_of_participant</code> <p>Read all trial results of a given participant.</p> <code>read_trial_results_of_session</code> <p>Read all trial results of a given session.</p> <code>read_trial_results_of_set</code> <p>Read all trial results of a given Set number.</p> <code>remove_invalid_trials</code> <p>Remove invalid trials from a given trial results table.</p> <code>save_merged_tables_of_set</code> <p>Merge all tables of a given type (\"<code>TrialResults</code>\", \"<code>SessionLog</code>\") in a given Set.</p> <code>set_infix</code> <p>Generate the Set infix (e.g., 's004' OR 's011') from a set number.</p> <code>update_triplet_table_on_dynamodb</code> <p>Update the triplet table on <code>DynamoDB</code>.</p> <code>update_triplet_table_on_dynamodb_multi_sub_sample</code> <p>Update triplet table on <code>DynamoDB</code> for the given session of the <code>multi-sampled-sub-sample</code>.</p> <code>where_to_find_trial_and_log_data</code> <p>Get information about in which files trial results and log data can be found for a given Set number.</p>"},{"location":"reference/read_data/#facesim3d.read_data.archive_former_tables","title":"archive_former_tables","text":"<pre><code>archive_former_tables(\n    path_to_save: str | Path, table_name: str\n) -&gt; None\n</code></pre> <p>Move former tables to archive.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_save</code> <code>str | Path</code> <p>Path where new table will be saved</p> required <code>table_name</code> <code>str</code> <p>name of table</p> required Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def archive_former_tables(path_to_save: str | Path, table_name: str) -&gt; None:\n    \"\"\"\n    Move former tables to archive.\n\n    :param path_to_save: Path where new table will be saved\n    :param table_name: name of table\n    \"\"\"\n    list_of_other_tables = Path(str(path_to_save)).parent.glob(f\"*{table_name}.csv\")\n    list_of_other_tables = [p for p in list_of_other_tables if str(p) != path_to_save]\n\n    for p in list_of_other_tables:\n        p.rename(str(p).replace(paths.data.MAIN, paths.data.main.archive))\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.convert_date_time","title":"convert_date_time","text":"<pre><code>convert_date_time(date_time: str) -&gt; str\n</code></pre> <p>Convert date-time string to format <code>YYYY-MM-DD HH:MM:SS:MS</code>.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def convert_date_time(date_time: str) -&gt; str:\n    \"\"\"Convert date-time string to format `YYYY-MM-DD HH:MM:SS:MS`.\"\"\"\n    if pd.isna(date_time):\n        return date_time\n    if not date_time.startswith(\"2022-\") and not date_time.startswith(\"2023-\"):\n        # Bring in format YYYY-MM-DD HH:MM:SS:MS (e.g., 2022-11-14 17:12:18:358)\n        d = date_time[: date_time.find(\":\") - 2]\n        t = date_time[date_time.find(\":\") - 2 :]\n        d = d.replace(\" \", \"\").replace(\".\", \"-\").replace(\"/\", \"-\")  # remove blanks &amp; replace\n        date_time = f\"{d} {t}\"\n    if date_time[10] != \" \":\n        # This solves an issue with dates like this '2022-12-1412-19-28- 880'\n        date_time = date_time[:10] + \" \" + date_time[10:].replace(\"-\", \":\").replace(\": \", \".\")\n\n    return date_time\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.convert_dynamodb_json_in_row_of_df","title":"convert_dynamodb_json_in_row_of_df","text":"<pre><code>convert_dynamodb_json_in_row_of_df(\n    df_row: Series,\n) -&gt; Series\n</code></pre> <p>Convert a row in a pandas Dataframe (df), which is in the 'DynamoDB<code></code>json`-format, into a normal df format.</p> <p>Rows/cells come often in the following <code>json</code> format of <code>DynamoDB</code>: <code>' [{\"N\":\"25\"}]'</code>, or similar. Convert this (example) to: <code>25</code>.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def convert_dynamodb_json_in_row_of_df(df_row: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Convert a row in a pandas Dataframe (df), which is in the 'DynamoDB` `json`-format, into a normal df format.\n\n    Rows/cells come often in the following `json` format of `DynamoDB`: `' [{\"N\":\"25\"}]'`, or similar.\n    Convert this (example) to: `25`.\n    \"\"\"\n    _row = df_row.copy()\n\n    # Get the type and key of the type\n    dtype_key = next(iter(literal_eval(_row[0])[0].keys()))\n    row_dtype = DT_MAP[dtype_key]  # type mapper defined above\n\n    # Convert row\n    _row = _row.map(lambda x: literal_eval(x)[0][dtype_key])\n    return _row.astype(row_dtype)\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.create_all_triple_combinations","title":"create_all_triple_combinations","text":"<pre><code>create_all_triple_combinations(n_faces: int) -&gt; DataFrame\n</code></pre> <p>Create all triplet combinations.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def create_all_triple_combinations(n_faces: int) -&gt; pd.DataFrame:\n    \"\"\"Create all triplet combinations.\"\"\"\n    n_faces_in_triplet: int = 3\n    if n_faces &lt; n_faces_in_triplet:\n        msg = f\"Number of faces must be at least 3, but is {n_faces}!\"\n        raise ValueError(msg)\n    triplet_combinations = list(combinations(range(1, n_faces + 1), r=n_faces_in_triplet))\n    return pd.DataFrame(triplet_combinations, columns=[\"head1\", \"head2\", \"head3\"])\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.delete_all_items_in_table_on_dynamodb","title":"delete_all_items_in_table_on_dynamodb","text":"<pre><code>delete_all_items_in_table_on_dynamodb(\n    table_name: str,\n) -&gt; None\n</code></pre> <p>Delete all items in table on <code>DynamoDB</code> (e.g., <code>'UXFData.FaceSim.TrialResults'</code>).</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def delete_all_items_in_table_on_dynamodb(table_name: str) -&gt; None:\n    \"\"\"Delete all items in table on `DynamoDB` (e.g., `'UXFData.FaceSim.TrialResults'`).\"\"\"\n    delete = ask_true_false(\n        f\"\\nAre you sure you downloaded and saved all data/items of table '{table_name}' from DynamoDB?\", col=\"r\"\n    )\n\n    if delete and ask_true_false(\n        f\"Are you sure you want to delete all items in table '{table_name}' on DynamoDB?\", col=\"r\"\n    ):\n        cprint(string=f\"Scanning &amp; deleting all items in '{table_name}' on DynamoDB ...\", col=\"y\")\n\n        dynamodb = boto3.resource(\"dynamodb\", region_name=\"eu-central-1\")  # connect to DynamoDB\n        table = dynamodb.Table(table_name)\n\n        response = table.scan()\n        data = response[\"Items\"]\n        # The following is necessary because the response is paginated (limit 1 MB)\n        while \"LastEvaluatedKey\" in response:\n            response = table.scan(ExclusiveStartKey=response[\"LastEvaluatedKey\"])\n            data.extend(response[\"Items\"])\n\n        key_schema = table.key_schema\n        key_names = [k[\"AttributeName\"] for k in key_schema]\n        with table.batch_writer() as batch:\n            for row in tqdm(data, desc=f\"Deleting items in {table_name}\"):\n                batch.delete_item(\n                    Key=dict(zip(key_names, [row[key] for key in key_names], strict=True))\n                    # Key={\"ppid_session_dataname\": row[\"ppid_session_dataname\"],\n                    #      \"SystemDateTime_BeginTrial\": row[\"SystemDateTime_BeginTrial\"]}\n                )\n        cprint(string=f\"All items deleted from {table_name}.\", col=\"g\")\n\n    else:\n        cprint(string=\"Nothing will be  deleted.\", col=\"g\")\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.finalized_triplets","title":"finalized_triplets","text":"<pre><code>finalized_triplets(session: str) -&gt; list[int]\n</code></pre> <p>Provide an overview of the finalized triplets.</p> <p>For the given session, provide an overview of the finalized triplets. &amp; return a list of remaining triplets.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def finalized_triplets(session: str) -&gt; list[int]:\n    \"\"\"\n    Provide an overview of the finalized triplets.\n\n    For the given session, provide an overview of the finalized triplets. &amp; return a list of remaining\n    triplets.\n    \"\"\"\n    n_all_triplets = np.math.comb(params.main.n_faces, 3)\n\n    good_sess_tr_table = read_trial_results_of_session(session=session, clean_trials=True, verbose=False)\n\n    sampled_unique_triplets = good_sess_tr_table.triplet_id.astype(int).unique()\n\n    cprint(\n        string=f\"{len(sampled_unique_triplets) / n_all_triplets:.1%} of all triplets were sampled &amp; approved \"\n        f\"in session {session}.\",\n        col=\"g\",\n    )\n\n    remaining_triplets = sorted(set(range(1, n_all_triplets + 1)) - set(sampled_unique_triplets))\n\n    print(f\"Number of remaining triplets: {len(remaining_triplets)}\")\n\n    return remaining_triplets\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.finalized_triplets_multi_sub_sample","title":"finalized_triplets_multi_sub_sample","text":"<pre><code>finalized_triplets_multi_sub_sample() -&gt; list[int]\n</code></pre> <p>Provide an overview of finalized triplets.</p> <p>For the given session of the multi-sampled-sub-sample provide an overview &amp; return the list of remaining triplets.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def finalized_triplets_multi_sub_sample() -&gt; list[int]:\n    \"\"\"\n    Provide an overview of finalized triplets.\n\n    For the given session of the multi-sampled-sub-sample provide an overview &amp; return the list of remaining triplets.\n    \"\"\"\n    n_all_triplets = np.math.comb(params.multisubsample.n_faces, 3)\n\n    # The following includes the trials written in 'UXFData.FaceSim.OtherTrialData' as well\n    trial_results_table = load_table_from_dynamodb(table_name=\"UXFData.FaceSim.TrialResults\", save=False, merge=False)\n\n    good_sess_tr_table = remove_invalid_trials(trial_results_table=trial_results_table, verbose=True)\n\n    # Filter for multi-sub-sample\n    sampled_triplets_counts = good_sess_tr_table.triplet_id.astype(int).value_counts()\n\n    print(\"\\n\", sampled_triplets_counts, \"\\n\")\n\n    for i in range(1, params.multisubsample.n_reps + 1):\n        cprint(\n            string=f\"{(sampled_triplets_counts &gt;= i).sum()}/{n_all_triplets} \"\n            f\"({(sampled_triplets_counts &gt;= i).sum() / n_all_triplets:.1%}) of all triplets were sampled at \"\n            f\"least {i} times!\",\n            col=\"g\",\n        )\n    # perc_sampled_n_times = len(sampled_triplets_counts[  # noqa: ERA001, RUF100\n    #         sampled_triplets_counts == params.multisubsample.n_reps]) / n_all_triplets\n\n    # cprint(f\"{perc_sampled_n_times:.1%} of all triplets were sampled {params.multisubsample.n_reps} times!\", \"g\")  # noqa: ERA001, E501\n\n    remaining_triplets = sorted(\n        set(range(1, n_all_triplets + 1))\n        - set(sampled_triplets_counts[sampled_triplets_counts == params.multisubsample.n_reps].index)\n    )\n\n    print(f\"Number of remaining triplet IDs: {len(remaining_triplets)}\")\n\n    # done_triplets = sorted(set(  # noqa: ERA001, RUF100\n    #     sampled_triplets_counts[sampled_triplets_counts == params.multisubsample.n_reps].index))\n\n    # print(f\"Number of done triplets: {len(done_triplets)}\")  # noqa: ERA001\n\n    return remaining_triplets\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.get_current_state_of_triplets","title":"get_current_state_of_triplets","text":"<pre><code>get_current_state_of_triplets(\n    session: str, pilot: bool = PILOT, plot: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Get the current state of triplets (e.g., which triplet is currently in the experiment).</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def get_current_state_of_triplets(session: str, pilot: bool = params.PILOT, plot: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Get the current state of triplets (e.g., which triplet is currently in the experiment).\"\"\"\n    session = session.upper()\n    if session not in params.SESSIONS:\n        msg = f\"Session '{session}' not in {params.SESSIONS}!\"\n        raise ValueError(msg)\n\n    # Append table name\n    table_name = \"UXFData.FaceSim.TripletsIDB.\" + session\n    if pilot:\n        table_name += \".Pilot\"\n\n    # Load table from DynamoDB\n    triplet_table = load_table_from_dynamodb(table_name=table_name, save=False, merge=False)\n\n    # Get how many of each status\n    n_complete = len(triplet_table[triplet_table.status == \"G\"])\n    n_unseen = len(triplet_table[triplet_table.status == \"U\"])\n    n_lock = len(triplet_table[triplet_table.status == \"L\"])\n    n_total = len(triplet_table)  # == n_complete + n_lock + n_unseen == np.math.comb(n_faces, 3)\n\n    # Print information\n    cprint(string=f\"Current state of {session} triplets:\", fm=\"ul\", ts=True)\n    cprint(string=f\"\\t&gt; {n_complete / n_total:.1%} triplets are completed\", col=\"g\")\n    cprint(string=f\"\\t&gt; {n_unseen / n_total:.1%} triplets are unseen\", col=\"y\")\n    print(f\"\\t&gt; {n_lock / n_total:.1%} triplets are locked\")\n    print(f\"Data from {table_name} on DynamoDB\")\n\n    if plot:\n        fig = plot_triplet_matrix(\n            triplet_table=triplet_table[triplet_table.status == \"G\"],\n            n_faces=params.pilot.v2.n_faces if pilot else params.main.n_faces,\n        )\n\n        fig.savefig(\n            Path(paths.data.MAIN) / f\"{datetime.now().date()}_sampled_{session}\"\n            f\"{'-pilot' if pilot else ''}-triplets.png\"\n        )\n\n    return triplet_table\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.get_list_of_acquired_sets","title":"get_list_of_acquired_sets","text":"<pre><code>get_list_of_acquired_sets() -&gt; list\n</code></pre> <p>Get the list of all sets that have been acquired.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def get_list_of_acquired_sets() -&gt; list:\n    \"\"\"Get the list of all sets that have been acquired.\"\"\"\n    return sorted(\n        [str(f).split(\"-Set\")[1].split(\"_\")[0] for f in Path(paths.data.main.prolific).glob(\"*Participants-Set*.csv\")]\n    )  # [2.0, 2.1, ..., 3.0, 3.1,  ...]\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.get_participant_session","title":"get_participant_session","text":"<pre><code>get_participant_session(\n    ppid: str, pilot: bool = PILOT\n) -&gt; str | None\n</code></pre> <p>Get session (2D, 3D) of the given participant ID (<code>ppid</code>).</p> <p>Note</p> <p>Participant can only be part of one session (2D or 3D).</p> <p>Parameters:</p> Name Type Description Default <code>ppid</code> <code>str</code> <p>ID of participant</p> required <code>pilot</code> <code>bool</code> <p>True: use pilot data</p> <code>PILOT</code> <p>Returns:</p> Type Description <code>str | None</code> <p>session of participant</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def get_participant_session(ppid: str, pilot: bool = params.PILOT) -&gt; str | None:\n    \"\"\"\n    Get session (2D, 3D) of the given participant ID (`ppid`).\n\n    !!! note\n        Participant can only be part of one session (2D or 3D).\n\n    :param ppid: ID of participant\n    :param pilot: True: use pilot data\n    :return: session of participant\n    \"\"\"\n    # Get participant table\n    pid_table = read_pilot_participant_data() if pilot else read_participant_data()\n    session = pid_table[pid_table.ppid == ppid].group_exp\n    session = session.drop_duplicates()\n    if len(session) == 1:\n        return session.item()  # extract session\n    if len(session) &gt; 1:\n        msg = f\"Participant '{ppid}' was part of different conditions!\\n{session}\\nThis must be solved manually!\"\n        raise ValueError(msg)\n    cprint(string=f\"Participant '{ppid}' not found!\", col=\"r\")\n    return None\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.get_participant_set_numbers","title":"get_participant_set_numbers","text":"<pre><code>get_participant_set_numbers(ppid: str) -&gt; list[str]\n</code></pre> <p>Get the Set-number(s) of a given participant.</p> <p>Note</p> <p>Participants can be part of up to three sets, however, only of one session (2D or 3D).</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def get_participant_set_numbers(ppid: str) -&gt; list[str]:\n    \"\"\"\n    Get the Set-number(s) of a given participant.\n\n    !!! note\n        Participants can be part of up to three sets, however, only of one session (2D or 3D).\n\n    \"\"\"\n    set_nrs = []\n    for set_nr in get_list_of_acquired_sets():\n        prolific_tab = read_prolific_participant_data(set_nr=set_nr)\n        if ppid in prolific_tab[\"Participant id\"].values:  # noqa: PD011\n            set_nrs.append(set_nr)\n    return set_nrs\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.get_triplet_ids_and_heads","title":"get_triplet_ids_and_heads","text":"<pre><code>get_triplet_ids_and_heads(pilot: bool = PILOT) -&gt; DataFrame\n</code></pre> <p>Get all <code>triplet_id</code>'s and heads corresponding to <code>UXFData.FaceSim.TripletsIDB.*D[.Pilot]</code>.</p> <p>Of the form:</p> <pre><code>triplet_id   triplet\n         1  19_26_56\n         2  22_68_73\n         3  35_39_54\n       ...       ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pilot</code> <code>bool</code> <p>Whether to load pilot data or not (main).</p> <code>PILOT</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table of the triplet_ids and heads [<code>pd.DataFrame</code>].</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def get_triplet_ids_and_heads(pilot: bool = params.PILOT) -&gt; pd.DataFrame:\n    \"\"\"\n    Get all `triplet_id`'s and heads corresponding to `UXFData.FaceSim.TripletsIDB.*D[.Pilot]`.\n\n    Of the form:\n\n        triplet_id   triplet\n                 1  19_26_56\n                 2  22_68_73\n                 3  35_39_54\n               ...       ...\n\n    :param pilot: Whether to load pilot data or not (main).\n    :return: Table of the triplet_ids and heads [`pd.DataFrame`].\n    \"\"\"\n    p2_table = paths.data.pilot.triplets if pilot else paths.data.main.triplets\n    return pd.read_csv(p2_table)\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.load_local_table","title":"load_local_table","text":"<pre><code>load_local_table(\n    table_name: str | None = None,\n) -&gt; DataFrame | None\n</code></pre> <p>Load a <code>UXFData.FaceSim.*</code> table from the local storage system.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def load_local_table(table_name: str | None = None) -&gt; pd.DataFrame | None:\n    \"\"\"Load a `UXFData.FaceSim.*` table from the local storage system.\"\"\"\n    if table_name is None:\n        cprint(string=\"Specify the table to load:\", col=\"y\", fm=\"ul\")\n        p2_table = browse_files(initialdir=paths.data.MAIN, filetypes=\"*.csv\")\n    else:\n        p2_table = list(Path(paths.data.MAIN).glob(f\"*{table_name}*\"))\n        if len(p2_table) &gt; 1:\n            cprint(\n                string=f\"Found more than one file w.r.t. table '{table_name}'!\\n\"\n                f\"Choose the corresponding table file by index:\",\n                col=\"b\",\n            )\n            print(\"\", *[f\"{i}:\\t'{tab.name}'\" for i, tab in enumerate(p2_table)], sep=\"\\n\\t\")\n            tab_idx = cinput(string=\"\\nType index of table you want to load: \", col=\"y\")\n            p2_table = p2_table[int(tab_idx)]\n        elif len(p2_table) == 0:\n            cprint(string=f\"No table found w.r.t. '{table_name}'!\", col=\"y\")\n            return None\n        else:\n            p2_table = p2_table.pop()\n\n    # Load tablet\n    tab = pd.read_csv(p2_table)\n\n    # Convert table rows if necessary: unpack the DynamoDB json format\n    for col in tab.columns:\n        if isinstance(tab[col][0], str) and tab[col][0].startswith(\"[{\"):\n            tab[col] = tab[col].map(literal_eval)\n\n            if isinstance(tab[col][0], list) and len(tab[col][0]) == 1:\n                tab[col] = tab[col].map(lambda x: x[0])\n                tab[col] = convert_dynamodb_json_in_row_of_df(df_row=tab[col])\n            elif isinstance(tab[col][0], list) and len(tab[col][0]) &gt; 1:\n                # Primarily a case for 'UXFData.FaceSim.SessionLog'.\n                # Most cells in row (i.e., one participant session)\n                # are lists of DynamoDB jsons (i.e., dicts)\n                # convert [{'S': 'Log'}, {'S': 'Log'}, , ...] -&gt; [Log, Log, ...] per row\n                tab[col] = tab[col].map(lambda x: [cell[next(iter(cell.keys()))] for cell in x])\n            else:\n                cprint(string=f\"Unknown format of column '{col}' at index 0!\", col=\"r\")\n\n        if isinstance(tab[col][0], str) and tab[col][0].startswith(\"[\"):\n            # Primarily a case for 'UXFData.FaceSim.SessionLog' after formatting\n            tab[col] = tab[col].map(literal_eval)\n\n        if \"SystemDateTime\" in col:\n            tab[col] = tab[col].map(convert_date_time)\n\n    return tab\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.load_table_from_dynamodb","title":"load_table_from_dynamodb","text":"<pre><code>load_table_from_dynamodb(\n    table_name: str | None = None,\n    save: bool = False,\n    merge: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Load a <code>UXFData.FaceSim.*</code> table from DynamoDB.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def load_table_from_dynamodb(table_name: str | None = None, save: bool = False, merge: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Load a `UXFData.FaceSim.*` table from DynamoDB.\"\"\"\n    dynamodb = boto3.resource(\"dynamodb\", region_name=\"eu-central-1\")  # connect to DynamoDB\n\n    table_list = list(dynamodb.tables.all())  # pull all tables (names) from DynamoDB\n    table_list = [t.name for t in table_list]  # extract table names\n\n    if table_name is None:\n        cprint(string=\"Specify table to download:\", col=\"y\", fm=\"ul\")\n        print(\"\", *[f\"{i}:\\t'{tab.split('.FaceSim.')[-1]}'\" for i, tab in enumerate(table_list)], sep=\"\\n\\t\")\n        tab_idx = cinput(string=\"\\nType index of table you want to download: \", col=\"y\")\n        table_name = table_list[int(tab_idx)]\n    elif table_name not in table_list:\n        msg = (\n            f\"Given table '{table_name}' was not found on DynamoDB!\\n\"\n            f\"\\nFollowing tables are available:\\n\\n{table_list!s}\"\n        )\n        raise ValueError(msg)\n\n    cprint(string=f\"Scanning &amp; loading table '{table_name}' from DynamoDB ...\", col=\"b\")\n\n    table = dynamodb.Table(table_name)\n\n    response = table.scan()  # -&gt; dict\n    data = response[\"Items\"]\n    # The following is necessary because the response is paginated (limit 1 MB)\n    while \"LastEvaluatedKey\" in response:\n        response = table.scan(ExclusiveStartKey=response[\"LastEvaluatedKey\"])\n        data.extend(response[\"Items\"])\n\n    # Convert to pandas DataFrame\n    loaded_df = pd.DataFrame(data)\n\n    # Unbox table cells\n    # Some tables have list entries in cells, with list-length==0 (see below): Unpack them:\n    #     screen_width      SystemDateTime_StartExp  ... screen_height age_years\n    # 0         [1600]    [2022/11/14 15:50:05.086]  ...         [900]      [25]\n    # 1         [1920]    [2022/11/14 14:51:55.245]  ...        [1080]      [21]\n    # 2         [1440]    [2022/11/14 15:49:49.150]  ...         [900]      [22]\n\n    # Filter out triplet unlocker\n    ppid_sess_col = [c for c in loaded_df.columns if c.startswith(\"ppid_session\")]\n    for i, v in loaded_df[ppid_sess_col].iterrows():  # noqa: B007\n        if isinstance(v.values[0], str) and v.values[0].startswith(\"UnlockTriplets_\"):  # noqa: PD011\n            break\n    else:\n        i = None\n    if i is not None:\n        loaded_df = loaded_df.drop(index=i).reset_index(drop=True)\n\n    # Process columns\n    for col in loaded_df.columns:\n        rnd_idx = np.random.randint(0, len(loaded_df), 10)\n        if all(\n            (isinstance(cell, list) and len(cell) == 1)\n            for cell in loaded_df[col].iloc[rnd_idx].values  # noqa: PD011\n        ):  # noqa: PD011, RUF100\n            loaded_df[col] = loaded_df[col].map(lambda x: x[0])\n        # Do not unpack those with list length &gt; 1\n\n        # Correct datetime in table\n        if \"SystemDateTime\" in col:\n            loaded_df[col] = loaded_df[col].map(convert_date_time)\n\n    # Extract ppid from ppid_session_dataname\n    if \"ppid_session_dataname\" in loaded_df.columns and \"ppid\" not in loaded_df.columns:\n        loaded_df[\"ppid\"] = loaded_df.ppid_session_dataname.map(lambda x: x.split(\"_s0\")[0])\n\n    # Merge with existing tables\n    log_or_trial = (\n        \"UXFData.FaceSim.TrialResults\" in table_name\n        or \"UXFData.FaceSim.OtherTrialData\" in table_name\n        or \"UXFData.FaceSim.SessionLog\" in table_name\n    )\n    merge = merge and not log_or_trial\n    # *.TrialResults &amp; *.SessionLog tables are relatively big and should not be merged\n    df_split = None  # init / this is due to the updated protocol, how data is written from May 2023\n    table_name_split = None  # init\n    if merge:\n        if table_name == \"UXFData.FaceSim.OtherSessionData\":\n            table_name_split = table_name.replace(\"OtherSessionData\", \"ParticipantDetails\")\n            df_split = loaded_df[loaded_df.ppid_session_dataname.str.contains(\"_participant_details\")].reset_index(\n                drop=True\n            )\n            loaded_df = loaded_df[~loaded_df.ppid_session_dataname.str.contains(\"_participant_details\")].reset_index(\n                drop=True\n            )\n            # Remove empty columns in dfs\n            loaded_df = loaded_df.dropna(axis=1, how=\"all\")\n            df_split = df_split.dropna(axis=1, how=\"all\")\n\n            df_split, _ = merge_tables(df=df_split, table_name=table_name_split)\n\n        loaded_df, merge = merge_tables(df=loaded_df, table_name=table_name)\n\n    if table_name == \"UXFData.FaceSim.TrialResults\":\n        # Merge 'TrialResults' with 'OtherTrialData' table\n        table_name_other = table_name.replace(\"TrialResults\", \"OtherTrialData\")\n        df_other = load_table_from_dynamodb(table_name=table_name_other, merge=False, save=False)\n        if set(loaded_df.columns) != set(df_other.columns):\n            msg = \"No column match of 'TrialResults' &amp; 'OtherTrialData' tables!\"\n            raise ValueError(msg)\n        loaded_df = pd.concat([df_other, loaded_df], ignore_index=True)\n\n        if loaded_df.duplicated().any():\n            cprint(f\"Dropping {loaded_df.duplicated().sum()} duplicates ...\", col=\"b\")\n            loaded_df = loaded_df.drop_duplicates(ignore_index=True)\n\n        # Remove column 'trial_results_location_0'\n        loaded_df = loaded_df.drop(columns=[\"trial_results_location_0\"])\n\n    if save:\n        # Save table\n        date_tag = str(datetime.today().date())\n        path_to_save = Path(paths.data.MAIN, f\"{date_tag}_{table_name}.csv\")\n\n        i = 0\n        while log_or_trial and path_to_save.exists():\n            # Do not overwrite *.TrialResults &amp; *.SessionLog tables\n            path_to_save = Path(str(path_to_save.replace(date_tag, f\"{date_tag}{ascii_lowercase[i]}\")))\n            i += 1\n\n        loaded_df.to_csv(path_to_save, index=False)\n        path_to_save_split = None  # init\n        if df_split is not None:\n            path_to_save_split = Path(str(path_to_save.replace(table_name, table_name_split)))\n            df_split.to_csv(path_to_save_split, index=False)\n\n        if merge:\n            # Move former tables to archive\n            archive_former_tables(path_to_save=path_to_save, table_name=table_name)\n            if path_to_save_split is not None:\n                archive_former_tables(path_to_save=path_to_save_split, table_name=table_name_split)\n\n    return loaded_df\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.load_trial_results_from_dynamodb","title":"load_trial_results_from_dynamodb","text":"<pre><code>load_trial_results_from_dynamodb(\n    bucket_name: str | None = \"facesimdb\",\n    via_s3: bool = False,\n    save: bool = True,\n    verbose: bool = True,\n) -&gt; DataFrame\n</code></pre> <p>Load all trial-results from <code>DynamoDB</code>.</p> Blog posts on getting data from DynamoDB with Python and Boto3 <p>https://www.fernandomc.com/posts/ten-examples-of-getting-data-from-dynamodb-with-python-and-boto3/ https://dashbird.io/blog/aws-s3-python-tricks/</p> <p>Also, check out this <code>Stack Overflow</code> post:</p> <p>https://stackoverflow.com/questions/10450962/how-can-i-fetch-all-items-from-a-dynamodb-table-without-specifying-the-primary-k</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame with all trial results</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def load_trial_results_from_dynamodb(\n    bucket_name: str | None = \"facesimdb\",\n    via_s3: bool = False,\n    save: bool = True,\n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load all trial-results from `DynamoDB`.\n\n    ??? tip \"Blog posts on getting data from DynamoDB with Python and Boto3\"\n\n        https://www.fernandomc.com/posts/ten-examples-of-getting-data-from-dynamodb-with-python-and-boto3/\n        https://dashbird.io/blog/aws-s3-python-tricks/\n\n        Also, check out this `Stack Overflow` post:\n\n        https://stackoverflow.com/questions/10450962/how-can-i-fetch-all-items-from-a-dynamodb-table-without-specifying-the-primary-k\n\n    :return: Pandas DataFrame with all trial results\n    \"\"\"\n    if not via_s3:\n        # Get all trial results directly from DynamoDB\n        return load_table_from_dynamodb(table_name=\"UXFData.FaceSim.TrialResults\", save=save)\n\n    # Download trial results from S3 (after export of table to S3)\n    s3 = boto3.client(\"s3\")\n\n    if bucket_name is None:\n        bucket_list = s3.list_buckets()[\"Buckets\"]\n\n        if len(bucket_list) &gt; 1:\n            # TODO: implement choice of bucket  # noqa: FIX002\n            bucket_name = None\n            msg = \"More than one bucket found. Implement selection functionality!\"\n            raise NotImplementedError(msg)\n        else:  # noqa: RET506\n            bucket_name = bucket_list[0][\"Name\"]  # == \"facesimdb\"\n\n    available_files = s3.list_objects(Bucket=bucket_name)\n\n    available_files = [\n        f for f in available_files[\"Contents\"] if (f[\"Key\"].endswith(\".json.gz\") and \"AWSDynamoDB/\" in f[\"Key\"])\n    ]\n\n    if verbose:\n        cprint(string=f\"Found following files to download in S3 bucket '{bucket_name}':\", fm=\"ul\")\n        print(\"\", *[f\"{f['LastModified']!s} : {f['Key']}\" for f in available_files], sep=\"\\n\\t&gt; \")\n\n    # Check for different download folders\n    data_folder = {d[\"Key\"].split(\"/data/\")[0].split(\"AWSDynamoDB/\")[-1] for d in available_files}\n\n    if len(data_folder) &gt; 1:\n        # In case there are multiple folders to download from, choose a folder\n        data_folder = list(data_folder)\n        cprint(string=\"Specify folder to download from:\", col=\"y\", fm=\"ul\")\n        print(\"\", *[f\"{i}:\\t'{d}'\" for i, d in enumerate(data_folder)], sep=\"\\n\\t\")\n        f_idx = cinput(string=\"\\nType index of folder you want to download from: \", col=\"y\")\n        data_folder = data_folder[int(f_idx)]\n\n    else:\n        data_folder = data_folder.pop()\n\n    available_files = [f for f in available_files if data_folder in f[\"Key\"]]  # filter for folder\n\n    for s3_file in available_files:\n        # Download file\n        file_date = str(s3_file[\"LastModified\"].date())\n        p2_store = Path(paths.data.main.s3, \"TrialResults\", file_date, s3_file[\"Key\"].split(\"/\")[-1])\n        if p2_store.is_file():\n            cprint(string=f\"File '{p2_store}' already exists. Skipping download.\", col=\"g\")\n            continue\n        p2_store.parent.mkdir(parents=True, exist_ok=True)\n\n        s3.download_file(Bucket=bucket_name, Key=s3_file[\"Key\"], Filename=p2_store)\n        print(\"Downloaded file to:\", p2_store)\n\n    cprint(string=\"Running now the following function: read_and_convert_s3_results_json_data() ...\", col=\"y\")\n    return read_and_convert_s3_results_json_data(verbose=verbose)\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Run the main function of <code>read_data.py</code>.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Run the main function of `read_data.py`.\"\"\"\n    table_list = [\n        \"UXFData.FaceSim.\" + name\n        for name in [\n            \"OtherSessionData\",\n            \"ParticipantDetails\",\n            \"Settings\",\n            \"SessionLog\",\n            \"TrialResults\",\n            \"OtherTrialData\",\n        ]\n    ]  # \"OtherTrialData\" must be last &amp; after \"TrialResults\"\n\n    if FLAGS.mss:\n        # Set all other boolean FLAGS to False\n        for flag in FLAGS.__dict__:\n            if flag not in {\"mss\", \"set_nr\", \"triplets\"}:\n                setattr(FLAGS, flag, False)\n            # FLAGS.load = FLAGS.delete FLAGS.plot = FLAGS.pilot = FLAGS.verbose = False\n\n        update_triplet_table_on_dynamodb_multi_sub_sample(\n            session=f\"{FLAGS.set_nr[0]}D\", set_finalised_triplets_to_g=True\n        )\n\n    if FLAGS.load:\n        for table_name in table_list[:-1]:  # \"*.OtherTrialData\" will be merged with \"*.TrialResults\"\n            tab = load_table_from_dynamodb(table_name=table_name, save=True, merge=True)\n            if FLAGS.verbose:\n                print(tab)\n\n    # Delete all items in all tables (but *TripletsIDB.*D)\n    if FLAGS.delete:\n        for table_name in table_list:\n            delete_all_items_in_table_on_dynamodb(table_name=table_name)\n\n    if FLAGS.verbose:\n        cprint(string=f\"\\nLoad data of Set{FLAGS.set_nr} ...\", col=\"b\")\n        tab = read_trial_results_of_set(set_nr=FLAGS.set_nr, clean_trials=False, verbose=True)\n        print(tab)\n\n    if FLAGS.triplets:\n        get_current_state_of_triplets(session=FLAGS.triplets, pilot=FLAGS.pilot, plot=FLAGS.plot)\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.merge_tables","title":"merge_tables","text":"<pre><code>merge_tables(\n    df: DataFrame, table_name: str\n) -&gt; tuple[DataFrame, bool]\n</code></pre> <p>Merge a given table (<code>df</code>) with an existing table of the given table name.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def merge_tables(df: pd.DataFrame, table_name: str) -&gt; tuple[pd.DataFrame, bool]:\n    \"\"\"Merge a given table (`df`) with an existing table of the given table name.\"\"\"\n    cprint(string=\"Merging tables ...\", col=\"b\")\n    merge = True  # init\n    df2 = load_local_table(table_name=table_name)\n    if df2 is None:\n        merge = False\n        merged_df = df\n    else:\n        if set(df.columns) != set(df2.columns):\n            msg = \"No column match of downloaded &amp; local tables!\"\n            raise ValueError(msg)\n        merged_df = pd.concat([df2, df], ignore_index=True)\n\n        if merged_df.duplicated().any():\n            cprint(string=f\"Dropping {merged_df.duplicated().sum()} duplicates ...\", col=\"b\")\n            merged_df = merged_df.drop_duplicates(ignore_index=True)\n\n    return merged_df, merge\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.plot_triplet_matrix","title":"plot_triplet_matrix","text":"<pre><code>plot_triplet_matrix(\n    triplet_table: DataFrame, n_faces: int\n) -&gt; Figure\n</code></pre> <p>Plot matrix of triplets.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def plot_triplet_matrix(triplet_table: pd.DataFrame, n_faces: int) -&gt; plt.Figure:\n    \"\"\"Plot matrix of triplets.\"\"\"\n    triplet_table[\"triplet\"] = triplet_table.triplet.map(lambda x: x.split(\"_\"))\n    sampling_mat = np.zeros((n_faces, n_faces))\n    for _i, triplet_row in tqdm(iterable=triplet_table.iterrows(), desc=\"Fill count matrix of triplets\"):\n        triplet = [int(f_id) for f_id in triplet_row.triplet]\n        for comb in combinations(triplet, r=2):\n            sampling_mat[comb[0] - 1, comb[1] - 1] += 1\n            sampling_mat[comb[1] - 1, comb[0] - 1] += 1\n\n    sampling_mat /= n_faces - 2\n    np.fill_diagonal(sampling_mat, np.nan)\n\n    # Plot the sampling matrix\n    fig, ax = plt.subplots(num=f\"{datetime.now().replace(microsecond=0)} | Sampled triplets\", figsize=(10, 8))\n    h = sns.heatmap(sampling_mat, cmap=\"YlOrBr\", vmin=0, vmax=1, ax=ax)\n    h.set(\n        title=f\"{datetime.now().replace(microsecond=0)} | \"\n        f\"{len(triplet_table) / np.math.comb(n_faces, 3):.1%} Sampled triplets | \"\n        f\"{np.nanmin(sampling_mat):.1%}-{np.nanmax(sampling_mat):.1%} (min-max)\"\n    )\n    fig.tight_layout()\n    plt.show()\n    return fig\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.read_and_convert_s3_results_json_data","title":"read_and_convert_s3_results_json_data","text":"<pre><code>read_and_convert_s3_results_json_data(\n    verbose: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Get the full trial table of the main study from memory.</p> <p>This table must be constructed from several <code>json</code> tables downloaded from <code>S3</code>.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Be verbose or not</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Processed trial table</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def read_and_convert_s3_results_json_data(verbose: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Get the full trial table of the main study from memory.\n\n    This table must be constructed from several `json` tables downloaded from `S3`.\n\n    :param verbose: Be verbose or not\n    :return: Processed trial table\n    \"\"\"\n    # Define the session path\n    p2_s3 = Path(paths.data.main.s3, \"TrialResults\")\n\n    if verbose:\n        tree(p2_s3)\n\n    # Find the correct directory\n    trial_result_dirs = [d for d in os.listdir(p2_s3) if (d.startswith(\"2022-\") and not d.endswith(\".csv\"))]\n\n    if len(trial_result_dirs) &gt; 1:\n        cprint(string=\"Choose random *.json* file from folder which should be unpacked:\", col=\"y\")\n        trial_result_dirs = Path(browse_files(initialdir=p2_s3)).parent  # , filetypes=\".json\"))\n    else:\n        trial_result_dirs = trial_result_dirs.pop()\n    trial_result_dirs = Path(p2_s3, trial_result_dirs)\n\n    # Set path to processed trial table\n    trial_result_full_table_path = Path(paths.data.MAIN, f\"{trial_result_dirs.name}_UXFData.FaceSim.TrialResults.csv\")\n\n    # Check if the full (concatenated) table is already there\n    convert_json_to_csv = True\n    append_table = False\n    table_processed = None  # one table to save all data\n    if trial_result_full_table_path.is_file():\n        table_processed = pd.read_csv(trial_result_full_table_path)\n        cprint(string=f\"\\nTable with trial results already exists: {trial_result_full_table_path}\", col=\"g\")\n        append_table = convert_json_to_csv = ask_true_false(\n            question=\"Do you want to append new data to the existing table?\", col=\"y\"\n        )\n\n    if convert_json_to_csv:\n        # Read json files\n        trial_result_files = [f for f in os.listdir(trial_result_dirs) if (f.endswith((\".json\", \".json.gz\")))]\n\n        type_dict = None  # init DynamoDB type dict\n        for json_file_path in tqdm(trial_result_files, desc=\"Read json files\", position=0):\n            p2_json = Path(trial_result_dirs, json_file_path)\n\n            # Process json file via pandas\n            table_raw = pd.read_json(p2_json, lines=True)\n\n            if type_dict is None:\n                td = [(col, next(iter(table_raw.iloc[0][\"Item\"][col].keys()))) for col in table_raw.iloc[0][\"Item\"]]\n\n                td = pd.DataFrame(td, columns=[\"col_name\", \"type\"])\n                type_dict = dict(zip(td[\"col_name\"], td[\"type\"].map(DT_MAP), strict=True))\n\n            # Remove type info from json file\n            for row in tqdm(table_raw.values, desc=f\"Read rows of '{json_file_path}'\", position=1):\n                current_row = row.item()\n\n                if append_table:\n                    ppid = current_row[\"ppid\"][\"S\"]\n                    sys_t = convert_date_time(date_time=current_row[\"SystemDateTime_BeginTrial\"][\"S\"])\n                    if sys_t in table_processed.loc[table_processed.ppid == ppid].SystemDateTime_BeginTrial.to_list():\n                        continue\n\n                trial_tab = pd.DataFrame(current_row)\n                # append empty row to table\n                copy_row = trial_tab.iloc[0:1].copy()\n                copy_row[~pd.isna(copy_row)] = np.nan\n                trial_tab = pd.concat([trial_tab, copy_row])\n\n                # Write non-nan-value in empty row for each column\n                for col in trial_tab.columns:\n                    trial_tab.iloc[-1][col] = trial_tab[col].dropna().item()\n\n                # Keep only filled row\n                trial_tab = trial_tab.iloc[-1:]\n\n                # Exclude empty rows\n                if \"triplet\" not in trial_tab.columns:\n                    continue\n                if (\n                    (trial_tab.head1 == trial_tab.head2).all()\n                    and (trial_tab.head2 == trial_tab.head3).all()\n                    and not trial_tab.head2.item()\n                ) or (not trial_tab.triplet.item()):\n                    # These rows are empty after the experiment was stopped early, usually after 3 missed\n                    # catch trials\n                    continue\n\n                # Concatenate to big table\n                table_processed = trial_tab if table_processed is None else pd.concat([table_processed, trial_tab])\n                # , ignore_index=True)\n\n        # Fill empty slots with nan\n        table_processed = table_processed.replace(\"\", np.nan)\n\n        # Adapt dtypes\n        table_processed = table_processed.astype(type_dict)\n\n        # Remove unnecessary columns &amp; sort the rest\n        table_processed = table_processed[SORTED_COLS]\n\n        # Solve date issue\n        table_processed.SystemDateTime_BeginTrial = table_processed.SystemDateTime_BeginTrial.map(convert_date_time)\n\n        # Sort rows table by ppid and start time/date of trial\n        table_processed = table_processed.sort_values(\n            by=[\"SystemDateTime_BeginTrial\", \"ppid\"], axis=0, ascending=True\n        ).reset_index(drop=True)\n\n        # Save table\n        table_processed.to_csv(trial_result_full_table_path, index=False)\n\n    return table_processed\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.read_logs_of_set","title":"read_logs_of_set","text":"<pre><code>read_logs_of_set(set_nr: str) -&gt; DataFrame\n</code></pre> <p>Read all log tables of a given Set number.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def read_logs_of_set(set_nr: str) -&gt; pd.DataFrame:\n    \"\"\"Read all log tables of a given Set number.\"\"\"\n    where_tab = where_to_find_trial_and_log_data(set_nr=set_nr, update_table=False)\n    log_table = None  # init\n    for p2_log_table in where_tab[where_tab.type == \"SessionLog\"].table_name:\n        print(\"Load:\", p2_log_table)\n        if log_table is None:\n            log_table = load_local_table(table_name=p2_log_table)\n\n        else:\n            print(\"Append:\", p2_log_table)\n            log_table = log_table.append(load_local_table(table_name=p2_log_table))\n\n    # Remove participants from table which are not part of given Set (set_nr)\n    prolific_set_ppids = read_prolific_participant_data(set_nr=set_nr)[\"Participant id\"]\n    if not (log_table.ppid.isin(prolific_set_ppids)).all():\n        cprint(\n            string=f\"{len(log_table[~log_table.ppid.isin(prolific_set_ppids)])} participant(s) are in the \"\n            f\"log table, but are not part of Set{set_nr}! They will be dropped ...\",\n            col=\"y\",\n        )\n        log_table = log_table[log_table.ppid.isin(prolific_set_ppids)]  # drop (out-of-set_nr) ppids\n        log_table = log_table.reset_index(drop=True)\n\n    return log_table\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.read_participant_data","title":"read_participant_data","text":"<pre><code>read_participant_data(process: bool = False) -&gt; DataFrame\n</code></pre> <p>Get the full participant table of the main study.</p> <p>Table name: <code>'*_UXFData.FaceSim.ParticipantDetails_processed.csv'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>process</code> <code>bool</code> <p>True: force (re-)processing of data</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>participant table</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def read_participant_data(process: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Get the full participant table of the main study.\n\n    Table name: `'*_UXFData.FaceSim.ParticipantDetails_processed.csv'`.\n\n    :param process: True: force (re-)processing of data\n    :return: participant table\n    \"\"\"\n    # Load full table\n    p2_participant_files = list(Path(paths.data.MAIN).glob(\"*UXFData.FaceSim.ParticipantDetails.csv\"))\n\n    if len(p2_participant_files) &gt; 1:\n        p2_participant_files = Path(browse_files(initialdir=paths.data.MAIN, filetypes=\"*.csv\"))\n    else:\n        p2_participant_files = p2_participant_files.pop()\n\n    p2_raw_participant_files = Path(\n        str(p2_participant_files).replace(paths.data.MAIN, paths.data.main.archive).replace(\".csv\", \"_raw.csv\")\n    )\n\n    # Check if the full (concatenated) table is already there\n    if p2_raw_participant_files.exists() and not process:\n        # We know that p2_participant_files.is_file() is True\n        table_processed = pd.read_csv(p2_participant_files)\n\n    else:  # process == True:\n        table_processed = load_local_table(table_name=p2_participant_files.name)\n\n        # Extract from ppid_session_dataname the ppid\n        if \"ppid\" not in table_processed.columns:\n            table_processed[\"ppid\"] = table_processed.ppid_session_dataname.map(lambda x: x.split(\"_s0\")[0])\n\n        # Remove debug &amp; UnlockTriplets (has NaN's in 'group_exp' column) users\n        table_processed = table_processed.loc[\n            table_processed.ppid.isin([p for p in table_processed.ppid if \"debug\" not in p])\n        ]\n        table_processed = table_processed[table_processed.ppid != \"UnlockTriplets\"].reset_index(drop=True)\n\n        # Archive unprocessed table\n        p2_participant_files.rename(p2_raw_participant_files)\n\n        # Save processed table\n        table_processed.to_csv(p2_participant_files, index=False)\n\n    return table_processed\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.read_pilot_data","title":"read_pilot_data","text":"<pre><code>read_pilot_data(\n    clean_trials: bool = False, verbose: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Get the full trial table of the pilot study (version 2).</p> <p>This table is downloaded as <code>csv</code> in one sweep from <code>DynamoDB</code>.</p> <p>Parameters:</p> Name Type Description Default <code>clean_trials</code> <code>bool</code> <p>clean trials (remove trials with no response, etc.)</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>be verbose or not</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>processed trial table</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def read_pilot_data(clean_trials: bool = False, verbose: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Get the full trial table of the pilot study (version 2).\n\n    This table is downloaded as `csv` in one sweep from `DynamoDB`.\n\n    :param clean_trials: clean trials (remove trials with no response, etc.)\n    :param verbose: be verbose or not\n    :return: processed trial table\n    \"\"\"\n    # Check data dir\n    if verbose:\n        tree(paths.data.pilot.v2)\n\n    # Load full table\n    trial_result_files = list(Path(paths.data.pilot.v2).glob(\"*UXFData.FaceSim.TrialResults.csv\"))\n    if len(trial_result_files) &gt; 1:\n        trial_result_files = browse_files(initialdir=paths.data.pilot.v2, filetypes=\"*.csv\")\n    else:\n        trial_result_files = trial_result_files.pop()\n\n    # Check if the full (concatenated) table is already there\n    trial_result_full_table_path = str(trial_result_files).replace(\"TrialResults\", \"TrialResults_processed\")\n    if Path(trial_result_full_table_path).is_file():\n        table_processed = pd.read_csv(trial_result_full_table_path)\n\n    else:\n        table_processed = pd.read_csv(trial_result_files)\n\n    # Remove unnecessary columns &amp; sort the rest\n    table_processed = table_processed[SORTED_COLS_PILOT]\n\n    # Sort rows\n    table_processed = table_processed.sort_values(\n        by=[\"ppid\", \"trial_num\"], axis=0, ascending=True, inplace=False\n    ).reset_index(drop=True)\n\n    # Save table\n    table_processed.to_csv(trial_result_full_table_path, index=False)\n\n    if clean_trials:\n        cath_head_trials = 0.0\n        table_processed = table_processed[table_processed.block_num &gt; 1]  # remove training\n        table_processed = table_processed[table_processed.catch_head == cath_head_trials]  # remove catch trials\n        table_processed = table_processed[table_processed.head_odd != 0]  # remove time-outs\n        table_processed = table_processed[~table_processed.ppid.isin(dropouts_pilot_v2)]\n\n    return table_processed.reset_index(drop=True)\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.read_pilot_participant_data","title":"read_pilot_participant_data","text":"<pre><code>read_pilot_participant_data() -&gt; DataFrame\n</code></pre> <p>Get the full participant table of the pilot study (version 2).</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>participant table</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def read_pilot_participant_data() -&gt; pd.DataFrame:\n    \"\"\"\n    Get the full participant table of the pilot study (version 2).\n\n    :return: participant table\n    \"\"\"\n    # Load full table\n    participant_files = list(Path(paths.data.pilot.v2).glob(\"*UXFData.FaceSim.ParticipantDetails.csv\"))\n    if len(participant_files) &gt; 1:\n        participant_files = browse_files(initialdir=paths.data.pilot.v2, filetypes=\"*.csv\")\n    else:\n        participant_files = participant_files.pop()\n\n    # Check if the full (concatenated) table is already there\n    participant_full_table_path = Path(\n        str(participant_files).replace(\"ParticipantDetails\", \"ParticipantDetails_processed\")\n    )\n    if participant_full_table_path.is_file():\n        table_processed = pd.read_csv(participant_full_table_path, index_col=0)\n\n    else:\n        table_processed = pd.read_csv(participant_files)\n\n        # Exchange ppid_session_dataname with ppid\n        table_processed = table_processed.rename(columns={\"ppid_session_dataname\": \"ppid\"})\n        table_processed.ppid = table_processed.ppid.replace(\"_s001_participant_details\", \"\", regex=True)\n\n        # Remove UnlockTriplets user (has NaN's in 'group_exp' column)\n        table_processed = table_processed[table_processed.ppid != \"UnlockTriplets\"].reset_index(drop=True)\n        # table_processed.drop(index=table_processed[table_processed.group_exp.isna()].index, axis=1,\n        #                      inplace=True)  # This should be the UnlockTriplets 'User'\n\n        # Clean up columns\n        dtype_map = {\"N\": np.int64, \"S\": str}  # \"D\": np.datetime64\n        for col in table_processed.columns:\n            if col == \"ppid\":\n                continue\n\n            # Get dtype\n            cell = table_processed[col].iloc[0]\n            dt = cell[cell.find(\":\") - 2 : cell.find(\":\") - 1]\n            # Update cells in column\n            table_processed[col] = (\n                table_processed[col]\n                .map(lambda x: x.replace('[{\"' + f'{dt}\":\"', \"\"))  # noqa: B023\n                .replace('\"}]', \"\", regex=True)\n                .astype(dtype_map[dt])\n            )\n\n        # Save processed table\n        table_processed.to_csv(participant_full_table_path)\n\n    return table_processed\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.read_prolific_participant_data","title":"read_prolific_participant_data","text":"<pre><code>read_prolific_participant_data(\n    set_nr: str | float, return_path: bool = False\n) -&gt; DataFrame | tuple[DataFrame, str]\n</code></pre> <p>Read the participant table of a given Set downloaded from Prolific.</p> <p>Parameters:</p> Name Type Description Default <code>set_nr</code> <code>str | float</code> <p>Prolific Set number 2. for 2D AND 3. for 3D</p> required <code>return_path</code> <code>bool</code> <p>if True also return the path to the file</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame | tuple[DataFrame, str]</code> <p>participant table of the given Prolific Set</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def read_prolific_participant_data(\n    set_nr: str | float, return_path: bool = False\n) -&gt; pd.DataFrame | tuple[pd.DataFrame, str]:\n    \"\"\"\n    Read the participant table of a given Set downloaded from Prolific.\n\n    :param set_nr: Prolific Set number 2.* for 2D AND 3.* for 3D\n    :param return_path: if True also return the path to the file\n    :returns: participant table of the given Prolific Set\n    \"\"\"\n    p_files = [\n        f for f in os.listdir(paths.data.main.prolific) if f.endswith(\".csv\") and f\"Participants-Set{set_nr}_\" in f\n    ]\n    if not p_files:  # empty\n        cprint(string=f\"No participant table found for Set{set_nr}!\", col=\"r\")\n        return None\n    if len(p_files) &gt; 1:\n        cprint(string=f\"Choose the corresponding Participant file of Set{set_nr}!\", col=\"b\")\n        cprint(string=\"Note: There should be only one file per Set!\", col=\"y\")\n        p_files = browse_files(initialdir=paths.data.main.prolific, filetypes=\"*.csv\")\n    else:\n        p_files = p_files.pop()\n\n    # Read table\n    full_path = Path(paths.data.main.prolific, p_files)\n    ppid_prolific_table = pd.read_csv(full_path)\n\n    # Add decision column (if not there)\n    if \"decision\" not in ppid_prolific_table.columns:\n        ppid_prolific_table[\"decision\"] = np.nan\n\n    if return_path:\n        return ppid_prolific_table, str(full_path)\n    return ppid_prolific_table\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.read_trial_results_of_participant","title":"read_trial_results_of_participant","text":"<pre><code>read_trial_results_of_participant(\n    ppid: str,\n    clean_trials: bool = False,\n    verbose: bool = True,\n) -&gt; DataFrame\n</code></pre> <p>Read all trial results of a given participant.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def read_trial_results_of_participant(ppid: str, clean_trials: bool = False, verbose: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Read all trial results of a given participant.\"\"\"\n    set_nrs = get_participant_set_numbers(ppid=ppid)\n    tr_table = None  # init\n    for set_nr in set_nrs:\n        if tr_table is None:\n            tr_table = read_trial_results_of_set(set_nr=set_nr, clean_trials=clean_trials, verbose=verbose)\n        else:\n            tr_table = pd.concat(\n                objs=[tr_table, read_trial_results_of_set(set_nr=set_nr, clean_trials=clean_trials, verbose=verbose)]\n            )\n\n    return tr_table[tr_table.ppid == ppid].reset_index(drop=True)\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.read_trial_results_of_session","title":"read_trial_results_of_session  <code>cached</code>","text":"<pre><code>read_trial_results_of_session(\n    session: str,\n    clean_trials: bool = False,\n    drop_subsamples: bool = True,\n    verbose: bool = True,\n) -&gt; DataFrame\n</code></pre> <p>Read all trial results of a given session.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>@lru_cache(maxsize=24)\ndef read_trial_results_of_session(\n    session: str, clean_trials: bool = False, drop_subsamples: bool = True, verbose: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"Read all trial results of a given session.\"\"\"\n    if session.upper() not in params.SESSIONS:\n        msg = f\"Session '{session}' not in {params.SESSIONS}!\"\n        raise ValueError(msg)\n\n    set_nrs_of_session = [s for s in get_list_of_acquired_sets() if s.split(\".\")[0] == session[0]]\n    set_nr_sub_sample = f\"{session[0]}.20\"\n    if drop_subsamples:\n        set_nrs_of_session.remove(set_nr_sub_sample)\n    else:\n        cprint(string=f\"Multi-subsample Set-{set_nr_sub_sample} is included in the returned table!\", col=\"r\")\n\n    tr_table = None  # init\n    for set_nr in set_nrs_of_session:\n        if tr_table is None:\n            tr_table = read_trial_results_of_set(set_nr=set_nr, clean_trials=clean_trials, verbose=verbose)\n        else:\n            tr_table = pd.concat(\n                [tr_table, read_trial_results_of_set(set_nr=set_nr, clean_trials=clean_trials, verbose=verbose)],\n                ignore_index=True,\n            )\n\n    return tr_table\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.read_trial_results_of_set","title":"read_trial_results_of_set","text":"<pre><code>read_trial_results_of_set(\n    set_nr: str,\n    clean_trials: bool = True,\n    verbose: bool = True,\n) -&gt; DataFrame\n</code></pre> <p>Read all trial results of a given Set number.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def read_trial_results_of_set(set_nr: str, clean_trials: bool = True, verbose: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Read all trial results of a given Set number.\"\"\"\n    where_tab = where_to_find_trial_and_log_data(set_nr=set_nr, update_table=False)\n    tr_table = None  # init\n    for p2_tr_table in where_tab[where_tab.type == \"TrialResults\"].table_name:\n        if verbose:\n            print(\"Load:\", p2_tr_table)\n        if tr_table is None:\n            tr_table = _read_trial_results(process=False, date=p2_tr_table.split(\"_\")[0])\n\n        else:\n            if verbose:\n                print(\"Append:\", p2_tr_table)\n            tr_table = tr_table.append(\n                _read_trial_results(process=False, date=p2_tr_table.split(\"_\")[0]), ignore_index=True\n            )\n\n    # Remove participants from table which are not part of given Set (set_nr)\n    prolific_set_ppids = read_prolific_participant_data(set_nr=set_nr)[\"Participant id\"]\n\n    th_multi_sub_sample: int = 20\n    if not set(tr_table.ppid).issubset(set(prolific_set_ppids)) and int(set_nr.split(\".\")[-1]) &lt; th_multi_sub_sample:\n        # Ignore Sets of multi-sub-sample\n        if verbose:\n            cprint(\n                string=f\"{len(set(tr_table.ppid) - set(prolific_set_ppids))} participant(s) are in the \"\n                f\"trial results table, but are not part of Set{set_nr}! They will be dropped ...\",\n                col=\"y\",\n            )\n        tr_table = tr_table[tr_table.ppid.isin(prolific_set_ppids)]  # drop (out-of-set_nr) ppids\n        tr_table = tr_table.reset_index(drop=True)\n\n    if clean_trials:\n        tr_table = remove_invalid_trials(trial_results_table=tr_table, verbose=verbose)\n\n    # Set column types\n    if tr_table.caught.isna().any() or (tr_table.caught == \"\").any():\n        return tr_table\n    # Convert 'caught' column only to boolean, when there are no NaNs, i.e., missing trials\n    # This is due to pd.Series([True, False, np.nan]).astype(bool) -&gt; pd.Series([True, False, True]), ...\n    # ..and to this: pd.Series([True, False, \"\"]).astype(bool) -&gt; pd.Series([True, False, False])\n    return tr_table.astype({\"caught\": bool})\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.remove_invalid_trials","title":"remove_invalid_trials","text":"<pre><code>remove_invalid_trials(\n    trial_results_table: DataFrame, verbose: bool = True\n) -&gt; DataFrame\n</code></pre> <p>Remove invalid trials from a given trial results table.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def remove_invalid_trials(trial_results_table: pd.DataFrame, verbose: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Remove invalid trials from a given trial results table.\"\"\"\n    original_len = len(trial_results_table)\n\n    # Remove training trials\n    trial_results_table = trial_results_table[trial_results_table.block_num &gt; 1]\n\n    # Remove trials without data\n    trial_results_table = trial_results_table[~trial_results_table.caught.isna()]\n    trial_results_table = trial_results_table[trial_results_table.caught != \"\"]\n    trial_results_table = trial_results_table.astype({\"caught\": bool})\n\n    # Remove trials of blocks with catches\n    for ppid_session_dataname, tr_table in tqdm(\n        trial_results_table.groupby(\"ppid_session_dataname\"),\n        desc=\"Clean trial results table\",\n        total=len(trial_results_table.ppid_session_dataname.unique()),\n    ):\n        _ppid = tr_table.ppid.unique().item()  # == ppid_session_dataname.split(\"_\")[0]\n\n        # Remove all trials of participants with 3+ catches\n        n_catches = tr_table.caught.sum()\n        catch_threshold = 3\n        if n_catches &gt;= catch_threshold:\n            if verbose:\n                cprint(f\"Participant {_ppid} has {n_catches} missed catch trials. Removing all trials ...\", col=\"y\")\n            trial_results_table = trial_results_table[\n                trial_results_table.ppid_session_dataname != ppid_session_dataname\n            ]\n            continue  # all blocks were removed, hence we can jump to the next participant session\n\n        for b_idx, tr_block in tr_table.groupby(\"block_num\"):\n            if tr_block.caught.sum() &gt; 0:\n                if verbose:\n                    cprint(\n                        f\"Participant {_ppid} has missed catch trials in block {int(b_idx)}. Removing block ...\",\n                        col=\"y\",\n                    )\n                trial_results_table = trial_results_table[\n                    ~(\n                        (trial_results_table.ppid_session_dataname == ppid_session_dataname)\n                        &amp; (trial_results_table.block_num == b_idx)\n                    )\n                ]\n\n    # Remove catch trials\n    catch_head_trial = 0.0\n    trial_results_table = trial_results_table[trial_results_table.catch_head == catch_head_trial]\n\n    # Remove time-outs\n    n_remove = (trial_results_table.head_odd == 0).sum()\n    if verbose and n_remove &gt; 0:\n        cprint(string=f\"{n_remove} time-out trials will be removed ...\", col=\"y\")\n    trial_results_table = trial_results_table[trial_results_table.head_odd != 0]\n\n    # TODO: Remove trials of unrealistic response times (&lt; X sec):  # noqa: FIX002\n    #  check: determine_threshold_for_minimal_response_time()  # noqa: ERA001\n    #  trial_results_table[trial_results_table.response_time &lt; params.MIN_RT_2D]  # params.MIN_RT_3D  # noqa: ERA001\n    #  n_rt_outliers = (trial_results_table.response_time &lt; params.MIN_RT_2D).sum()  # noqa: ERA001\n    pass\n\n    # TODO: Remove trials with monotonous choice behavior (&gt; Y-times same side).  # noqa: FIX002\n    #  This could also entail repeating patterns (e.g., left-right-left-right...)\n    pass\n\n    # TODO: Define other criteria (e.g., BQS, etc.)  # noqa: FIX002\n    pass\n\n    if verbose:\n        n_removed = original_len - len(trial_results_table)\n        cprint(\n            f\"\\n{n_removed} of original {original_len} ({n_removed / original_len:.1%}) trials were removed ...\",\n            col=\"y\",\n            fm=\"bo\",\n        )\n\n    return trial_results_table.reset_index(drop=True)\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.save_merged_tables_of_set","title":"save_merged_tables_of_set","text":"<pre><code>save_merged_tables_of_set(set_nr: str) -&gt; None\n</code></pre> <p>Merge all tables of a given type (\"<code>TrialResults</code>\", \"<code>SessionLog</code>\") in a given Set.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def save_merged_tables_of_set(set_nr: str) -&gt; None:\n    \"\"\"Merge all tables of a given type (\"`TrialResults`\", \"`SessionLog`\") in a given Set.\"\"\"\n    where_tab = where_to_find_trial_and_log_data(set_nr=set_nr, update_table=False)\n\n    for table_type in [\"TrialResults\", \"SessionLog\"]:\n        if table_type == \"TrialResults\":\n            table_merged = read_trial_results_of_set(set_nr=set_nr, clean_trials=False, verbose=True)\n        else:\n            table_merged = read_logs_of_set(set_nr=set_nr)\n\n        table_name = where_tab[where_tab.type == table_type].table_name.values[0]  # noqa: PD011\n        prefix_date, suffix = table_name.split(\"_\")\n        if table_type == \"TrialResults\":\n            table_merged = table_merged.drop_duplicates().reset_index(drop=True)\n        else:\n            table_merged = table_merged.drop_duplicates(subset=[\"ppid\"]).reset_index(drop=True)\n        prefix_date_m = prefix_date[:10] + \"m\"\n        table_merged.to_csv(Path(paths.data.MAIN, f\"{prefix_date_m}_{suffix}\"), index=False)\n\n        # Move other tables to \"archive\" folder\n        for table_name in where_tab[where_tab.type == table_type].table_name:\n            Path(paths.data.MAIN, table_name).rename(Path(paths.data.MAIN, \"archive\", table_name))\n\n    # Remove table location file in where_to_find_trial_and_log_data()\n    Path(paths.data.MAIN, \"Where_are_TrialResults_and_Logs.csv\").unlink()\n\n    cprint(\n        string=\"Tables are merged and saved, former tables are moved to 'archive' folder.\\n\"\n        f\"Consider renaming current tables with prefix '{prefix_date_m}_UXFData.FaceSim*.csv'.\\n\"\n        \"Then rerun where_to_find_trial_and_log_data()!\",\n        col=\"y\",\n    )\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.set_infix","title":"set_infix","text":"<pre><code>set_infix(set_nr: str) -&gt; str\n</code></pre> <p>Generate the Set infix (e.g., 's004' OR 's011') from a set number.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def set_infix(set_nr: str) -&gt; str:\n    \"\"\"Generate the Set infix (e.g., 's004' OR 's011') from a set number.\"\"\"\n    return f\"s{int(set_nr.split('.')[-1]):03d}\"\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.update_triplet_table_on_dynamodb","title":"update_triplet_table_on_dynamodb","text":"<pre><code>update_triplet_table_on_dynamodb(\n    session: str,\n    set_finalised_triplets_to_g: bool = False,\n    delete_done_triplets: bool = False,\n) -&gt; None\n</code></pre> <p>Update the triplet table on <code>DynamoDB</code>.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D' OR '3D'</p> required <code>set_finalised_triplets_to_g</code> <code>bool</code> <p>Set finalized triplets to 'G' (if not already done)</p> <code>False</code> <code>delete_done_triplets</code> <code>bool</code> <p>Whether to delete triplets that are done</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def update_triplet_table_on_dynamodb(\n    session: str, set_finalised_triplets_to_g: bool = False, delete_done_triplets: bool = False\n) -&gt; None:\n    \"\"\"\n    Update the triplet table on `DynamoDB`.\n\n    :param session: '2D' OR '3D'\n    :param set_finalised_triplets_to_g: Set finalized triplets to 'G' (if not already done)\n    :param delete_done_triplets: Whether to delete triplets that are done\n    :return: None\n    \"\"\"\n    if not ask_true_false(f\"\\nWas the latest trial data downloaded for session '{session}'?\"):\n        cprint(string=f\"Download the latest trial data of the '{session}' session first.\", col=\"r\")\n        return\n\n    open_triplets = finalized_triplets(session=session)\n\n    table_name = \"UXFData.FaceSim.TripletsIDB.\" + session.upper()\n\n    # Connect to DynamoDB\n    dynamodb = boto3.resource(\"dynamodb\", region_name=\"eu-central-1\")  # connect to DynamoDB\n    db_table = dynamodb.Table(table_name)\n\n    # Get key names\n    key_schema = db_table.key_schema\n    key_names = [k[\"AttributeName\"] for k in key_schema]\n\n    # Load current state of triplet table\n    cprint(string=f\"Loading current state of {session} triplet table ...\", col=\"b\")\n    response = db_table.scan()\n    data = response[\"Items\"]\n    while \"LastEvaluatedKey\" in response:\n        response = db_table.scan(ExclusiveStartKey=response[\"LastEvaluatedKey\"])\n        data.extend(response[\"Items\"])\n\n    df_current_state = pd.DataFrame(data)  # transform to pd.DataFrame\n\n    # Unlock triplets\n    locked: str = \"L\"  # locked symbol\n    if (df_current_state.status == locked).any():\n        cprint(\n            string=f\"\\nUnlocking {(df_current_state.status == locked).sum()} previously locked triplets ...\", col=\"b\"\n        )\n        for row in tqdm(data, desc=f\"Unlocking items in {table_name}\", colour=\"#02B580\"):\n            if row[\"status\"] != locked:\n                continue\n            _update_status_item(\n                dynamodb_table=db_table, table_name=table_name, data_row=row, key_names=key_names, new_status=\"U\"\n            )  # set \"L\" to \"U\"\n        cprint(string=f\"All previously locked triplets are now unlocked in {table_name}.\", col=\"g\")\n\n    # Set open triplets to \"U\"\n    cprint(string=f\"\\nResetting {len(open_triplets)} open triplets to 'U' ...\", col=\"b\")\n    for row in tqdm(data, desc=f\"Reset open triplets items in {table_name}\", colour=\"#E86A03\"):\n        if row[\"triplet_id\"] not in open_triplets or row[\"status\"] == \"U\":\n            continue\n        _update_status_item(\n            dynamodb_table=db_table, table_name=table_name, data_row=row, key_names=key_names, new_status=\"U\"\n        )  # reset open triplets\n    cprint(string=f\"All open triplets are now ready to be sampled in {table_name}.\", col=\"g\")\n\n    # Set finalized triplets to \"G\" (if not already done)\n    if set_finalised_triplets_to_g:\n        cprint(string=\"\\nSetting finalised triplets to 'G' ...\", col=\"b\")\n        for row in tqdm(data, desc=f\"Reset open triplets items in {table_name}\", colour=\"#8F29E8\"):\n            if row[\"triplet_id\"] not in open_triplets and row[\"status\"] != \"G\":\n                _update_status_item(\n                    dynamodb_table=db_table, table_name=table_name, data_row=row, key_names=key_names, new_status=\"G\"\n                )\n        cprint(string=f\"All finalised triplets are now set to 'G' in {table_name}.\", col=\"g\")\n\n    # Delete triplets that are done\n    if delete_done_triplets:\n        cprint(string=\"\\nDeleting triplets that are done ...\", col=\"b\")\n        # TODO: Test this (should work but will interfere with other functions as for cost-estimation)  # noqa: FIX002\n        msg = \"This is not tested yet. Be careful!\"\n        raise NotImplementedError(msg)\n        with db_table.batch_writer() as batch:\n            for row in tqdm(data, desc=f\"Deleting items in {table_name}\"):\n                if row[\"status\"] != \"G\":\n                    continue\n                batch.delete_item(Key=dict(zip(key_names, [row[key] for key in key_names], strict=True)))\n        cprint(string=f\"All done triplets deleted from {table_name}.\", col=\"g\")\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.update_triplet_table_on_dynamodb_multi_sub_sample","title":"update_triplet_table_on_dynamodb_multi_sub_sample","text":"<pre><code>update_triplet_table_on_dynamodb_multi_sub_sample(\n    session: str, set_finalised_triplets_to_g: bool = True\n) -&gt; None\n</code></pre> <p>Update triplet table on <code>DynamoDB</code> for the given session of the <code>multi-sampled-sub-sample</code>.</p> <p>Note: This asserts that all data on <code>DynamoDB</code> is only from the given session. Do not execute this function, if also data from other sessions is on <code>DynamoDB</code>.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def update_triplet_table_on_dynamodb_multi_sub_sample(session: str, set_finalised_triplets_to_g: bool = True) -&gt; None:\n    \"\"\"\n    Update triplet table on `DynamoDB` for the given session of the `multi-sampled-sub-sample`.\n\n    Note: This asserts that all data on `DynamoDB` is only from the given session.\n    Do not execute this function, if also data from other sessions is on `DynamoDB`.\n    \"\"\"\n    cprint(\n        f\"\\nUpdating triplet table on DynamoDB for the {session}-session of the multi-sampled-sub-sample ...\\n\",\n        col=\"y\",\n        fm=\"bo\",\n    )\n    if not ask_true_false(\n        question=f\"Are you sure you want to update the triplet table for the {session}-session on \"\n        f\"DynamoDB AND that only data of that session is currently on DynamoDB \"\n        f\"(this is asserted)? \"\n    ):\n        cprint(string=\"Aborting ...\", col=\"r\")\n        return\n\n    open_triplets = finalized_triplets_multi_sub_sample()\n\n    table_name = \"UXFData.FaceSim.TripletsIDB.\" + session.upper()\n\n    # Connect to DynamoDB\n    dynamodb = boto3.resource(\"dynamodb\", region_name=\"eu-central-1\")  # connect to DynamoDB\n    db_table = dynamodb.Table(table_name)\n\n    # Get key names\n    key_schema = db_table.key_schema\n    key_names = [k[\"AttributeName\"] for k in key_schema]\n\n    # Load current state of triplet table\n    cprint(string=f\"Loading current state of {session} triplet table ...\", col=\"b\")\n    response = db_table.scan()\n    data = response[\"Items\"]\n    while \"LastEvaluatedKey\" in response:\n        response = db_table.scan(ExclusiveStartKey=response[\"LastEvaluatedKey\"])\n        data.extend(response[\"Items\"])\n\n    df_current_state = pd.DataFrame(data)  # transform to pd.DataFrame\n\n    # Unlock triplets\n    locked: str = \"L\"  # locked symbol\n    if (df_current_state.status == locked).any():\n        cprint(\n            string=f\"\\nUnlocking {(df_current_state.status == locked).sum()} previously locked triplets ...\", col=\"b\"\n        )\n        for row in tqdm(data, desc=f\"Unlocking items in {table_name}\"):\n            if row[\"status\"] != locked:\n                continue\n            _update_status_item(\n                dynamodb_table=db_table, table_name=table_name, data_row=row, key_names=key_names, new_status=\"U\"\n            )  # set \"L\" to \"U\"\n        cprint(string=f\"All previously locked triplets are now unlocked in {table_name}.\", col=\"g\")\n\n    # Set open triplets to \"U\"\n    cprint(string=f\"\\nResetting {len(open_triplets)} open triplets to 'U' ...\", col=\"b\")\n    for row in tqdm(data, desc=f\"Reset open triplets items in {table_name}\"):\n        if row[\"triplet_id\"] not in open_triplets or row[\"status\"] == \"U\":\n            continue\n        _update_status_item(\n            dynamodb_table=db_table, table_name=table_name, data_row=row, key_names=key_names, new_status=\"U\"\n        )  # reset open triplets\n    cprint(string=f\"All open triplets are now ready to be sampled in {table_name}.\", col=\"g\")\n\n    # Set finalized triplets to \"G\" (if not already done)\n    found_finalised_triplets = False\n    if set_finalised_triplets_to_g:\n        cprint(string=\"\\nSetting finalised triplets to 'G' ...\", col=\"b\")\n        for row in tqdm(data, desc=f\"Reset open triplets items in {table_name}\"):\n            if row[\"triplet_id\"] not in open_triplets and row[\"status\"] != \"G\":\n                _update_status_item(\n                    dynamodb_table=db_table, table_name=table_name, data_row=row, key_names=key_names, new_status=\"G\"\n                )\n                found_finalised_triplets = True\n        if found_finalised_triplets:\n            cprint(string=f\"All finalised triplets are now set to 'G' in {table_name}.\", col=\"g\")\n\n    if len(open_triplets) == 0:\n        cprint(string=f\"\\nAll triplets are finalised in {table_name}.\", col=\"g\")\n    else:\n        cprint(\n            string=f\"\\nInvite max {np.maximum(np.floor(len(open_triplets) / 171).astype(int), 1)} \"\n            f\"participants at once!\\n\",\n            col=\"y\",\n            fm=\"bo\",\n        )\n</code></pre>"},{"location":"reference/read_data/#facesim3d.read_data.where_to_find_trial_and_log_data","title":"where_to_find_trial_and_log_data","text":"<pre><code>where_to_find_trial_and_log_data(\n    set_nr: str, update_table: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Get information about in which files trial results and log data can be found for a given Set number.</p> Source code in <code>code/facesim3d/read_data.py</code> <pre><code>def where_to_find_trial_and_log_data(set_nr: str, update_table: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Get information about in which files trial results and log data can be found for a given Set number.\"\"\"\n    # Path to look-up table\n    table_name: str = \"Where_are_TrialResults_and_Logs.csv\"\n    p2_where_table = list(Path(paths.data.MAIN).glob(f\"*{table_name}\"))\n    if len(p2_where_table) &gt; 1:\n        msg = f\"More than one table found:\\n{p2_where_table}\"\n        raise AssertionError(msg)\n\n    if (len(p2_where_table) == 0) or update_table:\n        if len(p2_where_table) == 0:\n            cprint(string=\"Generating where-to-find table ...\", col=\"b\")\n\n        # Init table\n        where_table = pd.DataFrame(columns=[\"set_nr\", \"table_name\", \"type\"])\n\n        # Find all trial results and log files\n        list_of_trial_result_tables = list(Path(paths.data.MAIN).glob(\"*UXFData.FaceSim.TrialResults.csv\"))\n        list_of_log_tables = list(Path(paths.data.MAIN).glob(\"*UXFData.FaceSim.SessionLog.csv\"))\n        list_of_tables = list_of_trial_result_tables + list_of_log_tables\n\n        # Iterate through different sets\n        set_files = sorted(Path(paths.data.main.prolific).glob(\"*Participants-Set*\"))\n        for p2_ppid_set in tqdm(\n            set_files, desc=\"Find tables for each Set\", total=len(set_files), position=0, colour=\"#51F1EE\"\n        ):\n            # Get set number\n            current_set_nr = p2_ppid_set.name.split(\"-Set\")[-1].split(\"_\")[0]\n\n            # Get participants of current Set\n            ppid_set = read_prolific_participant_data(set_nr=current_set_nr)[\"Participant id\"].to_list()\n\n            # Populate table\n            for p2_table in tqdm(\n                list_of_tables,\n                desc=f\"Iterate through all tables for Set{current_set_nr}\",\n                total=len(list_of_tables),\n                position=1,\n                leave=False,\n                colour=\"#51A4F1\",\n            ):\n                if \"TrialResults\" in p2_table.name:\n                    tr_table = _read_trial_results(process=False, date=p2_table.name.split(\"_\")[0])\n\n                    # Extract those with matching set_nr\n                    ppid_set_tr = [f\"{p}_{set_infix(current_set_nr)}_trial_results\" for p in ppid_set]\n\n                    if tr_table.ppid_session_dataname.isin(ppid_set_tr).any():\n                        where_table = where_table.append(\n                            {\"set_nr\": current_set_nr, \"table_name\": p2_table.name, \"type\": \"TrialResults\"},\n                            ignore_index=True,\n                        )\n                else:  # \"SessionLog\" in p2_table.name\n                    log_table = load_local_table(table_name=p2_table.name)\n                    ppid_set_log = [f\"{p}_{set_infix(current_set_nr)}_log\" for p in ppid_set]\n                    if log_table.ppid_session_dataname.isin(ppid_set_log).any():\n                        where_table = where_table.append(\n                            {\"set_nr\": current_set_nr, \"table_name\": p2_table.name, \"type\": \"SessionLog\"},\n                            ignore_index=True,\n                        )\n\n        # Sort table by set number\n        where_table = where_table.sort_values(by=[\"set_nr\", \"type\", \"table_name\"], axis=0, ascending=True).reset_index(\n            drop=True\n        )\n\n        # Save (or overwrite) table\n        where_table.to_csv(Path(paths.data.MAIN) / table_name, index=False)\n\n        # Return table\n        return where_table[where_table.set_nr == set_nr]\n\n    # else use:  # len(p2_where_table) == 1:\n    p2_where_table = p2_where_table.pop()\n    where_table = pd.read_csv(p2_where_table, dtype=object)\n\n    # Check if set_nr is in table (if not update table)\n    if len(where_table[where_table.set_nr == set_nr]) == 0:\n        cprint(string=f\"Set{set_nr} is not in table {p2_where_table.name}. Updating table ...\", col=\"y\")\n        where_table = where_to_find_trial_and_log_data(set_nr=set_nr, update_table=True)\n\n    # Return table\n    return where_table[where_table.set_nr == set_nr]\n</code></pre>"},{"location":"reference/modeling/","title":"Index","text":""},{"location":"reference/modeling/#facesim3d.modeling","title":"modeling","text":"<p>Init facesim3d.modeling submodule.</p> <p>Modules:</p> Name Description <code>FLAME</code> <p>Init for FLAME model params.</p> <code>VGG</code> <p>Init of the <code>VGG-Face</code>-based modeling pipeline.</p> <code>computational_choice_model</code> <p>Computational models of human choice behavior.</p> <code>compute_noise_ceiling</code> <p>Compute the noise ceiling for human similarity judgments.</p> <code>compute_similarity</code> <p>Compute similarity matrices from feature tables.</p> <code>face_attribute_processing</code> <p>Extract attributes from face images.</p> <code>prep_computational_choice_model</code> <p>Prepare the computational choice model by loading the best weights and computing the similarity matrix.</p> <code>rsa</code> <p>Compute RDMs of face similarity judgments and run RSA for the pilot study (version 2) &amp; the main experiment.</p>"},{"location":"reference/modeling/computational_choice_model/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> computational_choice_model","text":""},{"location":"reference/modeling/computational_choice_model/#facesim3d.modeling.computational_choice_model","title":"computational_choice_model","text":"<p>Computational models of human choice behavior.</p> <p>The <code>SPoSE</code> model is inspired by Hebart et al. (Nat. Hum Beh., 2020), see Fig.1c. The <code>VICE</code> model is taken from Muttenthaler et al. (arXiv, 2022)</p> VICE or SPoSE <p>'VICE rivals or outperforms its predecessor, SPoSE, at predicting human behavior in the odd-one-out triplet task. Furthermore, VICE's object representations are more reproducible and consistent across random initializations.' - Muttenthaler et al. (arXiv, 2022)</p> <p>Other computational embedding models are in <code>facesim3d.modeling.VGG</code>.</p> <p>Functions:</p> Name Description <code>display_representative_faces</code> <p>Display representative faces for the first <code>m</code> dimensions of the trained sparse model.</p> <code>extract_faces_for_spose_dimensions</code> <p>Extract the first <code>n</code> most representative faces for the first <code>m</code> dimensions of the trained SPoSE model.</p> <code>extract_faces_for_vice_dimensions</code> <p>Extract the first <code>n</code> most representative faces for the first <code>m</code> dimensions of the trained VICE model.</p> <code>plot_weight_matrix</code> <p>Plot the weight (i.e., <code>m</code>-dimensional embedding) matrix of <code>VICE</code> | <code>SPoSE</code>.</p> <code>prepare_data_for_spose_and_vice</code> <p>Prepare data for <code>SPoSE</code> &amp; <code>VICE</code> models.</p>"},{"location":"reference/modeling/computational_choice_model/#facesim3d.modeling.computational_choice_model.display_representative_faces","title":"display_representative_faces","text":"<pre><code>display_representative_faces(\n    face_dim_idx_mat: ndarray,\n    pilot: bool = PILOT,\n    as_grid: bool = True,\n    title: str | None = None,\n    dim_indices: list | None = None,\n    save_path: str | Path | None = None,\n) -&gt; None\n</code></pre> <p>Display representative faces for the first <code>m</code> dimensions of the trained sparse model.</p> <p>Parameters:</p> Name Type Description Default <code>face_dim_idx_mat</code> <code>ndarray</code> <p>nxm matrix of indices in representative faces per dimension</p> required <code>pilot</code> <code>bool</code> <p>True: use pilot data</p> <code>PILOT</code> <code>as_grid</code> <code>bool</code> <p>plot as image grid</p> <code>True</code> <code>title</code> <code>str | None</code> <p>Image or figure title</p> <code>None</code> <code>dim_indices</code> <code>list | None</code> <p>list of dimension indices to display</p> <code>None</code> <code>save_path</code> <code>str | Path | None</code> <p>if the path is given, save figure</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>code/facesim3d/modeling/computational_choice_model.py</code> <pre><code>def display_representative_faces(\n    face_dim_idx_mat: np.ndarray,\n    pilot: bool = params.PILOT,\n    as_grid: bool = True,\n    title: str | None = None,\n    dim_indices: list | None = None,\n    save_path: str | Path | None = None,\n) -&gt; None:\n    \"\"\"\n    Display representative faces for the first `m` dimensions of the trained sparse model.\n\n    :param face_dim_idx_mat: nxm matrix of indices in representative faces per dimension\n    :param pilot: True: use pilot data\n    :param as_grid: plot as image grid\n    :param title: Image or figure title\n    :param dim_indices: list of dimension indices to display\n    :param save_path: if the path is given, save figure\n    :return: None\n    \"\"\"\n    # Choose the map for CFD indices\n    idx_mapper = (\n        partial(pilot_matrix_index_to_head_nr, pilot_version=params.PILOT_VERSION)\n        if pilot\n        else main_matrix_index_to_head_nr\n    )\n\n    fig, axes = None, None  # init\n    if as_grid:\n        dim_indices = range(face_dim_idx_mat.shape[1]) if dim_indices is None else dim_indices\n        fig, axes = plt.subplots(\n            *face_dim_idx_mat.shape,\n            figsize=(face_dim_idx_mat.shape[1] * 2, face_dim_idx_mat.shape[0] * 2),\n            sharex=True,\n            sharey=True,\n            num=title,\n        )\n\n    for dim_i, face_indices in enumerate(face_dim_idx_mat.T):\n        print(dim_i, face_indices)\n\n        for i, face_idx in enumerate(face_indices):\n            # Map indices back to CFD index\n            face_id = idx_mapper(face_idx)\n\n            # Display faces with the strongest weight(s) for each dimension\n            if as_grid:\n                try:\n                    face_img = plt.imread(fname=face_image_path(head_id=face_id))  # load as np.array\n                except FileNotFoundError:\n                    # This computes the screenshot of the 3D reconstructed face\n                    display_face(head_id=face_id, data_mode=\"3d-reconstructions\", interactive=False, verbose=False)\n                    face_img = plt.imread(fname=face_image_path(head_id=face_id))\n\n                # Zooming takes a while; now it is done already at earlier stage in display_face()\n                # face_img = clipped_zoom(img=face_img, zoom_factor=1.8)  # zoom into image  # noqa: ERA001\n                axes[i, dim_i].imshow(face_img)\n                if i == 0:\n                    axes[i, dim_i].set_title(f\"Dimension {dim_indices[dim_i] + 1}\")\n                axes[i, dim_i].set_xticks([])\n                axes[i, dim_i].set_xlabel(face_id)\n                axes[i, dim_i].yaxis.set_visible(False)\n                for spine in axes[i, dim_i].spines.values():  # remove axes-box around image\n                    spine.set_visible(False)\n            else:\n                # Display face images externally\n                # TODO: add somewhere a caption  # noqa: FIX002\n                display_face(head_id=face_id)\n    if as_grid:\n        fig.tight_layout()\n        if save_path:\n            fn = (\n                f\"{title}_representative_faces-{face_dim_idx_mat.shape[0]}_\"\n                f\"dims-{str(tuple(np.array(dim_indices) + 1)).replace(' ', '')}\"\n            )\n            cprint(string=f\"Saving figure in '{Path(str(save_path), fn)}' ... \", col=\"b\")\n            for ext in [\"png\", \"svg\"]:\n                plt.savefig(fname=Path(save_path, fn).with_suffix(\".\" + ext), dpi=300, format=ext)\n            plt.close()\n</code></pre>"},{"location":"reference/modeling/computational_choice_model/#facesim3d.modeling.computational_choice_model.extract_faces_for_spose_dimensions","title":"extract_faces_for_spose_dimensions","text":"<pre><code>extract_faces_for_spose_dimensions(\n    session: str,\n    n_face: int | None = None,\n    m_dims: int | None = None,\n    pilot: bool = PILOT,\n    return_path: bool = False,\n    **kwargs\n) -&gt; ndarray | tuple[ndarray, str]\n</code></pre> <p>Extract the first <code>n</code> most representative faces for the first <code>m</code> dimensions of the trained SPoSE model.</p> <p>See Hebart et al. (2020), caption of Fig. 2:</p> <p>Quote</p> <p>'The images reflect the objects with the highest weights along those dimensions.'</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>\"2D\", OR \"3D\"</p> required <code>n_face</code> <code>int | None</code> <p>[int] restrict the number of faces OR [None] all faces are returned</p> <code>None</code> <code>m_dims</code> <code>int | None</code> <p>[int] restrict the number of dimensions (or weights) in the SPoSE model OR [None] all dimensions are returned</p> <code>None</code> <code>pilot</code> <code>bool</code> <p>True: use pilot data</p> <code>PILOT</code> <code>return_path</code> <code>bool</code> <p>True: return the path to the weights-file</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray | tuple[ndarray, str]</code> <p>indices of representative faces</p> Source code in <code>code/facesim3d/modeling/computational_choice_model.py</code> <pre><code>def extract_faces_for_spose_dimensions(\n    session: str,\n    n_face: int | None = None,\n    m_dims: int | None = None,\n    pilot: bool = params.PILOT,\n    return_path: bool = False,\n    **kwargs,\n) -&gt; np.ndarray | tuple[np.ndarray, str]:\n    \"\"\"\n    Extract the first `n` most representative faces for the first `m` dimensions of the trained SPoSE model.\n\n    See Hebart et al. (2020), caption of Fig. 2:\n\n    !!! quote\n        'The images reflect the objects with the highest weights along those dimensions.'\n\n    :param session: \"2D\", OR \"3D\"\n    :param n_face: [int] restrict the number of faces OR [None] all faces are returned\n    :param m_dims:  [int] restrict the number of dimensions (or weights) in the SPoSE model OR\n                    [None] all dimensions are returned\n    :param pilot: True: use pilot data\n    :param return_path: True: return the path to the weights-file\n    :return: indices of representative faces\n    \"\"\"\n    # Load weights\n    p2_weights = \"\"  # init\n    weights = load_spose_weights(session=session, pilot=pilot, return_path=return_path, **kwargs)\n    if return_path:\n        weights, p2_weights = weights\n\n    # Extract representative faces\n    face_dim_idx_mat = np.argsort(weights, axis=0)[::-1][:n_face, :m_dims]\n    # rows: index of most representative faces (descending) | cols: most relevant dimensions (descending)\n    # E.g., face_img_idx[1, 0] = index of second most representative face for first dimension\n\n    if return_path:\n        return face_dim_idx_mat, p2_weights\n    return face_dim_idx_mat\n</code></pre>"},{"location":"reference/modeling/computational_choice_model/#facesim3d.modeling.computational_choice_model.extract_faces_for_vice_dimensions","title":"extract_faces_for_vice_dimensions","text":"<pre><code>extract_faces_for_vice_dimensions(\n    session: str,\n    n_face: int | None = None,\n    m_dims: int | None = None,\n    pilot: bool = PILOT,\n    pruned: bool = True,\n    return_path: bool = False,\n    param_path: str | None = \"\",\n) -&gt; ndarray | tuple[ndarray, str]\n</code></pre> <p>Extract the first <code>n</code> most representative faces for the first <code>m</code> dimensions of the trained VICE model.</p> <p>See Muttenthaler et al. (arXiv, 2022), p.19, Section F \"Interpretability\":</p> <p>Quote</p> <p>'Objects were sorted in descending order according to their absolute embedding value.'</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>\"2D\", OR \"3D\"</p> required <code>n_face</code> <code>int | None</code> <p>[int] restrict the number of faces OR [None] all faces are returned</p> <code>None</code> <code>m_dims</code> <code>int | None</code> <p>[int] restrict the number of dimensions (or weights) of the VICE model OR [None] all dimensions are returned</p> <code>None</code> <code>pilot</code> <code>bool</code> <p>True: use pilot data</p> <code>PILOT</code> <code>pruned</code> <code>bool</code> <p>True: return the pruned parameters</p> <code>True</code> <code>return_path</code> <code>bool</code> <p>True: return path to the parameter file</p> <code>False</code> <code>param_path</code> <code>str | None</code> <p>path to weight file, defined by the corresponding VICE params (after /[session]/..)</p> <code>''</code> <p>Returns:</p> Type Description <code>ndarray | tuple[ndarray, str]</code> <p>indices of representative faces</p> Source code in <code>code/facesim3d/modeling/computational_choice_model.py</code> <pre><code>def extract_faces_for_vice_dimensions(\n    session: str,\n    n_face: int | None = None,\n    m_dims: int | None = None,\n    pilot: bool = params.PILOT,\n    pruned: bool = True,\n    return_path: bool = False,\n    param_path: str | None = \"\",\n) -&gt; np.ndarray | tuple[np.ndarray, str]:\n    \"\"\"\n    Extract the first `n` most representative faces for the first `m` dimensions of the trained VICE model.\n\n    See Muttenthaler et al. (arXiv, 2022), p.19, Section F \"Interpretability\":\n\n    !!! quote\n        'Objects were sorted in descending order according to their absolute embedding value.'\n\n    :param session: \"2D\", OR \"3D\"\n    :param n_face: [int] restrict the number of faces OR [None] all faces are returned\n    :param m_dims:  [int] restrict the number of dimensions (or weights) of the VICE model OR\n                    [None] all dimensions are returned\n    :param pilot: True: use pilot data\n    :param pruned: True: return the pruned parameters\n    :param return_path: True: return path to the parameter file\n    :param param_path: path to weight file, defined by the corresponding VICE params (after /[session]/..)\n    :return: indices of representative faces\n    \"\"\"\n    # Load weights\n    p2_weights = \"\"  # init\n    weights = load_vice_weights(\n        session=session, pilot=pilot, pruned=pruned, return_path=return_path, param_path=param_path\n    )\n    if return_path:\n        loc_param, _, p2_weights = weights  # scale_param = _\n    else:\n        loc_param, _ = weights  # _ = scale_param\n\n    # # Extract representative faces\n    # In the paper they report taking the 'absolute embedding value' to sort objects.\n    # However, in facesim3d.modeling.VICE.visualization.plot_topk_objects_per_dimension(), objects are\n    # just sorted based on the corresponding weight-value (mu, not sigma), so we do this here, too.\n    # Bt also check out: np.linalg.norm(loc_param, axis=0), dimensions are here semi-sorted.\n    n_face = loc_param.shape[0] if n_face is None else n_face\n    m_dims = loc_param.shape[1] if m_dims is None else m_dims\n    face_dim_idx_mat = np.argsort(loc_param, axis=0)[::-1][:n_face, :m_dims]\n    # face_dim_idx_mat = np.argsort(np.abs(loc_param), axis=0)[::-1][:n_face, :m_dims]  # noqa: ERA001\n    # Note, that taking the absolute didn't change the results anyway (tested for '2D')\n    # rows: index of most representative faces (descending) | cols: most relevant dimensions (descending)\n    # E.g., face_img_idx[1, 0] = index of second most representative face for first dimension\n\n    if return_path:\n        return face_dim_idx_mat, p2_weights\n    return face_dim_idx_mat\n</code></pre>"},{"location":"reference/modeling/computational_choice_model/#facesim3d.modeling.computational_choice_model.plot_weight_matrix","title":"plot_weight_matrix","text":"<pre><code>plot_weight_matrix(\n    weights: ndarray,\n    norm: bool,\n    fig_name: str,\n    save: bool = False,\n    save_path: str | Path | None = \"\",\n)\n</code></pre> <p>Plot the weight (i.e., <code>m</code>-dimensional embedding) matrix of <code>VICE</code> | <code>SPoSE</code>.</p> Source code in <code>code/facesim3d/modeling/computational_choice_model.py</code> <pre><code>def plot_weight_matrix(\n    weights: np.ndarray, norm: bool, fig_name: str, save: bool = False, save_path: str | Path | None = \"\"\n):\n    \"\"\"Plot the weight (i.e., `m`-dimensional embedding) matrix of `VICE` | `SPoSE`.\"\"\"\n    if norm:\n        weights /= np.abs(weights).max()\n\n    plt.matshow(weights, cmap=\"seismic\", fignum=fig_name)  # could reduce to M_DIMENSIONS\n    plt.colorbar()\n    plt.tight_layout()\n\n    if save:\n        for ext in [\"png\", \"svg\"]:\n            plt.savefig(Path(save_path, f\"{fig_name}.{ext}\"), dpi=300, format=ext)\n        plt.close()\n    else:\n        plt.show()\n</code></pre>"},{"location":"reference/modeling/computational_choice_model/#facesim3d.modeling.computational_choice_model.prepare_data_for_spose_and_vice","title":"prepare_data_for_spose_and_vice","text":"<pre><code>prepare_data_for_spose_and_vice(\n    session: str,\n    percentage: int | None = None,\n    gender: bool | str = False,\n    pilot: bool = PILOT,\n) -&gt; None\n</code></pre> <p>Prepare data for <code>SPoSE</code> &amp; <code>VICE</code> models.</p> <p>Quote</p> <p>(...) triplets are expected to be in the format N x 3, where N = number of trials (e.g., 100k) and 3 refers to the triplets, where col_0 = anchor_1, col_1 = anchor_2, col_2 = odd one out. Triplet data must be split into train and test splits, and named <code>train_90.txt</code> and <code>test_10.txt</code>, respectively.</p> <p>For hyperparameter tuning, prepare only a percentage of the data.</p> <p>For more information, see the repos of:     SPoSE &amp; VICE.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>percentage</code> <code>int | None</code> <p>percentage of data to use (e.g., 10, 20, ...)</p> <code>None</code> <code>gender</code> <code>bool | str</code> <p>True: use only triplets of the same gender, respectively. Compute recursively for both genders. OR str: specify the gender 'female' or 'male'.</p> <code>False</code> <code>pilot</code> <code>bool</code> <p>True: use pilot data</p> <code>PILOT</code> Source code in <code>code/facesim3d/modeling/computational_choice_model.py</code> <pre><code>def prepare_data_for_spose_and_vice(\n    session: str, percentage: int | None = None, gender: bool | str = False, pilot: bool = params.PILOT\n) -&gt; None:\n    \"\"\"\n    Prepare data for `SPoSE` &amp; `VICE` models.\n\n    !!! quote\n        (...) triplets are expected to be in the format N x 3, where N = number of trials (e.g., 100k) and\n        3 refers to the triplets, where col_0 = anchor_1, col_1 = anchor_2, col_2 = odd one out.\n        Triplet data must be split into train and test splits, and named `train_90.txt` and `test_10.txt`,\n        respectively.\n\n    For hyperparameter tuning, prepare only a percentage of the data.\n\n    For more information, see the repos of:\n        [SPoSE](https://github.com/ViCCo-Group/SPoSE) &amp; [VICE](https://github.com/LukasMut/VICE).\n\n    :param session: '2D', OR '3D'\n    :param percentage: percentage of data to use (e.g., 10, 20, ...)\n    :param gender: True: use only triplets of the same gender, respectively. Compute recursively for both genders.\n                   OR str: specify the gender 'female' or 'male'.\n    :param pilot: True: use pilot data\n    \"\"\"\n    # Check for gender specification\n    if gender is True:\n        prepare_data_for_spose_and_vice(session=session, percentage=percentage, gender=\"female\", pilot=pilot)\n        prepare_data_for_spose_and_vice(session=session, percentage=percentage, gender=\"male\", pilot=pilot)\n        return  # stop here\n\n    if isinstance(gender, str):\n        gender = gender.lower()\n        if gender not in {\"female\", \"male\"}:\n            msg = \"Gender must be 'female' OR 'male'!\"\n            raise ValueError(msg)\n\n    # Set paths\n    spose_data_dir = Path(paths.data.pilot.v2, \"for_SPoSE\", session) if pilot else Path(paths.data.main.spose, session)\n    if gender:\n        spose_data_dir /= gender\n    if percentage is not None:\n        if percentage not in {10, 20, 30, 40, 50}:\n            msg = \"'percentage' must be in [10, 20, 30, 40, 50]!\"\n            raise ValueError(msg)\n        spose_data_dir /= f\"{percentage}perc\"\n    spose_data_dir.mkdir(parents=True, exist_ok=True)\n\n    p2_training_set = spose_data_dir / \"train_90.txt\"\n    p2_test_set = spose_data_dir / \"test_10.txt\"\n\n    # Check if data already exists\n    if p2_training_set.is_file() and p2_test_set.is_file():\n        cprint(string=f\"SPoSE &amp; VICE data for {session} already prepared.\", col=\"g\")\n        return\n\n    # Load data tables\n    if pilot:\n        data_table = read_pilot_data(clean_trials=True, verbose=False)\n        participant_table = read_pilot_participant_data()[[\"ppid\", \"group_exp\"]]\n        # Use data of one session (2D, 3D) only\n        participant_table = participant_table.loc[participant_table.group_exp == session]\n        data_table = data_table.loc[data_table.ppid.isin(participant_table.ppid)].reset_index(drop=True)\n    else:\n        data_table = read_trial_results_of_session(session=session, clean_trials=True, verbose=False)\n\n    # Prepare training tables\n    data_table = data_table[[\"head1\", \"head2\", \"head3\", \"head_odd\"]]\n    data_table = data_table.drop(\n        index=data_table.loc[data_table.head_odd == 0].index, axis=1\n    )  # remove trials w/o judgment\n    data_table = data_table.dropna()\n\n    # In the case of gender specification, filter data for gender-specific triplets\n    gender_cut = params.main.n_faces // 2  # == 50\n    if gender:\n        if gender == \"female\":\n            data_table = data_table[(data_table &lt;= gender_cut).all(axis=1)]\n        else:\n            data_table = data_table[(data_table &gt; gender_cut).all(axis=1)]\n\n    data_table = data_table.astype(int).reset_index(drop=True)\n\n    # Bring the table in the following format: col_0: anchor_1, col_1: anchor_2, col_2: odd-one-out\n    for i, row in tqdm(\n        iterable=data_table.iterrows(), desc=f\"Prepare data for SPoSE &amp; VICE in {session}\", total=len(data_table)\n    ):\n        data_table.iloc[i, 0:3] = pd.value_counts(row, sort=True, ascending=True).index\n    data_table = data_table.drop(columns=[\"head_odd\"])\n    data_table.columns = [\"col_0\", \"col_1\", \"col_2\"]\n\n    # Replace head number with index\n    index_mapper = (\n        partial(head_nr_to_pilot_matrix_index, pilot_version=params.PILOT_VERSION)\n        if pilot\n        else head_nr_to_main_matrix_index\n    )\n    # pilot v2: female: 0-12, male: 13-25\n    data_table = data_table.applymap(index_mapper)\n    # for main: == data_table = data_table - 1\n\n    if gender == \"male\":\n        # When we have male-only triplets, we need to re-index the heads starting from 0 (instead of 50)\n        data_table -= gender_cut\n\n    sampled_index = None  # init\n    if percentage is not None:\n        data_table = data_table.sample(frac=percentage / 100)\n        sampled_index = data_table.index\n        data_table = data_table.reset_index(drop=True)\n\n    # Extract training and test set (9-1-Ratio)\n    training_set = data_table.sample(frac=0.9)\n    test_set = data_table.drop(index=training_set.index)\n\n    # Save training and test\n    training_set.to_csv(p2_training_set, index=False, header=False, sep=\" \")\n    test_set.to_csv(p2_test_set, index=False, header=False, sep=\" \")\n    # Note: SPoSe takes .npy files as input, too\n    np.save(file=p2_training_set.with_suffix(\".npy\"), arr=training_set.to_numpy())\n    np.save(file=p2_test_set.with_suffix(\".npy\"), arr=test_set.to_numpy())\n\n    if percentage is not None:\n        # Save sampled index for a fraction of data\n        np.save(file=spose_data_dir / \"sampled_index.npy\", arr=sampled_index)\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> compute_noise_ceiling","text":""},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling","title":"compute_noise_ceiling","text":"<p>Compute the noise ceiling for human similarity judgments.</p> <p>In terms of:</p> <pre><code>* regression coefficients\n* prediction accuracy\n</code></pre> <p>Functions:</p> Name Description <code>check_gender_suffix</code> <p>Check the gender suffix.</p> <code>check_sampling_mode</code> <p>Check the sampling mode.</p> <code>compute_cross_session_accuracy</code> <p>Compute the cross-session accuracy.</p> <code>compute_maximal_empirical_accuracy_in_session</code> <p>Compute the maximal empirical accuracy (<code>MEA</code>) for a given session.</p> <code>compute_maximal_empirical_accuracy_in_triplet</code> <p>Compute the maximal empirical accuracy (<code>MEA</code>) within a resampled triplet.</p> <code>get_mea_table</code> <p>Get the maximal empirical accuracy (<code>MEA</code>) table.</p> <code>get_mer_table</code> <p>Get the maximal empirical R (<code>MER</code>) table.</p> <code>get_r_stats</code> <p>Get min, mean, max, and variance from a list of correlation coefficients.</p> <code>get_trial_tables</code> <p>Get the trial results table for each session.</p> <code>save_mea_table</code> <p>Save the maximal empirical accuracy (<code>MEA</code>) table.</p> <code>save_mer_table</code> <p>Save the maximal empirical R (<code>MER</code>) table.</p> <code>statistical_difference_maximal_empirical_accuracy_between_sessions</code> <p>Run a significance test of differences between maximal empirical accuracies (MEA) for the two sessions (2D, 3D).</p>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.check_gender_suffix","title":"check_gender_suffix","text":"<pre><code>check_gender_suffix(suffix: str | None) -&gt; str\n</code></pre> <p>Check the gender suffix.</p> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def check_gender_suffix(suffix: str | None) -&gt; str:\n    \"\"\"Check the gender suffix.\"\"\"\n    if suffix:\n        suffix = suffix.lower()\n        if suffix not in params.GENDERS:\n            msg = \"suffix must be 'female' OR 'male'!\"\n            raise ValueError(msg)\n        return f\"_{suffix}\"\n\n    return \"\"\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.check_sampling_mode","title":"check_sampling_mode","text":"<pre><code>check_sampling_mode(sampling_mode: str) -&gt; str\n</code></pre> <p>Check the sampling mode.</p> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def check_sampling_mode(sampling_mode: str) -&gt; str:\n    \"\"\"Check the sampling mode.\"\"\"\n    sampling_mode = sampling_mode.lower()\n    if sampling_mode not in {\"full-sample\", \"multi-sub-sample\"}:\n        msg = \"Mode must be either 'full-sample' or 'multi-sub-sample'!\"\n        raise ValueError(msg)\n    return sampling_mode\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.compute_cross_session_accuracy","title":"compute_cross_session_accuracy","text":"<pre><code>compute_cross_session_accuracy(\n    trial_results_table_2d: DataFrame,\n    trial_results_table_3d: DataFrame,\n    sampling_mode: str,\n    gender_suffix: str = \"\",\n) -&gt; None\n</code></pre> <p>Compute the cross-session accuracy.</p> <p>Question</p> <p>\"Can one predict from trials in the 2D-condition trials in the 3D-condition?\"</p> <p>In case there are multiple samples of the same tripled ID, the most frequent choice is taken.</p> Minimal impact of the random choices <p>The partially random choices during the comparison between the two viewing conditions (below) can lead to slightly different results in the <code>'match'</code> column, when this would be run again. Nonetheless, the contribution of these random choices should be small, since they should cancel each other out (no-match vs. match) in terms of their impact on the overall accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>trial_results_table_2d</code> <code>DataFrame</code> <p>trial results table of the 2D-session</p> required <code>trial_results_table_3d</code> <code>DataFrame</code> <p>trial results table of the 3D-session</p> required <code>sampling_mode</code> <code>str</code> <p>\"full-sample\" or \"multi-sub-sample\"</p> required <code>gender_suffix</code> <code>str</code> <p>gender suffix \"female\" or \"male\" (if applicable) else empty string \"\"</p> <code>''</code> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def compute_cross_session_accuracy(\n    trial_results_table_2d: pd.DataFrame,\n    trial_results_table_3d: pd.DataFrame,\n    sampling_mode: str,\n    gender_suffix: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Compute the cross-session accuracy.\n\n    !!! question\n        \"Can one predict from trials in the 2D-condition trials in the 3D-condition?\"\n    In case there are multiple samples of the same tripled ID, the most frequent choice is taken.\n\n    ??? note \"Minimal impact of the random choices\"\n        The partially random choices during the comparison between the two viewing conditions (below) can lead to\n        slightly different results in the `'match'` column, when this would be run again.\n        Nonetheless, the contribution of these random choices should be small,\n        since they should cancel each other out (no-match vs. match) in terms of their impact on the overall accuracy.\n\n    :param trial_results_table_2d: trial results table of the 2D-session\n    :param trial_results_table_3d: trial results table of the 3D-session\n    :param sampling_mode: \"full-sample\" or \"multi-sub-sample\"\n    :param gender_suffix: gender suffix \"female\" or \"male\" (if applicable) else empty string \"\"\n    \"\"\"\n    # Check triplet IDs\n    if set(trial_results_table_2d.triplet) != set(trial_results_table_3d.triplet):\n        msg = \"Triplets (ID's) must match between tables!\"\n        raise ValueError(msg)\n\n    if not np.all(\n        trial_results_table_2d.sort_values(by=[\"triplet_id\"]).triplet.unique()\n        == trial_results_table_3d.sort_values(by=[\"triplet_id\"]).triplet.unique()\n    ):\n        msg = \"Triplet ID to triplet mapping must match between tables!\"\n        raise ValueError(msg)\n\n    # Check sampling_mode\n    sampling_mode = check_sampling_mode(sampling_mode=sampling_mode)\n    suffix = check_gender_suffix(suffix=gender_suffix)\n\n    # First create a choice table per session (2D, 3D)\n    path_to_choice_table = Path(\n        paths.results.main.behavior, f\"compare_choices_between_sessions_{sampling_mode}{suffix}.csv\"\n    )\n    if path_to_choice_table.exists():\n        choice_table = pd.read_csv(path_to_choice_table, index_col=\"triplet_id\", low_memory=False)\n\n    else:\n        choice_table = pd.DataFrame(\n            columns=[\"head_odd_2D\", \"head_odd_3D\", \"match\"],\n            index=np.sort(trial_results_table_2d.triplet_id.unique()),\n        )\n    choice_table.index.name = \"triplet_id\"\n\n    # Fill choice table via a majority vote per triplet-ID\n    new_entries = False\n\n    for tid in tqdm(\n        choice_table.index,\n        desc=f\"Filling choice table with {sampling_mode}{suffix} data\",\n        total=len(choice_table),\n        colour=\"#63B456\",\n    ):\n        if not pd.isna(choice_table.loc[tid]).all():\n            continue\n        new_entries = True\n\n        # Get triplet table for current triplet ID\n        tid_tab_2d = trial_results_table_2d.loc[trial_results_table_2d.triplet_id == tid, \"head_odd\"]\n        tid_tab_3d = trial_results_table_3d.loc[trial_results_table_3d.triplet_id == tid, \"head_odd\"]\n\n        # Get value counts in both conditions\n        tid_tab_2d_vc = tid_tab_2d.value_counts()\n        tid_tab_3d_vc = tid_tab_3d.value_counts()\n\n        # Keep only max count values\n        tid_tab_2d_vc = tid_tab_2d_vc[tid_tab_2d_vc == tid_tab_2d_vc.max()]\n        tid_tab_3d_vc = tid_tab_3d_vc[tid_tab_3d_vc == tid_tab_3d_vc.max()]\n\n        if len(tid_tab_2d_vc) &gt; 1 or len(tid_tab_3d_vc) &gt; 1:\n            # At least two heads were chosen n times\n\n            # Fill choice table with choices\n            choice_table.loc[tid, \"head_odd_2D\"] = sorted(tid_tab_2d_vc.index.astype(int).tolist())\n            choice_table.loc[tid, \"head_odd_3D\"] = sorted(tid_tab_3d_vc.index.astype(int).tolist())\n\n            if choice_table.loc[tid, \"head_odd_2D\"] == choice_table.loc[tid, \"head_odd_3D\"]:\n                # Case 1: The most chosen heads are equally distributed across viewing conditions\n                choice_table.loc[tid, \"match\"] = True\n                continue\n\n            if len(choice_table.loc[tid, \"head_odd_2D\"]) == len(choice_table.loc[tid, \"head_odd_3D\"]):\n                # Case 2: In both conditions we have the same number of most chosen heads, but the heads are not equal\n                # We randomly draw from the most selected heads &amp; compare them\n                choice_table.loc[tid, \"match\"] = np.random.choice(tid_tab_2d_vc.index) == np.random.choice(\n                    tid_tab_3d_vc.index\n                )\n                continue\n\n            # Case 3: The most chosen heads are not equally distributed across viewing conditions\n            # We randomly draw from the most selected heads &amp; compare them\n            choice_table.loc[tid, \"match\"] = np.random.choice(tid_tab_2d_vc.index) == np.random.choice(\n                tid_tab_3d_vc.index\n            )\n        else:\n            choice_table.loc[tid, \"head_odd_2D\"] = int(tid_tab_2d_vc.index[0])\n            choice_table.loc[tid, \"head_odd_3D\"] = int(tid_tab_3d_vc.index[0])\n            choice_table.loc[tid, \"match\"] = (\n                choice_table.loc[tid, \"head_odd_2D\"] == choice_table.loc[tid, \"head_odd_3D\"]\n            )\n\n    # Save choice table if there are new entries\n    if new_entries:\n        print(\"There are new entries in the choice_table which will be saved\")\n        path_to_choice_table.parent.mkdir(parents=True, exist_ok=True)\n        choice_table.to_csv(path_to_choice_table)\n    else:\n        print(\"There are no new entries in the choice_table.\")\n\n    # Get &amp; fill MEA table\n    mea_df = get_mea_table()\n    mea_df.loc[(\"both\", f\"{sampling_mode}{suffix}\"), :] = (\n        choice_table.match.astype(int).mean(),\n        np.minimum(\n            trial_results_table_2d.triplet_id.value_counts().min(),\n            trial_results_table_3d.triplet_id.value_counts().min(),\n        ),\n    )\n\n    cprint(\n        string=f\"\\n{sampling_mode.title()}{suffix}: Cross-session accuracy (using the majority vote in \"\n        f\"triplets with multiple samples): {choice_table.match.astype(int).mean():.2%}\",\n        col=\"g\",\n    )\n\n    # Save MEA table\n    save_mea_table(mea_df=mea_df)\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.compute_maximal_empirical_accuracy_in_session","title":"compute_maximal_empirical_accuracy_in_session","text":"<pre><code>compute_maximal_empirical_accuracy_in_session(\n    session: str,\n    tr_table_of_session: DataFrame,\n    sampling_mode: str,\n    gender_suffix: str = \"\",\n) -&gt; None\n</code></pre> <p>Compute the maximal empirical accuracy (<code>MEA</code>) for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>\"2D\" or \"3D\"</p> required <code>tr_table_of_session</code> <code>DataFrame</code> <p>trial results table of session</p> required <code>sampling_mode</code> <code>str</code> <p>\"full-sample\" or \"multi-sub-sample\"</p> required <code>gender_suffix</code> <code>str</code> <p>gender suffix \"female\" or \"male\" (if applicable) else empty string \"\"</p> <code>''</code> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def compute_maximal_empirical_accuracy_in_session(\n    session: str, tr_table_of_session: pd.DataFrame, sampling_mode: str, gender_suffix: str = \"\"\n) -&gt; None:\n    \"\"\"\n    Compute the maximal empirical accuracy (`MEA`) for a given session.\n\n    :param session: \"2D\" or \"3D\"\n    :param tr_table_of_session: trial results table of session\n    :param sampling_mode: \"full-sample\" or \"multi-sub-sample\"\n    :param gender_suffix: gender suffix \"female\" or \"male\" (if applicable) else empty string \"\"\n    \"\"\"\n    # Checks\n    sampling_mode = check_sampling_mode(sampling_mode=sampling_mode)\n    suffix = check_gender_suffix(suffix=gender_suffix)\n\n    # Get MEA table\n    mea_df = get_mea_table()\n\n    # Get value counts of triplets in table\n    tr_val_ctn_table = (\n        tr_table_of_session[[\"triplet_id\", \"triplet\"]]\n        .value_counts()\n        .rename_axis([\"triplet_id\", \"triplet\"])\n        .reset_index(name=\"counts\")\n    )\n\n    # Compute max empirical accuracy for each triplet\n    # Note for the multi-subsampling set, the triplet_id is different from previous sets\n    min_n_samples = MIN_N_SAMPLES_PER_TRIPLET\n    while True:\n        poss_accs = []  # init\n        len_trips = []  # init\n        for _triplet, multi_triplet_tr in tr_table_of_session[\n            tr_table_of_session.triplet.isin(\n                tr_val_ctn_table[  # take triplets sampled multiple times\n                    tr_val_ctn_table.counts &gt;= min_n_samples\n                ].triplet\n            )\n        ].groupby(\"triplet\"):\n            poss_accs.append(compute_maximal_empirical_accuracy_in_triplet(choices=multi_triplet_tr.head_odd.values))\n            len_trips.append(len(multi_triplet_tr.head_odd.values))\n\n        if len(poss_accs) == 0:\n            break\n\n        # Weighted aggregation of max empirical accuracy over triplets\n        max_acc = np.sum(np.array(poss_accs) * np.array(len_trips)) / np.sum(len_trips)\n        cprint(\n            string=f\"\\n{sampling_mode.title()} {session}{suffix}: \"\n            f\"Maximal empirical accuracy over {len(len_trips)} \"\n            f\"triplets (with {min_n_samples} or more samples): {max_acc:.2%}\",\n            col=\"g\",\n        )\n        mea_df.loc[(session, f\"{sampling_mode}{suffix}\"), :] = max_acc, min_n_samples\n\n        if not CHECK_MORE_SAMPLES:\n            break\n        min_n_samples += 1\n\n    # Save MEA table\n    save_mea_table(mea_df=mea_df)\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.compute_maximal_empirical_accuracy_in_triplet","title":"compute_maximal_empirical_accuracy_in_triplet","text":"<pre><code>compute_maximal_empirical_accuracy_in_triplet(\n    choices: ndarray,\n) -&gt; float | float64\n</code></pre> <p>Compute the maximal empirical accuracy (<code>MEA</code>) within a resampled triplet.</p> <p>Here, <code>MEA</code> is defined as the accuracy, when the most frequent choice across all samples of the given triplet would always be predicted.</p> <p>Parameters:</p> Name Type Description Default <code>choices</code> <code>ndarray</code> <p>array of choices for the same triplet</p> required <p>Returns:</p> Type Description <code>float | float64</code> <p>maximal empirical accuracy</p> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def compute_maximal_empirical_accuracy_in_triplet(choices: np.ndarray) -&gt; float | np.float64:\n    \"\"\"\n    Compute the maximal empirical accuracy (`MEA`) within a resampled triplet.\n\n    Here, `MEA` is defined as the accuracy, when the most frequent choice across all samples of the given\n    triplet would always be predicted.\n\n    :param choices: array of choices for the same triplet\n    :return: maximal empirical accuracy\n    \"\"\"\n    _, ctns = np.unique(choices, return_counts=True)  # _ = vals\n    return np.max(ctns) / np.sum(ctns)  # max_accuracy\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.get_mea_table","title":"get_mea_table","text":"<pre><code>get_mea_table()\n</code></pre> <p>Get the maximal empirical accuracy (<code>MEA</code>) table.</p> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def get_mea_table():\n    \"\"\"Get the maximal empirical accuracy (`MEA`) table.\"\"\"\n    if PATH_TO_ACC_TABLE.exists():\n        mea_df = pd.read_csv(PATH_TO_ACC_TABLE, index_col=[\"session\", \"sample_type\"])\n    else:\n        mea_df = pd.DataFrame(columns=[\"session\", \"sample_type\", \"max_acc\", \"min_n_samples\"])\n        mea_df = mea_df.set_index([\"session\", \"sample_type\"])\n    return mea_df\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.get_mer_table","title":"get_mer_table","text":"<pre><code>get_mer_table()\n</code></pre> <p>Get the maximal empirical R (<code>MER</code>) table.</p> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def get_mer_table():\n    \"\"\"Get the maximal empirical R (`MER`) table.\"\"\"\n    if PATH_TO_R_TABLE.exists():\n        mer_df = pd.read_csv(PATH_TO_R_TABLE, index_col=[\"session\", \"sample_type\"])\n    else:\n        mer_df = pd.DataFrame(\n            columns=[\"session\", \"sample_type\", \"min_r\", \"mean_r\", \"max_r\", \"var_r\", \"max_p_value\", \"min_n_samples\"]\n        )\n        mer_df = mer_df.set_index([\"session\", \"sample_type\"])\n\n    return mer_df\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.get_r_stats","title":"get_r_stats","text":"<pre><code>get_r_stats(ls_r: list) -&gt; tuple\n</code></pre> <p>Get min, mean, max, and variance from a list of correlation coefficients.</p> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def get_r_stats(ls_r: list) -&gt; tuple:\n    \"\"\"Get min, mean, max, and variance from a list of correlation coefficients.\"\"\"\n    return np.min(ls_r), np.mean(ls_r), np.max(ls_r), np.var(ls_r)\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.get_trial_tables","title":"get_trial_tables","text":"<pre><code>get_trial_tables(\n    multi_sub_sample_only: bool = True,\n) -&gt; dict\n</code></pre> <p>Get the trial results table for each session.</p> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def get_trial_tables(multi_sub_sample_only: bool = True) -&gt; dict:\n    \"\"\"Get the trial results table for each session.\"\"\"\n    trial_table_dict = {}  # init dict to store trial results tables\n    if multi_sub_sample_only:\n        for session in params.SESSIONS:\n            trial_table_dict[session] = read_trial_results_of_set(\n                set_nr=f\"{session[0]}.20\", clean_trials=True, verbose=False\n            )\n            # Note that in 2D-table there are more than 5 samples of the following triplet-IDs:\n            #\n            # &gt; ID:     COUNT\n            #   -------------\n            #   118:    13\n            #   419:    12\n            #   803:    12\n            #   643:    11\n            #   1018:   11\n            #   331:    11\n            #   62:     11\n            #   -------------\n            # &gt; SUM:    46\n            #\n            # This leads to 5,746 trials in total, whereas in the 3D-condition we have the expected 5,700 trials.\n\n    else:\n        for session in params.SESSIONS:\n            trial_table_dict[session] = read_trial_results_of_session(\n                session=session,\n                clean_trials=True,\n                drop_subsamples=True,  # here we remove the additionally acquired sub-sample from the data\n                verbose=False,\n            )\n\n    # Proces data\n    for session in params.SESSIONS:\n        # Select columns\n        trial_table_dict[session] = trial_table_dict[session][\n            [\"triplet_id\", \"triplet\", \"head1\", \"head2\", \"head3\", \"head_odd\"]\n        ]\n        # Convert column types\n        trial_table_dict[session] = trial_table_dict[session].astype(\n            {\n                \"triplet_id\": int,\n                \"head1\": int,\n                \"head2\": int,\n                \"head3\": int,\n                \"head_odd\": int,\n            }\n        )\n\n    return trial_table_dict\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.save_mea_table","title":"save_mea_table","text":"<pre><code>save_mea_table(mea_df: DataFrame)\n</code></pre> <p>Save the maximal empirical accuracy (<code>MEA</code>) table.</p> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def save_mea_table(mea_df: pd.DataFrame):\n    \"\"\"Save the maximal empirical accuracy (`MEA`) table.\"\"\"\n    # Prepare the path\n    PATH_TO_ACC_TABLE.parent.mkdir(parents=True, exist_ok=True)\n\n    # Round values\n    mea_df[\"max_acc\"] = mea_df[\"max_acc\"].round(4)\n\n    # Save\n    mea_df.to_csv(PATH_TO_ACC_TABLE)\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.save_mer_table","title":"save_mer_table","text":"<pre><code>save_mer_table(mer_df: DataFrame)\n</code></pre> <p>Save the maximal empirical R (<code>MER</code>) table.</p> Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def save_mer_table(mer_df: pd.DataFrame):\n    \"\"\"Save the maximal empirical R (`MER`) table.\"\"\"\n    # Prepare the path\n    PATH_TO_R_TABLE.parent.mkdir(parents=True, exist_ok=True)\n\n    # Sort index\n    mer_df = mer_df.sort_index()\n    print(mer_df[[\"mean_r\"]])  # ['min_r', 'mean_r', 'max_r']\n\n    # Round values\n    mer_df[[\"min_r\", \"mean_r\", \"max_r\"]] = mer_df[[\"min_r\", \"mean_r\", \"max_r\"]].round(4)\n    mer_df[[\"var_r\"]] = mer_df[[\"var_r\"]].round(6)\n    mer_df[[\"max_p_value\"]] = mer_df[[\"max_p_value\"]].round(8)\n\n    # Save\n    cprint(string=\"\\nSaving empirical R df ...\", col=\"b\")\n    mer_df.to_csv(PATH_TO_R_TABLE)\n</code></pre>"},{"location":"reference/modeling/compute_noise_ceiling/#facesim3d.modeling.compute_noise_ceiling.statistical_difference_maximal_empirical_accuracy_between_sessions","title":"statistical_difference_maximal_empirical_accuracy_between_sessions","text":"<pre><code>statistical_difference_maximal_empirical_accuracy_between_sessions(\n    tr_table_of_2d: DataFrame, tr_table_of_3d: DataFrame\n) -&gt; None\n</code></pre> <p>Run a significance test of differences between maximal empirical accuracies (MEA) for the two sessions (2D, 3D).</p> <p>Parameters:</p> Name Type Description Default <code>tr_table_of_2d</code> <code>DataFrame</code> <p>trial results table of 2D session</p> required <code>tr_table_of_3d</code> <code>DataFrame</code> <p>trial results table of 2D session</p> required Source code in <code>code/facesim3d/modeling/compute_noise_ceiling.py</code> <pre><code>def statistical_difference_maximal_empirical_accuracy_between_sessions(\n    tr_table_of_2d: pd.DataFrame,\n    tr_table_of_3d: pd.DataFrame,\n) -&gt; None:\n    \"\"\"\n    Run a significance test of differences between maximal empirical accuracies (MEA) for the two sessions (2D, 3D).\n\n    :param tr_table_of_2d: trial results table of 2D session\n    :param tr_table_of_3d: trial results table of 2D session\n    \"\"\"\n    poss_accs_dict = {}  # init\n    len_trips_dict = {}  # init\n    for sess in params.SESSIONS:\n        tr_table_of_session = {\"2D\": tr_table_of_2d, \"3D\": tr_table_of_3d}[sess]\n\n        # Get value counts of triplets in table\n        tr_val_ctn_table = (\n            tr_table_of_session[[\"triplet_id\", \"triplet\"]]\n            .value_counts()\n            .rename_axis([\"triplet_id\", \"triplet\"])\n            .reset_index(name=\"counts\")\n        )\n\n        # Compute max empirical accuracy for each triplet\n        # Note for the multi-subsampling set, the triplet_id is different from previous sets\n        poss_accs = []  # init\n        len_trips = []  # init\n        for _triplet, multi_triplet_tr in tr_table_of_session[\n            tr_table_of_session.triplet.isin(\n                tr_val_ctn_table[  # take triplets sampled multiple times\n                    tr_val_ctn_table.counts &gt;= MIN_N_SAMPLES_PER_TRIPLET\n                ].triplet\n            )\n        ].groupby(\"triplet\"):\n            poss_accs.append(compute_maximal_empirical_accuracy_in_triplet(choices=multi_triplet_tr.head_odd.values))\n            len_trips.append(len(multi_triplet_tr.head_odd.values))\n\n        # Fill dict\n        poss_accs_dict[sess] = poss_accs\n        len_trips_dict[sess] = len_trips\n\n    # Run significance test:\n    is_normal_dist = True  # init\n    for sess in params.SESSIONS:\n        # Test for normal distribution\n        a_d_test = sm.stats.diagnostic.normal_ad(np.array(poss_accs_dict[sess]))\n        if a_d_test[1] &lt; 0.05 / 2:  # Bonferroni-corrected\n            cprint(\n                string=f\"Maximal empirical accuracy (MEA) of {sess} is not normally distributed \"\n                f\"(Anderson-Darling test: {a_d_test[0]:.2f}, p-value={a_d_test[1]:.2g})\",\n                col=\"r\",\n            )\n            is_normal_dist = False\n    if is_normal_dist:  # not the case\n        t_stat, p_val, df = sm.stats.ttest_ind(\n            poss_accs_dict[\"2D\"], poss_accs_dict[\"3D\"], alternative=\"two-sided\", usevar=\"pooled\"\n        )\n        cprint(\n            string=f\"\\nStatistical difference between maximal empirical accuracies (MEA) of 2D &amp; 3D: \"\n            f\"t-statistic(df={df})={t_stat:.2f}, p-value={p_val:.2g}\",\n            col=\"g\",\n        )\n    else:\n        # Stats comparison for non-normal distributions\n        # The Mann-Whitney U test is a non-parametric version of the t-test for independent samples.\n        u2d, p_val = mannwhitneyu(poss_accs_dict[\"2D\"], poss_accs_dict[\"3D\"], alternative=\"two-sided\", method=\"auto\")\n        nx, ny = len(poss_accs_dict[\"2D\"]), len(poss_accs_dict[\"3D\"])\n        assert nx == ny  # noqa: S101\n        u3d = nx * ny - u2d\n\n        u_test = np.minimum(u2d, u3d)\n\n        # Calculate effect size\n        # 1. Pearson's r (however, not applicable for non-normal distributions)\n        se = np.sqrt(nx * ny * (nx + ny + 1) / 12)\n        e_u = nx * ny / 2  # expected value\n        z = (u_test - e_u) / se  # z-score\n        pearson_r = z / np.sqrt(nx + ny)\n\n        # 2. Probability of superiority (better for non-normal distributions)\n        ps = u_test / (nx * ny)\n\n        cprint(\n            string=f\"\\nStatistical difference between maximal empirical accuracies (MEA) of 2D &amp; 3D (both N={nx}): \"\n            f\"Mann-Whitney U-Test={u_test:.1f}, Z={z:.2f}, p-value={p_val:.2g}; \"\n            f\"effect size: r={pearson_r:.2f}, ps={ps:.2%}.\",\n            col=\"g\",\n        )\n</code></pre>"},{"location":"reference/modeling/compute_similarity/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> compute_similarity","text":""},{"location":"reference/modeling/compute_similarity/#facesim3d.modeling.compute_similarity","title":"compute_similarity","text":"<p>Compute similarity matrices from feature tables.</p> <p>Functions:</p> Name Description <code>compute_cosine_similarity_matrix_from_features</code> <p>Compute the cosine similarity matrix according to a given feature matrix.</p> <code>compute_feature_similarity_matrix</code> <p>Compute the similarity matrix of a given feature table.</p> <code>compute_pearson_correlation_between_two_feature_matrices</code> <p>Compute the Pearson correlation between two feature matrices.</p> <code>cosine_similarity</code> <p>Compute the cosine similarity between two vectors.</p> <code>euclidean_distance</code> <p>Compute the Euclidean distance between two vectors.</p>"},{"location":"reference/modeling/compute_similarity/#facesim3d.modeling.compute_similarity.compute_cosine_similarity_matrix_from_features","title":"compute_cosine_similarity_matrix_from_features","text":"<pre><code>compute_cosine_similarity_matrix_from_features(\n    features: ndarray,\n) -&gt; ndarray\n</code></pre> <p>Compute the cosine similarity matrix according to a given feature matrix.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>ndarray</code> <p>(n_items, m_features)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>cosine similarity matrix (n_items, n_items)</p> Source code in <code>code/facesim3d/modeling/compute_similarity.py</code> <pre><code>def compute_cosine_similarity_matrix_from_features(features: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the cosine similarity matrix according to a given feature matrix.\n\n    :param features: (n_items, m_features)\n    :return: cosine similarity matrix (n_items, n_items)\n    \"\"\"\n    # Take the magnitude of over model dimensions per face\n    magnitude_per_vice_dim = np.linalg.norm(features, axis=1)  # (n_faces, )\n\n    # Outer product of the magnitude\n    magnitude_per_cell = np.outer(magnitude_per_vice_dim, magnitude_per_vice_dim)  # (n_faces, n_faces)\n\n    # Dot product of the weights (compare weights/dimensions of each face pair)\n    similarity_matrix = features @ features.T  # (n_faces, n_faces)\n\n    # Normalize to get the cosine similarity for each face pair\n    return similarity_matrix / magnitude_per_cell  # (n_faces, n_faces)\n</code></pre>"},{"location":"reference/modeling/compute_similarity/#facesim3d.modeling.compute_similarity.compute_feature_similarity_matrix","title":"compute_feature_similarity_matrix","text":"<pre><code>compute_feature_similarity_matrix(\n    feature_table: DataFrame,\n    pca: bool | float = False,\n    metric: str = \"cosine\",\n    z_score: bool = True,\n) -&gt; NDArray[float64]\n</code></pre> <p>Compute the similarity matrix of a given feature table.</p> <p>Parameters:</p> Name Type Description Default <code>feature_table</code> <code>DataFrame</code> <p>table with features of heads</p> required <code>pca</code> <code>bool | float</code> <p>False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that pca [float] *100 % of variance is explained</p> <code>False</code> <code>z_score</code> <code>bool</code> <p>True: z-score features before computing similarity matrix</p> <code>True</code> <code>metric</code> <code>str</code> <p>similarity metric to use (cosine, Euclidean)</p> <code>'cosine'</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>similarity matrix</p> Source code in <code>code/facesim3d/modeling/compute_similarity.py</code> <pre><code>def compute_feature_similarity_matrix(\n    feature_table: pd.DataFrame,\n    pca: bool | float = False,\n    metric: str = \"cosine\",\n    z_score: bool = True,\n) -&gt; npt.NDArray[np.float64]:\n    \"\"\"\n    Compute the similarity matrix of a given feature table.\n\n    :param feature_table: table with features of heads\n    :param pca: False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that\n                pca [float] *100 % of variance is explained\n    :param z_score: True: z-score features before computing similarity matrix\n    :param metric: similarity metric to use (cosine, Euclidean)\n    :return: similarity matrix\n    \"\"\"\n    if pca &lt; 0.0 or pca &gt;= 1.0:\n        msg = \"pca must be between 0 and 1!\"\n        raise ValueError(msg)\n    metric = metric.lower()\n    if metric not in {\"cosine\", \"euclidean\"}:\n        msg = \"metric must be either 'cosine' or 'euclidean'!\"\n        raise ValueError(msg)\n\n    similarity_metric = cosine_similarity if metric == \"cosine\" else euclidean_distance\n\n    # Scale features (i.e., z-transform per dimension / column) before computing similarity matrix\n    if z_score:\n        scaler = StandardScaler().set_output(transform=\"pandas\")\n        feature_table = scaler.fit_transform(X=feature_table)\n        # this is the same as: scipy.stats.zscore(feature_table.to_numpy(), axis=0)\n\n    # Run PCA (if requested)\n    pca_feat_tab = None  # init\n    if pca:\n        pca_model = PCA(\n            n_components=pca,\n            svd_solver=\"full\",  # must be 0 &lt; pca &lt; 1 for svd_solver=\"full\" (see docs)\n        )  # .set_output(transform=\"pandas\")\n        # this finds n components such that pca*100 % of variance is explained\n        pca_feat_tab = pca_model.fit_transform(feature_table.to_numpy())  # (n_faces, n_features)\n        # pca_model.components_\n        explained_variance = pca_model.explained_variance_ratio_.sum()\n        print(f\"First {pca_model.n_components_} PCA components explain {explained_variance * 100:.1f}% of variance.\")\n        # f\"-&gt;\\t{pca_model.explained_variance_ratio_}\")\n\n    # Compute similarity matrix from given features\n    if metric == \"cosine\":\n        try:\n            feat_sim_mat = compute_cosine_similarity_matrix_from_features(\n                features=feature_table.to_numpy().astype(np.float64) if not pca else pca_feat_tab.astype(np.float64)\n            )\n            # Take care with interpretation of cosine sim, since 1 == identical, -1 == opposite, 0 == orthogonal.\n            # After normalization, this is not the case anymore.\n            return normalize(feat_sim_mat, lower_bound=0.0, upper_bound=1.0)\n        except TypeError as e:\n            cprint(e, col=\"r\")\n            pass\n\n    # else use: \"euclidean\" (or if the cosine computation above failed; this was the case with VGGface features)\n    feat_sim_mat = np.identity(len(feature_table))\n    for _i, head_pair in enumerate(\n        tqdm(\n            itertools.combinations(feature_table.index, r=2),\n            desc=\"Computing feature similarity matrix\",\n            total=np.math.comb(len(feature_table.index), 2),\n            colour=\"#5ED19B\",\n        )\n    ):\n        h1, h2 = head_pair\n        idx1 = np.where(feature_table.index == h1)[0][0]\n        idx2 = np.where(feature_table.index == h2)[0][0]\n\n        # Compute cosine similarity\n        if pca:\n            similarity = similarity_metric(vec1=pca_feat_tab[idx1], vec2=pca_feat_tab[idx2])\n        else:\n            similarity = similarity_metric(vec1=feature_table.loc[h1], vec2=feature_table.loc[h2])\n\n        # Fill in matrix\n        feat_sim_mat[idx1, idx2] = similarity\n        feat_sim_mat[idx2, idx1] = similarity\n    # For cosine: feat_sim_mat == compute_cosine_similarity_matrix_from_features(feature_table.to_numpy())\n\n    # Normalize similarity matrix to [0, 1]\n    if metric == \"euclidean\":\n        feat_sim_mat[np.diag_indices_from(feat_sim_mat)] = np.nan  # fill diagonal with nan's\n        feat_sim_mat = 1 - feat_sim_mat / np.nanmax(feat_sim_mat)  # normalize\n        feat_sim_mat[np.diag_indices_from(feat_sim_mat)] = 1.0  # refill diagonal with 1's\n    else:\n        feat_sim_mat = normalize(feat_sim_mat, lower_bound=0.0, upper_bound=1.0)\n\n    return feat_sim_mat\n</code></pre>"},{"location":"reference/modeling/compute_similarity/#facesim3d.modeling.compute_similarity.compute_pearson_correlation_between_two_feature_matrices","title":"compute_pearson_correlation_between_two_feature_matrices","text":"<pre><code>compute_pearson_correlation_between_two_feature_matrices(\n    x: ndarray, y: ndarray\n) -&gt; ndarray\n</code></pre> <p>Compute the Pearson correlation between two feature matrices.</p> <p>Both matrices must share at least one dimension with the same length.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>feature matrix X with shape (N, M)</p> required <code>y</code> <code>ndarray</code> <p>feature matrix Y with shape (N, K)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>correlation matrix of shape (M X K)</p> Source code in <code>code/facesim3d/modeling/compute_similarity.py</code> <pre><code>def compute_pearson_correlation_between_two_feature_matrices(x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the Pearson correlation between two feature matrices.\n\n    Both matrices must share at least one dimension with the same length.\n\n    :param x: feature matrix X with shape (N, M)\n    :param y: feature matrix Y with shape (N, K)\n    :return: correlation matrix of shape (M X K)\n    \"\"\"\n    if x.shape[0] != y.shape[0]:\n        msg = \"Make sure that matrix x comes with shape (N, M), and matrix y with shape (N, K)!\"\n        raise ValueError(msg)\n\n    # Z-score along common dimension\n    x = zscore(x, axis=0)  # shape (N, M)\n    y = zscore(y, axis=0)  # shape (N, K)\n\n    # Matrix multiplication along the shared dimension\n    r_matrix = x.T @ y  # -&gt; shape (M, K)\n\n    # Divide by product of standard deviation along the shared dimension\n    de_x = np.sqrt(np.sum(x**2, axis=0))\n    de_y = np.sqrt(np.sum(y**2, axis=0))\n    de = de_x.reshape((de_x.shape[0], 1)) @ de_y.reshape((1, de_y.shape[0]))\n    # Note, this should be equal to N:\n    # np.allclose(a=np.ones((x.shape[1], y.shape[1])) * x.shape[0], b=de)  # noqa: ERA001\n    # Note: np.isclose(a=corr_feat_mat[i, j], b=scipy.stats.pearsonr(x=x[:, i], y=y[:, j])[0])  # noqa: ERA001\n    return r_matrix / de\n</code></pre>"},{"location":"reference/modeling/compute_similarity/#facesim3d.modeling.compute_similarity.cosine_similarity","title":"cosine_similarity","text":"<pre><code>cosine_similarity(vec1: ndarray, vec2: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the cosine similarity between two vectors.</p> <p>Parameters:</p> Name Type Description Default <code>vec1</code> <code>ndarray</code> <p>vector 1</p> required <code>vec2</code> <code>ndarray</code> <p>vector 2</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>cosine similarity of two vectors</p> Source code in <code>code/facesim3d/modeling/compute_similarity.py</code> <pre><code>def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the cosine similarity between two vectors.\n\n    :param vec1: vector 1\n    :param vec2: vector 2\n    :return: cosine similarity of two vectors\n    \"\"\"\n    # np.inner(vec1, vec2) ==  np.dot(vec1, vec2) | vec1 @ vec2.T  # noqa: ERA001\n    return np.inner(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n</code></pre>"},{"location":"reference/modeling/compute_similarity/#facesim3d.modeling.compute_similarity.euclidean_distance","title":"euclidean_distance","text":"<pre><code>euclidean_distance(vec1: ndarray, vec2: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the Euclidean distance between two vectors.</p> <p>Parameters:</p> Name Type Description Default <code>vec1</code> <code>ndarray</code> <p>vector 1</p> required <code>vec2</code> <code>ndarray</code> <p>vector 2</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Euclidean distance of two vectors</p> Source code in <code>code/facesim3d/modeling/compute_similarity.py</code> <pre><code>def euclidean_distance(vec1: np.ndarray, vec2: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the Euclidean distance between two vectors.\n\n    :param vec1: vector 1\n    :param vec2: vector 2\n    :return: Euclidean distance of two vectors\n    \"\"\"\n    return np.linalg.norm(vec1 - vec2)\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> face_attribute_processing","text":""},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing","title":"face_attribute_processing","text":"<p>Extract attributes from face images.</p> <p>Classes:</p> Name Description <code>FaceViews</code> <p>Class for face views.</p> <p>Functions:</p> Name Description <code>cfd_var_converter</code> <p>Convert the <code>CFD</code> variable ID to a corresponding label name, and vice versa.</p> <code>display_face</code> <p>Display the <code>CFD</code> face given its head ID.</p> <code>display_set_of_faces</code> <p>Display a set of heads in a grid.</p> <code>face_image_path</code> <p>Construct the path to a face image.</p> <code>get_cfd_code_table</code> <p>Load the <code>CFD</code> code table.</p> <code>get_cfd_features</code> <p>Get <code>CFD</code> features.</p> <code>get_cfd_features_for_models</code> <p>Get <code>CFD</code> features for the given list of head models.</p> <code>get_head_mapper_table</code> <p>Get the table for mapping head numbers to head indices.</p> <code>head_index_to_head_nr</code> <p>Convert the head index (<code>'idx'</code> in <code>'headnmap.csv'</code>) to the head number (<code>'Head#'</code>).</p> <code>head_nr_to_index</code> <p>Convert the head number (<code>'Head#'</code>) to the head index (<code>'idx'</code> in <code>'headnmap.csv'</code>).</p> <code>head_nr_to_main_matrix_index</code> <p>Convert a head number to a corresponding matrix index in the main study.</p> <code>head_nr_to_pilot_matrix_index</code> <p>Convert a head number to a corresponding matrix index for the pilot studies.</p> <code>heads_naming_converter_table</code> <p>Load the table for head naming conversion.</p> <code>list_faulty_heads</code> <p>List faulty heads (i.e., heads for which the reconstruction is not optimal).</p> <code>main_index_to_model_name</code> <p>Convert the head index to a head model name in the main study.</p> <code>main_matrix_index_to_head_nr</code> <p>Convert the index to a corresponding head number in the main study.</p> <code>pilot_index_to_model_name</code> <p>Convert the head index to the head model name.</p> <code>pilot_matrix_index_to_head_nr</code> <p>Convert the matrix index of a head to a head number in the pilot studies.</p>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.FaceViews","title":"FaceViews","text":"<pre><code>FaceViews()\n</code></pre> <p>Class for face views.</p> <p>Initialise FaceViews class.</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialise FaceViews class.\"\"\"\n    self.path_to_views = paths.data.cfd.faceviews\n    self._check_path_to_views()\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.cfd_var_converter","title":"cfd_var_converter","text":"<pre><code>cfd_var_converter(var_id_or_label: str) -&gt; DataFrame\n</code></pre> <p>Convert the <code>CFD</code> variable ID to a corresponding label name, and vice versa.</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def cfd_var_converter(var_id_or_label: str) -&gt; pd.DataFrame:\n    \"\"\"Convert the `CFD` variable ID to a corresponding label name, and vice versa.\"\"\"\n    cfd_code_tab = get_cfd_code_table()\n\n    if var_id_or_label[1].isnumeric():\n        # ID -&gt; label name\n        var_return = cfd_code_tab.VarLabel[cfd_code_tab.VarID.str.contains(var_id_or_label, case=False)].item()\n    else:\n        # label name -&gt; ID\n        var_return = cfd_code_tab.VarID[cfd_code_tab.VarLabel.str.match(var_id_or_label, case=False)]\n\n        if len(var_return) &gt; 1:\n            print(\"Note, there are various IDs with that label name:\", var_return.to_list())\n\n    return var_return\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.display_face","title":"display_face","text":"<pre><code>display_face(\n    head_id: str | int,\n    data_mode: str = \"3d-reconstructions\",\n    angle: str | float | None = None,\n    interactive: bool = False,\n    show: bool = True,\n    verbose: bool = False,\n) -&gt; Image\n</code></pre> <p>Display the <code>CFD</code> face given its head ID.</p> <p>Parameters:</p> Name Type Description Default <code>head_id</code> <code>str | int</code> <p>either image name (e.g. \"CFD-WF-001-003-N\"), OR head number (e.g., \"Head5\"), OR head index</p> required <code>data_mode</code> <code>str</code> <p>path to the \"2d-original\", \"3d-reconstructions\", or \"3d-perspectives\"</p> <code>'3d-reconstructions'</code> <code>angle</code> <code>str | float | None</code> <p>viewing angle of face to display [only if data_mode == \"3d-perspectives\"]</p> <code>None</code> <code>interactive</code> <code>bool</code> <p>if True display 3D (.obj) in interactive mode.</p> <code>False</code> <code>show</code> <code>bool</code> <p>if True show image</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>if True print the path to the image.</p> <code>False</code> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def display_face(\n    head_id: str | int,\n    data_mode: str = \"3d-reconstructions\",\n    angle: str | float | None = None,\n    interactive: bool = False,\n    show: bool = True,\n    verbose: bool = False,\n) -&gt; Image:\n    \"\"\"\n    Display the `CFD` face given its head ID.\n\n    :param head_id: either image name (e.g. \"CFD-WF-001-003-N\"), OR head number (e.g., \"Head5\"), OR\n                    head index\n    :param data_mode: path to the \"2d-original\", \"3d-reconstructions\", or \"3d-perspectives\"\n    :param angle: viewing angle of face to display [only if data_mode == \"3d-perspectives\"]\n    :param interactive: if True display 3D (.obj) in interactive mode.\n    :param show: if True show image\n    :param verbose: if True print the path to the image.\n    \"\"\"\n    data_mode = data_mode.lower()\n    if data_mode == \"3d-perspectives\" and angle is None:\n        msg = \"angle must be given for data_mode == '3d-perspectives'\"\n        raise ValueError(msg)\n\n    original = \"original\" in data_mode or \"3d-perspectives\" in data_mode\n    if original and interactive:\n        cprint(string=\"For interactively displaying a 3D face mesh, set 'original' to False!\", col=\"r\")\n\n    # Display image\n    path_to_image, head_id = face_image_path(head_id=head_id, data_mode=data_mode, return_head_id=True, angle=angle)\n\n    if original:\n        face_image = Image.open(path_to_image)\n        if show:\n            face_image.show()\n\n    else:\n        path_to_3d_obj = path_to_image.replace(f\"{head_id}_screenshot.png\", f\"{head_id}.obj\")\n        face_image = vedo.Mesh(inputobj=path_to_3d_obj)\n        face_image.texture(tname=path_to_3d_obj.replace(\".obj\", \".png\"), scale=0.1)\n\n        if interactive:\n            # This is buggy on Mac. Cannot be called multiple times in one session.\n            # Documentation (incl. shortcuts): https://vedo.embl.es/autodocs/content/vedo/index.html\n            cprint(string=\"\\nPress 'q' to close window!\", col=\"b\")\n            face_image.show().close()\n        else:\n            if not Path(path_to_image).is_file():\n                vedo.settings.screenshotTransparentBackground = True\n                plotter = vedo.Plotter(interactive=interactive, offscreen=True)\n                plotter.show(face_image, zoom=1.8)\n                plotter.screenshot(path_to_image, scale=1).close()\n            face_image = Image.open(path_to_image)\n            if show:\n                face_image.show()\n\n    if verbose:\n        cprint(string=f\"\\nImage path of '{head_index_to_head_nr(face_idx=head_id)}': {path_to_image}\", col=\"b\")\n\n    return face_image\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.display_set_of_faces","title":"display_set_of_faces","text":"<pre><code>display_set_of_faces(\n    list_head_ids: list[str | int],\n    data_mode: str,\n    num_suffix: str = \"\",\n    verbose: bool = False,\n) -&gt; tuple[Any, ndarray]\n</code></pre> <p>Display a set of heads in a grid.</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def display_set_of_faces(\n    list_head_ids: list[str | int], data_mode: str, num_suffix: str = \"\", verbose: bool = False\n) -&gt; tuple[Any, np.ndarray]:\n    \"\"\"Display a set of heads in a grid.\"\"\"\n    # Prepare data_mode\n    data_mode = data_mode.lower()\n    data_mode_suffix = (\n        \"original\" if \"original\" in data_mode else \"3D-reconstructed\" if \"3d-recon\" in data_mode else \"3D-perspectives\"\n    )\n\n    # Load images\n    list_of_imgs = []\n    for head_id in list_head_ids:\n        list_of_imgs.append(\n            display_face(head_id=head_id, data_mode=data_mode, interactive=False, show=False, verbose=verbose)\n        )\n\n    # Plot in grid\n    # TODO: use this function in display_representative_faces() in computational_choice_model.py  # noqa: FIX002\n    grid_shape = get_n_cols_and_rows(n_plots=len(list_head_ids), square=True)\n    fig, axes = plt.subplots(\n        *grid_shape,\n        figsize=(grid_shape[1] * 2, grid_shape[0] * 2),\n        sharex=True,\n        sharey=True,\n        num=f\"{len(list_head_ids)} of the {data_mode_suffix} CFD faces {num_suffix}\",\n    )\n\n    for i, (face_id, face_img) in enumerate(zip(list_head_ids, list_of_imgs, strict=True)):\n        axes.flatten()[i].imshow(face_img)\n        axes.flatten()[i].set_xticks([])\n        axes.flatten()[i].set_xlabel(face_id)\n        axes.flatten()[i].yaxis.set_visible(False)\n        for spine in axes.flatten()[i].spines.values():  # remove axes-box around image\n            spine.set_visible(False)\n        fig.tight_layout()\n        plt.show()\n\n    return fig, axes\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.face_image_path","title":"face_image_path","text":"<pre><code>face_image_path(\n    head_id: str | int,\n    data_mode: str = \"3d-reconstructions\",\n    return_head_id: bool = False,\n    angle: float | str | None = None,\n) -&gt; str | tuple[str, int]\n</code></pre> <p>Construct the path to a face image.</p> <p>Parameters:</p> Name Type Description Default <code>head_id</code> <code>str | int</code> <p>Head number, as [str], e.g., \"Head4\", or as [int], e.g., 4.</p> required <code>data_mode</code> <code>str</code> <p>path to the \"2d-original\", \"3d-reconstructions\", or \"3d-perspectives\"</p> <code>'3d-reconstructions'</code> <code>return_head_id</code> <code>bool</code> <p>whether to return the head number (as it appears in the rating files, 'TrialResults.csv')</p> <code>False</code> <code>angle</code> <code>float | str | None</code> <p>for data_mode==\"3d-perspectives\", a face angle needs to be given.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | tuple[str, int]</code> <p>path to the face image</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def face_image_path(\n    head_id: str | int,\n    data_mode: str = \"3d-reconstructions\",\n    return_head_id: bool = False,\n    angle: float | str | None = None,\n) -&gt; str | tuple[str, int]:\n    \"\"\"\n    Construct the path to a face image.\n\n    :param head_id: Head number, as [str], e.g., \"Head4\", or as [int], e.g., 4.\n    :param data_mode: path to the \"2d-original\", \"3d-reconstructions\", or \"3d-perspectives\"\n    :param return_head_id: whether to return the head number (as it appears in the rating files, '*TrialResults*.csv')\n    :param angle: for data_mode==\"3d-perspectives\", a face angle needs to be given.\n    :return: path to the face image\n    \"\"\"\n    data_mode = data_mode.lower()\n\n    # Get mapping table\n    head_map_tab = get_head_mapper_table()\n\n    if isinstance(head_id, str):\n        # Map ID to head index\n        if head_id.startswith(\"CFD-\"):\n            head_id = head_map_tab[head_map_tab.CFD_filename == head_id + \".jpg\"].idx.item()\n        elif head_id.startswith(\"Head\"):\n            head_id = head_map_tab[head_map_tab.head_nr == head_id].idx.item()\n        else:\n            msg = f\"{head_id = } unknown!\"\n            raise ValueError(msg)\n\n    if data_mode == \"3d-perspectives\":\n        if angle is None:\n            msg = \"For data_mode == '3d-perspectives' angle must be given [int | float | str == 'frontal']!\"\n            raise ValueError(msg)\n\n        if isinstance(angle, str):\n            angle = angle.lower()\n\n        if angle == \"frontal\":\n            path_to_image = Path(paths.data.cfd.faceviews, f\"head-{head_id:03d}_frontal.png\")\n\n        else:\n            # even for floats (e.g., 348.75), integer parts are all unique, so search for those\n            angle = round(float(angle), 2)  # 348.7512 -&gt; 348.75, 315 -&gt; 315.00\n            angle = int(angle) if angle.is_integer() else angle  # in case of int, map back: 315.00 -&gt; 315\n            path_to_image = Path(paths.data.cfd.faceviews, f\"head-{head_id:03d}_angle-{angle}.png\")\n\n    else:\n        th_gender = 60  # threshold\n        path_to_pid = Path(paths.data.unity.cfd, \"female\" if head_id &lt;= th_gender else \"male\", str(head_id))\n\n        path_to_image = str(\n            path_to_pid / f\"{head_id}_inputs.jpg\"\n            if \"original\" in data_mode\n            else path_to_pid / f\"{head_id}_screenshot.png\"\n        )\n\n    if return_head_id:\n        return path_to_image, head_id\n    return path_to_image\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.get_cfd_code_table","title":"get_cfd_code_table  <code>cached</code>","text":"<pre><code>get_cfd_code_table()\n</code></pre> <p>Load the <code>CFD</code> code table.</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>@cache\ndef get_cfd_code_table():\n    \"\"\"Load the `CFD` code table.\"\"\"\n    __cfd_code_tab = pd.read_excel(\n        io=Path(paths.data.CFD, \"CFD 3.0 Norming Data and Codebook.xlsx\"),\n        sheet_name=\"CFD 3.0 Codebook\",\n        header=11,\n        usecols=range(6),\n    )\n    return __cfd_code_tab.drop(index=0)\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.get_cfd_features","title":"get_cfd_features","text":"<pre><code>get_cfd_features(\n    set_name: str = \"main\",\n    drop_nan_col: bool = True,\n    physical_attr_only: bool = True,\n) -&gt; DataFrame\n</code></pre> <p>Get <code>CFD</code> features.</p> <p>Parameters:</p> Name Type Description Default <code>set_name</code> <code>str</code> <p>CFD set name</p> <code>'main'</code> <code>drop_nan_col</code> <code>bool</code> <p>whether to drop columns that contain only nan</p> <code>True</code> <code>physical_attr_only</code> <code>bool</code> <p>whether to drop columns that do not represent physical attributes</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>CFD feature table</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def get_cfd_features(\n    set_name: str = \"main\", drop_nan_col: bool = True, physical_attr_only: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Get `CFD` features.\n\n    :param set_name: CFD set name\n    :param drop_nan_col: whether to drop columns that contain only nan\n    :param physical_attr_only: whether to drop columns that do not represent physical attributes\n    :return: CFD feature table\n    \"\"\"\n    set_name = set_name.lower()\n    set_dict = {\n        \"main\": \"CFD U.S. Norming Data\",\n        \"mr\": \"CFD-MR U.S. Norming Data\",\n        \"india-us\": \"CFD-I U.S. Norming Data\",\n        \"india-ind\": \"CFD-I INDIA Norming Data\",\n    }\n    set_names = list(set_dict.keys())\n    if set_name not in set_names:\n        msg = f\"set_name must be in {set_names}!\"\n        raise ValueError(msg)\n    cfd_tab = pd.read_excel(\n        io=Path(paths.data.CFD, \"CFD 3.0 Norming Data and Codebook.xlsx\"),\n        sheet_name=set_dict[set_name],\n        header=6,\n        index_col=0,\n    )\n    cfd_tab = cfd_tab.drop(index=np.nan)\n    cfd_tab = cfd_tab.drop(index=\"Model\")\n    cfd_tab.index = cfd_tab.index.set_names(names=\"Model\")\n\n    if drop_nan_col:\n        for col in cfd_tab.columns:\n            if (not cfd_tab[col].notna().any()) or (physical_attr_only and col[0] != \"P\"):\n                # if all entries in column are nan OR it is not a physical attribute: drop column\n                cfd_tab = cfd_tab.drop(columns=col)\n\n    return cfd_tab\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.get_cfd_features_for_models","title":"get_cfd_features_for_models","text":"<pre><code>get_cfd_features_for_models(\n    list_of_models: list[Any] | None = None,\n    physical_attr_only: bool = True,\n) -&gt; DataFrame\n</code></pre> <p>Get <code>CFD</code> features for the given list of head models.</p> <p>Parameters:</p> Name Type Description Default <code>list_of_models</code> <code>list[Any] | None</code> <p>list of models</p> <code>None</code> <code>physical_attr_only</code> <code>bool</code> <p>whether to drop columns that do not represent physical attributes</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>feature table for a list of models</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def get_cfd_features_for_models(\n    list_of_models: list[Any] | None = None, physical_attr_only: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Get `CFD` features for the given list of head models.\n\n    :param list_of_models: list of models\n    :param physical_attr_only: whether to drop columns that do not represent physical attributes\n    :return: feature table for a list of models\n    \"\"\"\n    _feat_tab = get_cfd_features(physical_attr_only=physical_attr_only)\n    list_of_models = _feat_tab.index if list_of_models is None else list_of_models\n    return _feat_tab.loc[list_of_models]\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.get_head_mapper_table","title":"get_head_mapper_table  <code>cached</code>","text":"<pre><code>get_head_mapper_table()\n</code></pre> <p>Get the table for mapping head numbers to head indices.</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>@cache\ndef get_head_mapper_table():\n    \"\"\"Get the table for mapping head numbers to head indices.\"\"\"\n    return pd.read_csv(Path(paths.data.unity.cfd, \"headnmap.csv\"), names=[\"CFD_filename\", \"idx\", \"unknown\", \"head_nr\"])\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.head_index_to_head_nr","title":"head_index_to_head_nr","text":"<pre><code>head_index_to_head_nr(face_idx: int) -&gt; str\n</code></pre> <p>Convert the head index (<code>'idx'</code> in <code>'headnmap.csv'</code>) to the head number (<code>'Head#'</code>).</p> <p>Note</p> <p>This is the inverse function of <code>head_nr_to_index()</code>.</p> <p>For previous pilot experiments</p> <p>This is not being used for the pilot experiment with fewer heads.</p> <p>Returns:</p> Type Description <code>str</code> <p>head number</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def head_index_to_head_nr(face_idx: int) -&gt; str:\n    \"\"\"\n    Convert the head index (`'idx'` in `'headnmap.csv'`) to the head number (`'Head#'`).\n\n    !!! note\n        This is the inverse function of `head_nr_to_index()`.\n\n    !!! note \"For previous pilot experiments\"\n        This is not being used for the pilot experiment with fewer heads.\n\n    :return: head number\n    \"\"\"\n    map_tab = get_head_mapper_table()\n    return map_tab.loc[map_tab.idx == face_idx, \"head_nr\"].values[0]  # 'Head#'  # noqa: PD011\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.head_nr_to_index","title":"head_nr_to_index","text":"<pre><code>head_nr_to_index(head_id: str | int | None) -&gt; int\n</code></pre> <p>Convert the head number (<code>'Head#'</code>) to the head index (<code>'idx'</code> in <code>'headnmap.csv'</code>).</p> <p>Parameters:</p> Name Type Description Default <code>head_id</code> <code>str | int | None</code> <p>Head number, as [str], e.g., \"Head4\", or as [int], e.g., 4.</p> required <p>Returns:</p> Type Description <code>int</code> <p>head index</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def head_nr_to_index(head_id: str | int | None) -&gt; int:\n    \"\"\"\n    Convert the head number (`'Head#'`) to the head index (`'idx'` in `'headnmap.csv'`).\n\n    :param head_id: Head number, as [str], e.g., \"Head4\", or as [int], e.g., 4.\n    :return: head index\n    \"\"\"\n    map_tab = get_head_mapper_table()\n    if isinstance(head_id, int):\n        head_id = f\"Head{head_id}\"\n    return map_tab.loc[map_tab.head_nr == head_id, \"idx\"].values[0]  # noqa: PD011\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.head_nr_to_main_matrix_index","title":"head_nr_to_main_matrix_index","text":"<pre><code>head_nr_to_main_matrix_index(\n    head_id: str | int | None,\n) -&gt; int\n</code></pre> <p>Convert a head number to a corresponding matrix index in the main study.</p> <p>Convert the head number (as they appear in the rating files, <code>'*TrialResults*.csv'</code>) to the index as it appears, e.g., in the similarity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>head_id</code> <code>str | int | None</code> <p>Head number, as [str], e.g., \"Head4\", or as [int], e.g., 4.</p> required <p>Returns:</p> Type Description <code>int</code> <p>head index</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def head_nr_to_main_matrix_index(head_id: str | int | None) -&gt; int:\n    \"\"\"\n    Convert a head number to a corresponding matrix index in the main study.\n\n    Convert the head number (as they appear in the rating files, `'*TrialResults*.csv'`) to the index as it\n    appears, e.g., in the similarity matrix.\n\n    :param head_id: Head number, as [str], e.g., \"Head4\", or as [int], e.g., 4.\n    :return: head index\n    \"\"\"\n    if isinstance(head_id, str):\n        head_id = int(head_id.title().removeprefix(\"Head\"))\n    if head_id not in range(1, params.main.n_faces + 1):\n        msg = \"head_id is out of range!\"\n        raise ValueError(msg)\n    head_idx = head_id - 1\n    return int(head_idx)\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.head_nr_to_pilot_matrix_index","title":"head_nr_to_pilot_matrix_index","text":"<pre><code>head_nr_to_pilot_matrix_index(\n    head_id: str | int | None, pilot_version: int = 2\n) -&gt; int\n</code></pre> <p>Convert a head number to a corresponding matrix index for the pilot studies.</p> <p>Convert the head number (as they appear in the rating files, <code>'*TrialResults*.csv'</code>) to the pilot index as it appears, e.g., in the similarity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>head_id</code> <code>str | int | None</code> <p>Head number, as [str], e.g., \"Head4\", or as [int], e.g., 4.</p> required <code>pilot_version</code> <code>int</code> <p>version of pilot (v1, v2)</p> <code>2</code> <p>Returns:</p> Type Description <code>int</code> <p>head index</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def head_nr_to_pilot_matrix_index(head_id: str | int | None, pilot_version: int = 2) -&gt; int:\n    \"\"\"\n    Convert a head number to a corresponding matrix index for the pilot studies.\n\n    Convert the head number (as they appear in the rating files, `'*TrialResults*.csv'`) to the pilot index\n    as it appears, e.g., in the similarity matrix.\n\n    :param head_id: Head number, as [str], e.g., \"Head4\", or as [int], e.g., 4.\n    :param pilot_version: version of pilot (v1, v2)\n    :return: head index\n    \"\"\"\n    pv1, pv2 = 1, 2\n    if pilot_version not in {pv1, pv2}:\n        msg = \"pilot_version must be 1 or 2!\"\n        raise ValueError(msg)\n\n    n_fm = (12, 13) if pilot_version == pv2 else (15, 15)  # number of faces per gender (f, m)\n\n    if isinstance(head_id, str):\n        head_id = int(head_id.title().removeprefix(\"Head\"))\n\n    if head_id not in range(1, n_fm[0] + 1) and head_id not in range(51, 51 + n_fm[1]):\n        msg = \"head_id is out of range!\"\n        raise ValueError(msg)\n\n    n_face_split = params.main.n_faces // 2\n    head_idx = (\n        head_id - 1 if head_id &lt;= n_face_split else head_id - (n_face_split + 1) + n_fm[0]\n    )  # female or male faces\n\n    return int(head_idx)\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.heads_naming_converter_table","title":"heads_naming_converter_table","text":"<pre><code>heads_naming_converter_table(\n    pilot_version: int | None = None,\n) -&gt; DataFrame\n</code></pre> <p>Load the table for head naming conversion.</p> <p>Parameters:</p> Name Type Description Default <code>pilot_version</code> <code>int | None</code> <p>None: for the main experiment, OR for pilot: 1, OR: 2.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>converter table</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def heads_naming_converter_table(pilot_version: int | None = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Load the table for head naming conversion.\n\n    :param pilot_version: None: for the main experiment, OR for pilot: 1, OR: 2.\n    :return: converter table\n    \"\"\"\n    pv1, pv2 = 1, 2\n\n    # Load mapping table\n    heads_tab = get_head_mapper_table()\n    # Keep only samples where reconstructions are computed/used\n    heads_tab = heads_tab[~heads_tab.head_nr.str.match(\"_\")]\n    # Keep only the necessary columns\n    heads_tab = heads_tab.drop(columns=[\"idx\", \"unknown\"], inplace=False).copy()\n\n    if isinstance(pilot_version, int):\n        # Extract pilot heads\n        if pilot_version == pv1:\n            heads_tab = heads_tab[heads_tab.head_nr.str.fullmatch(r\"Head([1-9]|1[0-5]|5[1-9]|6[0-5])\")]\n        elif pilot_version == pv2:\n            heads_tab = heads_tab[heads_tab.head_nr.str.fullmatch(r\"Head([1-9]|1[0-2]|5[1-9]|6[0-3])\")]\n        else:\n            msg = f\"pilot_version must be None, {pv1}, OR {pv2}.\"\n            raise ValueError(msg)\n\n        # # Remove .jpg suffix\n\n    # Extract and save naming of Model\n    heads_tab[\"Model\"] = [fn[4:10] for fn in heads_tab.CFD_filename]\n\n    return heads_tab\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.list_faulty_heads","title":"list_faulty_heads","text":"<pre><code>list_faulty_heads(\n    run: bool = False, suffix: str = \"\"\n) -&gt; DataFrame\n</code></pre> <p>List faulty heads (i.e., heads for which the reconstruction is not optimal).</p> <p>DECA has a reported reconstructed error in the eyes (misalignment of the eyes), and open mouth, which is not open in the original image.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>bool</code> <p>if True: open all images and fill in the faulty heads in the table.</p> <code>False</code> <code>suffix</code> <code>str</code> <p>path suffix hinting to the focus of the observation among the face stimuli (e.g., 'eyes')</p> <code>''</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>the list of faulty heads (IDs)</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def list_faulty_heads(run: bool = False, suffix: str = \"\") -&gt; pd.DataFrame:\n    \"\"\"\n    List faulty heads (i.e., heads for which the reconstruction is not optimal).\n\n    DECA has a reported reconstructed error in the eyes (misalignment of the eyes), and open mouth,\n    which is not open in the original image.\n\n    :param run: if True: open all images and fill in the faulty heads in the table.\n    :param suffix: path suffix hinting to the focus of the observation among the face stimuli (e.g., 'eyes')\n    :return: the list of faulty heads (IDs)\n    \"\"\"\n    # Set path to faulty heads table\n    suffix = f\"_{suffix}\" if suffix else \"\"\n    path_to_table = Path(paths.data.unity.cfd, f\"faulty_heads{suffix}.csv\")\n\n    if run:\n        if path_to_table.is_file():\n            print(f\"{path_to_table} exists already!\")\n            df_faulty_heads = pd.read_csv(path_to_table, index_col=0)\n            if not ask_true_false(\"Do you want to add missing heads to table?\"):\n                return df_faulty_heads\n            # In case all heads shall be replaced, delete table manually\n\n        else:\n            df_faulty_heads = pd.DataFrame(columns=[\"head_nr\", \"faulty\"])\n            df_faulty_heads = df_faulty_heads.set_index(\"head_nr\")\n\n        for i in range(1, 159 + 1):\n            # TODO: ?  # noqa: FIX002\n            head_nr = head_index_to_head_nr(face_idx=i)\n\n            if head_nr in df_faulty_heads.index or head_nr == \"_\":\n                continue\n\n            try:\n                display_face(head_id=head_nr, data_mode=\"3d-reconstructions\", interactive=False, verbose=True)\n                q = suffix or \"eyes and/or mouth\"\n                df_faulty_heads.loc[head_nr, \"faulty\"] = int(ask_true_false(f\"{head_nr}: Are/is {q} corrupted?\"))\n                if platform.system().lower() == \"darwin\":\n                    os.system(\"\"\"/usr/bin/osascript -e 'tell app \"Preview\" to close (first window)' \"\"\")  # noqa: S605\n            except AttributeError:\n                cprint(string=f\"ID '{head_nr}' not valid!\", col=\"r\")\n\n            # Save table\n            df_faulty_heads.to_csv(path_to_table)\n\n    else:\n        # Load existing table\n        df_faulty_heads = pd.read_csv(path_to_table, index_col=0)\n\n    return df_faulty_heads\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.main_index_to_model_name","title":"main_index_to_model_name","text":"<pre><code>main_index_to_model_name(face_idx: int) -&gt; str\n</code></pre> <p>Convert the head index to a head model name in the main study.</p> <p>Convert the main study index as it appears, e.g., in the similarity matrix to the name of the corresponding head model (as it appears in the <code>CFD</code> feature table (<code>PFA</code>)).</p> <p>Parameters:</p> Name Type Description Default <code>face_idx</code> <code>int</code> <p>head index in pilot</p> required <p>Returns:</p> Type Description <code>str</code> <p>name of the head model</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def main_index_to_model_name(face_idx: int) -&gt; str:\n    \"\"\"\n    Convert the head index to a head model name in the main study.\n\n    Convert the main study index as it appears, e.g., in the similarity matrix to the name of the corresponding head\n    model (as it appears in the `CFD` feature table (`PFA`)).\n\n    :param face_idx: head index in pilot\n    :return: name of the head model\n    \"\"\"\n    if face_idx not in range(100):\n        msg = \"face_idx is out of range!\"\n        raise ValueError(msg)\n\n    head_nr = main_matrix_index_to_head_nr(face_idx=face_idx)\n    convert_tab = get_head_mapper_table()\n    model_name = convert_tab[convert_tab.head_nr == head_nr].CFD_filename.item()\n    return model_name.removeprefix(\"CFD-\")[:6]\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.main_matrix_index_to_head_nr","title":"main_matrix_index_to_head_nr","text":"<pre><code>main_matrix_index_to_head_nr(face_idx: int) -&gt; str\n</code></pre> <p>Convert the index to a corresponding head number in the main study.</p> <p>Convert the main index as it appears, e.g., in the similarity matrix to the corresponding head number (as they appear in the rating files, <code>'*TrialResults*.csv'</code>).</p> <p>Note</p> <p>This is the inverse function of <code>head_nr_to_main_matrix_index()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>face_idx</code> <code>int</code> <p>head index in the main study</p> required <p>Returns:</p> Type Description <code>str</code> <p>the head number</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def main_matrix_index_to_head_nr(face_idx: int) -&gt; str:\n    \"\"\"\n    Convert the index to a corresponding head number in the main study.\n\n    Convert the main index as it appears, e.g., in the similarity matrix to the corresponding head number\n    (as they appear in the rating files, `'*TrialResults*.csv'`).\n\n    !!! note\n        This is the inverse function of `head_nr_to_main_matrix_index()`.\n\n    :param face_idx: head index in the main study\n    :return: the head number\n    \"\"\"\n    if face_idx not in range(100):\n        msg = \"face_idx is out of range!\"\n        raise ValueError(msg)\n    head_id = face_idx + 1\n    return f\"Head{head_id}\"\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.pilot_index_to_model_name","title":"pilot_index_to_model_name","text":"<pre><code>pilot_index_to_model_name(\n    pilot_face_idx: int, pilot_version: int = 2\n) -&gt; str\n</code></pre> <p>Convert the head index to the head model name.</p> <p>Convert the pilot index as it appears, e.g., in the similarity matrix to the corresponding name of the head model (as it appears in the <code>CFD</code> feature table (<code>PFA</code>)).</p> <p>Parameters:</p> Name Type Description Default <code>pilot_face_idx</code> <code>int</code> <p>head index in the pilot experiment</p> required <code>pilot_version</code> <code>int</code> <p>the version of pilot experiment (v1, v2)</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>name of the head model</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def pilot_index_to_model_name(pilot_face_idx: int, pilot_version: int = 2) -&gt; str:\n    \"\"\"\n    Convert the head index to the head model name.\n\n    Convert the pilot index as it appears, e.g., in the similarity matrix to the corresponding name of the head model\n    (as it appears in the `CFD` feature table (`PFA`)).\n\n    :param pilot_face_idx: head index in the pilot experiment\n    :param pilot_version: the version of pilot experiment (v1, v2)\n    :return: name of the head model\n    \"\"\"\n    pv1, pv2 = 1, 2\n    if pilot_version not in {pv1, pv2}:\n        msg = \"pilot_version must be 1 or 2!\"\n        raise ValueError(msg)\n\n    n_fm = (12, 13) if pilot_version == pv2 else (15, 15)  # number of faces per gender (f, m)\n\n    if pilot_face_idx not in range(n_fm[0] + n_fm[1]):\n        msg = \"pilot_face_idx is out of range!\"\n        raise ValueError(msg)\n\n    head_nr = pilot_matrix_index_to_head_nr(pilot_face_idx=pilot_face_idx, pilot_version=pilot_version)\n    convert_tab = get_head_mapper_table()\n    model_name = convert_tab[convert_tab.head_nr == head_nr].CFD_filename.item()\n    return model_name.removeprefix(\"CFD-\")[:6]\n</code></pre>"},{"location":"reference/modeling/face_attribute_processing/#facesim3d.modeling.face_attribute_processing.pilot_matrix_index_to_head_nr","title":"pilot_matrix_index_to_head_nr","text":"<pre><code>pilot_matrix_index_to_head_nr(\n    pilot_face_idx: int, pilot_version: int = 2\n) -&gt; str\n</code></pre> <p>Convert the matrix index of a head to a head number in the pilot studies.</p> <p>Convert the pilot index as it appears, e.g., in the similarity matrix to the corresponding head number (as they appear in the rating files, <code>'*TrialResults*.csv'</code>).</p> <p>Note</p> <p>This is the inverse function of <code>head_nr_to_pilot_matrix_index()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pilot_face_idx</code> <code>int</code> <p>head index in the pilot matrix</p> required <code>pilot_version</code> <code>int</code> <p>version of pilot experiment (v1, v2)</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>the head number</p> Source code in <code>code/facesim3d/modeling/face_attribute_processing.py</code> <pre><code>def pilot_matrix_index_to_head_nr(pilot_face_idx: int, pilot_version: int = 2) -&gt; str:\n    \"\"\"\n    Convert the matrix index of a head to a head number in the pilot studies.\n\n    Convert the pilot index as it appears, e.g., in the similarity matrix to the corresponding head number\n    (as they appear in the rating files, `'*TrialResults*.csv'`).\n\n    !!! note\n        This is the inverse function of `head_nr_to_pilot_matrix_index()`.\n\n    :param pilot_face_idx: head index in the pilot matrix\n    :param pilot_version: version of pilot experiment (v1, v2)\n    :return: the head number\n    \"\"\"\n    pv1, pv2 = 1, 2\n    if pilot_version not in {pv1, pv2}:\n        msg = f\"pilot_version must be {pv1} or {pv2}!\"\n        raise ValueError(msg)\n\n    n_fm = (12, 13) if pilot_version == pv2 else (15, 15)  # number of faces per gender (f, m)\n\n    if pilot_face_idx not in range(n_fm[0] + n_fm[1]):\n        msg = \"pilot_face_idx is out of range!\"\n        raise ValueError(msg)\n\n    head_id = pilot_face_idx + 1 if pilot_face_idx &lt; n_fm[0] else pilot_face_idx + 51 - n_fm[0]  # female or male faces\n\n    return f\"Head{head_id}\"\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> prep_computational_choice_model","text":""},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model","title":"prep_computational_choice_model","text":"<p>Prepare the computational choice model by loading the best weights and computing the similarity matrix.</p> <p>Author: Simon M. Hofmann Years: 2023</p> <p>Functions:</p> Name Description <code>check_gender_arg</code> <p>Check the gender argument.</p> <code>compute_spose_similarity_matrix</code> <p>Compute the similarity matrix of the <code>SPoSE</code> model.</p> <code>compute_vice_similarity_matrix</code> <p>Compute the similarity matrix of the <code>VICE</code> model.</p> <code>create_path_from_vice_params</code> <p>Create a path from <code>VICE</code> parameters.</p> <code>extract_vice_params_from_path</code> <p>Extract <code>VICE</code> parameters from the path that leads to the model directory.</p> <code>get_best_hp_vice</code> <p>Get the best hyperparameters for <code>VICE</code> models.</p> <code>list_spose_model_performances</code> <p>List <code>SPoSE</code> model performances for a given session.</p> <code>list_vice_model_performances</code> <p>List <code>VICE</code> model performances for a given session.</p> <code>load_best_vice_weights</code> <p>Load the best <code>VICE</code> weights.</p> <code>load_spose_weights</code> <p>Load weights for the <code>SPoSE</code> implementation by the <code>Vision and Computational Cognition Group</code> by Martin N. Hebart.</p> <code>load_vice_similarity_matrix</code> <p>Load the similarity matrix of the <code>VICE</code> model.</p> <code>load_vice_weights</code> <p>Load parameters for <code>VICE</code>.</p>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.check_gender_arg","title":"check_gender_arg","text":"<pre><code>check_gender_arg(gender: str | None = None) -&gt; str | None\n</code></pre> <p>Check the gender argument.</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def check_gender_arg(gender: str | None = None) -&gt; str | None:\n    \"\"\"Check the gender argument.\"\"\"\n    if gender is None:\n        return None\n\n    msg = \"gender must be 'female' OR 'male' OR None!\"\n    if isinstance(gender, str):\n        gender = gender.lower()\n        if gender not in {\"female\", \"male\"}:\n            raise ValueError(msg)\n        return gender\n\n    raise TypeError(msg)\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.compute_spose_similarity_matrix","title":"compute_spose_similarity_matrix","text":"<pre><code>compute_spose_similarity_matrix(\n    session: str,\n    gender: str | None = None,\n    pilot: bool = PILOT,\n    save: bool = True,\n) -&gt; ndarray\n</code></pre> <p>Compute the similarity matrix of the <code>SPoSE</code> model.</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def compute_spose_similarity_matrix(\n    session: str, gender: str | None = None, pilot: bool = params.PILOT, save: bool = True\n) -&gt; np.ndarray:\n    \"\"\"Compute the similarity matrix of the `SPoSE` model.\"\"\"\n    spose_weights, p2_spose_weights = load_spose_weights(\n        session=session, gender=gender, pilot=pilot, return_path=True, **BEST_HP_SPOSE[session]\n    )\n\n    spose_sim_mat = compute_cosine_similarity_matrix_from_features(features=spose_weights)\n\n    # Save the SPoSE similarity matrix\n    if save:\n        save_obj(\n            obj=spose_sim_mat,\n            name=\"similarity_matrix\",\n            folder=p2_spose_weights.parent,\n            as_zip=True,\n            save_as=\"npy\",\n        )\n\n    return spose_sim_mat\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.compute_vice_similarity_matrix","title":"compute_vice_similarity_matrix","text":"<pre><code>compute_vice_similarity_matrix(\n    session: str,\n    gender: str | None = None,\n    hp_search: bool = False,\n    pilot: bool = PILOT,\n    save: bool = True,\n    verbose: bool = False,\n) -&gt; ndarray\n</code></pre> <p>Compute the similarity matrix of the <code>VICE</code> model.</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def compute_vice_similarity_matrix(\n    session: str,\n    gender: str | None = None,\n    hp_search: bool = False,\n    pilot: bool = params.PILOT,\n    save: bool = True,\n    verbose: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Compute the similarity matrix of the `VICE` model.\"\"\"\n    # Get the weight matrix / embedding of the best VICE model in the given session\n    vice_weights, p2_vice_weights = load_best_vice_weights(\n        session=session,\n        gender=gender,\n        hp_search=hp_search,\n        pilot=pilot,\n        pruned=True,\n        verbose=verbose,\n    )\n\n    vice_sim_mat = compute_cosine_similarity_matrix_from_features(features=vice_weights)\n\n    # Save the VICE similarity matrix\n    if save:\n        save_obj(\n            obj=vice_sim_mat,\n            name=\"similarity_matrix\",\n            folder=Path(p2_vice_weights).parent,\n            as_zip=True,\n            save_as=\"npy\",\n        )\n\n    return vice_sim_mat\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.create_path_from_vice_params","title":"create_path_from_vice_params","text":"<pre><code>create_path_from_vice_params(\n    params_dict: dict,\n    gender: str | None = None,\n    pilot: bool = PILOT,\n) -&gt; Path\n</code></pre> <p>Create a path from <code>VICE</code> parameters.</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def create_path_from_vice_params(params_dict: dict, gender: str | None = None, pilot: bool = params.PILOT) -&gt; Path:\n    \"\"\"Create a path from `VICE` parameters.\"\"\"\n    gender = check_gender_arg(gender=gender)\n\n    path_from_params = Path(paths.results.pilot.v2.vice if pilot else paths.results.main.vice)\n\n    hp_suffix = \"\"\n    if \"hp_perc\" in params_dict:\n        if gender is not None:\n            msg = (\n                \"There was no gender-exclusive hyperparameter search! \"\n                \"Please set gender to None OR remove 'hp_perc' from params_dict.\"\n            )\n            raise ValueError(msg)\n        hp_suffix = f\"hp_{params_dict['hp_perc']}perc/\"\n\n    gender_suffix = \"\"\n    if gender is not None:\n        gender_suffix = gender\n\n    return (\n        path_from_params\n        / params_dict[\"session\"]\n        / gender_suffix\n        / hp_suffix\n        / PATH_TO_VICE_WEIGHTS.format(**params_dict)\n    )\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.extract_vice_params_from_path","title":"extract_vice_params_from_path","text":"<pre><code>extract_vice_params_from_path(\n    path_to_model_dir: str,\n) -&gt; Series\n</code></pre> <p>Extract <code>VICE</code> parameters from the path that leads to the model directory.</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def extract_vice_params_from_path(path_to_model_dir: str) -&gt; pd.Series:\n    \"\"\"Extract `VICE` parameters from the path that leads to the model directory.\"\"\"\n    param_keys = [\"session\", \"hp_perc\", \"modality\", \"dims\", \"optim\", \"prior\", \"spike\", \"slab\", \"pi\", \"seed\"]\n    model_params = path_to_model_dir.split(\"/\")\n    has_hp_perc = any((\"hp_\" in pa) for pa in model_params)\n    if not has_hp_perc:\n        param_keys.remove(\"hp_perc\")\n\n    gender = [g for g in [\"female\", \"male\"] if g in model_params]\n    if len(gender) &gt; 1:\n        msg = \"More than one gender found in the path\"\n        raise ValueError(msg)\n    if gender:  # e.g., [\"male\"]\n        import warnings\n\n        warnings.warn(\n            message=\"If gender should be kept, a different implementation is required\",\n            category=UserWarning,\n            stacklevel=2,\n        )\n        gender = gender.pop()\n        cprint(string=f\"Remove '{gender}' from model_params!\", col=\"y\")\n        model_params.remove(gender)\n\n    params_dict = dict(zip(param_keys, model_params, strict=True))\n\n    # Convert params\n    if has_hp_perc:\n        params_dict[\"hp_perc\"] = int(params_dict[\"hp_perc\"].split(\"_\")[-1].removesuffix(\"perc\"))\n    params_dict[\"dims\"] = int(params_dict[\"dims\"].removesuffix(\"d\"))\n    params_dict[\"seed\"] = int(params_dict[\"seed\"].removeprefix(\"seed\"))\n    for param_key in [\"spike\", \"slab\", \"pi\"]:\n        params_dict[param_key] = float(params_dict[param_key])\n\n    return pd.Series(params_dict)\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.get_best_hp_vice","title":"get_best_hp_vice","text":"<pre><code>get_best_hp_vice(\n    hp_search: bool = True,\n    print_n: int = 1,\n    from_config: bool = True,\n) -&gt; dict\n</code></pre> <p>Get the best hyperparameters for <code>VICE</code> models.</p> <p>Parameters:</p> Name Type Description Default <code>hp_search</code> <code>bool</code> <p>True: get the best hyperparameters from the hyperparameter search</p> <code>True</code> <code>print_n</code> <code>int</code> <p>number of the best hyperparameter sets to print</p> <code>1</code> <code>from_config</code> <code>bool</code> <p>True: get the best hyperparameters from the config file</p> <code>True</code> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def get_best_hp_vice(hp_search: bool = True, print_n: int = 1, from_config: bool = True) -&gt; dict:\n    \"\"\"\n    Get the best hyperparameters for `VICE` models.\n\n    :param hp_search: True: get the best hyperparameters from the hyperparameter search\n    :param print_n: number of the best hyperparameter sets to print\n    :param from_config: True: get the best hyperparameters from the config file\n    \"\"\"\n    best_hp_vice = {}  # init\n    if from_config and hp_search:\n        for sess in params.SESSIONS:\n            # They are the same for both conditions (2D, 3D)\n            best_hp_vice.update({sess: {}})\n            best_hp_vice[sess].update(\n                {\n                    \"session\": sess,\n                    \"hp_perc\": params.vice.hp_perc,\n                    \"modality\": params.vice.modality,\n                    \"dims\": params.vice.dims,\n                    \"optim\": params.vice.optim,\n                    \"prior\": params.vice.prior,\n                    \"spike\": params.vice.spike,\n                    \"slab\": params.vice.slab,\n                    \"pi\": params.vice.pi,\n                    \"seed\": params.vice.seed,\n                }\n            )\n\n        return best_hp_vice\n\n    if from_config and not hp_search:\n        cprint(\n            string=\"The config file only contains the best hyperparameters from the hyperparameter search!\\n\"\n            \"Will load best hyperparameters from VICE results folder instead ...\",\n            col=\"y\",\n        )\n\n    # Otherwise extract from the VICE results folder\n    for session in params.SESSIONS:\n        vice_model_table = list_vice_model_performances(session=session, hp_search=hp_search)\n        if hp_search:\n            vice_model_table = vice_model_table[vice_model_table.model_path.str.contains(\"hp_20perc\")]\n\n        if print_n &gt; 0:\n            cprint(string=f\"\\nBest VICE hyperparameters (descending) | Session: {session}\", col=\"b\", fm=\"ul\")\n        for _ctn, _p2_model in enumerate(vice_model_table.model_path.values):\n            # Extract hyperparameters from the model path\n            current_params = extract_vice_params_from_path(_p2_model)\n\n            if print_n &gt; 0:\n                print()\n                print(current_params.loc[[\"spike\", \"slab\", \"pi\"]])\n            if _ctn == 0:\n                best_hp_vice[current_params.session] = current_params.to_dict()\n            if _ctn + 1 == print_n:\n                break\n    return best_hp_vice\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.list_spose_model_performances","title":"list_spose_model_performances","text":"<pre><code>list_spose_model_performances(\n    session: str,\n    gender: str | None = None,\n    modality: str = \"behavioral\",\n) -&gt; DataFrame\n</code></pre> <p>List <code>SPoSE</code> model performances for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>gender</code> <code>str | None</code> <p>\"female\" or \"male\" OR None</p> <code>None</code> <code>modality</code> <code>str</code> <p>\"behavioral\" OR others.</p> <code>'behavioral'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dataframe with the model path, epoch, and validation accuracy sorted by accuracy</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def list_spose_model_performances(\n    session: str, gender: str | None = None, modality: str = \"behavioral\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    List `SPoSE` model performances for a given session.\n\n    :param session: '2D', OR '3D'\n    :param gender: \"female\" or \"male\" OR None\n    :param modality: \"behavioral\" OR others.\n    :return: dataframe with the model path, epoch, and validation accuracy sorted by accuracy\n    \"\"\"\n    # Check input\n    gender = check_gender_arg(gender)\n\n    spose_results_df = pd.DataFrame(columns=[\"model_path\", \"epoch\", \"val_acc\"])  # init results df\n    sub_folder = modality if gender is None else f\"{gender}/{modality}\"\n\n    # Find all results.json files\n    for results_path in Path(paths.results.main.spose, session, sub_folder).glob(\"**/results.json\"):\n        # Get results.json\n        with results_path.open() as f:\n            spose_result = f.read()\n\n        # Extract accuracy and epoch\n        val_acc_at_epoch = json.loads(spose_result)[\"val_acc\"]\n        result_epoch = json.loads(spose_result)[\"epoch\"]\n\n        # Add to results dataframe\n        new_row = pd.DataFrame(\n            [\n                {\n                    \"model_path\": str(results_path.parent).split(\"SPoSE/\")[-1],\n                    \"epoch\": result_epoch,\n                    \"val_acc\": val_acc_at_epoch,\n                }\n            ]\n        )\n        spose_results_df = pd.concat([spose_results_df, new_row], ignore_index=True)\n\n    # List by accuracy\n    return spose_results_df.sort_values(by=\"val_acc\", ascending=False).reset_index(drop=True)\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.list_vice_model_performances","title":"list_vice_model_performances","text":"<pre><code>list_vice_model_performances(\n    session: str,\n    gender: str | None = None,\n    modality: str = \"behavioral\",\n    hp_search: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>List <code>VICE</code> model performances for a given session.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>\"2D\", OR \"3D\"</p> required <code>gender</code> <code>str | None</code> <p>\"female\" or \"male\" OR None</p> <code>None</code> <code>modality</code> <code>str</code> <p>\"behavioral\" OR others. Note, hp-search results can be found with \"hp_20perc/behavioral\".</p> <code>'behavioral'</code> <code>hp_search</code> <code>bool</code> <p>True check the hyperparameter search results</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>dataframe with the model path, epoch, and validation accuracy sorted by accuracy</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def list_vice_model_performances(\n    session: str, gender: str | None = None, modality: str = \"behavioral\", hp_search: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    List `VICE` model performances for a given session.\n\n    :param session: \"2D\", OR \"3D\"\n    :param gender: \"female\" or \"male\" OR None\n    :param modality: \"behavioral\" OR others. Note, hp-search results can be found with \"hp_20perc/behavioral\".\n    :param hp_search: True check the hyperparameter search results\n    :return: dataframe with the model path, epoch, and validation accuracy sorted by accuracy\n    \"\"\"\n    # Check input\n    gender = check_gender_arg(gender)\n\n    vice_results_df = pd.DataFrame(columns=[\"model_path\", \"epoch\", \"val_acc\"])  # init results df\n    if hp_search and gender is not None:\n        cprint(string=\"gender should be None if searching for HP results.\", col=\"y\")\n    sub_folder = \"hp_20perc/\" if hp_search else \"\"\n    sub_folder += modality if gender is None else f\"{gender}/{modality}\"\n\n    # Find all results*.json files\n    for results_path in Path(paths.results.main.vice, session, sub_folder).glob(\"**/results*.json\"):\n        # Get results.json\n        with results_path.open() as f:\n            vice_result = f.read()\n\n        # Extract accuracy and epoch\n        val_acc_at_epoch = json.loads(vice_result)[\"val_acc\"]\n        result_epoch = json.loads(vice_result)[\"epoch\"]\n\n        # Check if the model is already in the dataframe (since each model has several results_*.json files)\n        model_path = str(results_path.parent).split(\"VICE/\")[-1]\n        if model_path in vice_results_df[\"model_path\"].values:  # noqa: PD011\n            if (\n                val_acc_at_epoch\n                &lt; vice_results_df.loc[vice_results_df[\"model_path\"] == model_path, \"val_acc\"].to_numpy()[0]\n            ):\n                continue  # skip if worse than previous\n\n            # Remove previous entry\n            vice_results_df = vice_results_df.drop(vice_results_df[vice_results_df[\"model_path\"] == model_path].index)\n\n        # Add to results dataframe\n        vice_results_df = pd.concat(\n            [\n                vice_results_df,\n                pd.DataFrame([{\"model_path\": model_path, \"epoch\": result_epoch, \"val_acc\": val_acc_at_epoch}]),\n            ],\n            ignore_index=True,\n        )\n\n    # List by accuracy\n    return vice_results_df.sort_values(by=\"val_acc\", ascending=False).reset_index(drop=True)\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.load_best_vice_weights","title":"load_best_vice_weights","text":"<pre><code>load_best_vice_weights(\n    session: str,\n    gender: str | None = None,\n    hp_search: bool = False,\n    pilot: bool = PILOT,\n    pruned: bool = True,\n    verbose: bool = False,\n) -&gt; tuple[ndarray, str]\n</code></pre> <p>Load the best <code>VICE</code> weights.</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def load_best_vice_weights(\n    session: str,\n    gender: str | None = None,\n    hp_search: bool = False,\n    pilot: bool = params.PILOT,\n    pruned: bool = True,\n    verbose: bool = False,\n) -&gt; tuple[np.ndarray, str]:\n    \"\"\"Load the best `VICE` weights.\"\"\"\n    # Get the best hyperparameters for VICE\n    best_hp_vice = get_best_hp_vice(hp_search=True, print_n=int(verbose))[session]\n    # take the best from the hp search\n\n    if not hp_search:\n        # We have to attach the hp_search percentage to the path if we want to load these weights\n        best_hp_vice.pop(\"hp_perc\")\n\n    path_to_vice_sim_mat = create_path_from_vice_params(params_dict=best_hp_vice, gender=gender, pilot=pilot)\n    # Cut Path at VICE/\n    param_path_vice = str(path_to_vice_sim_mat).split(f\"VICE/{session}/\")[-1]\n\n    vice_weights, _, p2_vice_weights = load_vice_weights(\n        session=session,\n        pilot=pilot,\n        pruned=pruned,\n        return_path=True,\n        param_path=param_path_vice,\n    )  # take only loc_params\n\n    return vice_weights, p2_vice_weights\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.load_spose_weights","title":"load_spose_weights","text":"<pre><code>load_spose_weights(\n    session: str,\n    gender: str | None = None,\n    pilot: bool = PILOT,\n    return_path: bool = False,\n    **hp_kwargs\n) -&gt; ndarray | tuple[ndarray, str]\n</code></pre> <p>Load weights for the <code>SPoSE</code> implementation by the <code>Vision and Computational Cognition Group</code> by Martin N. Hebart.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>gender</code> <code>str | None</code> <p>provide gender ('female', 'male') for model training on exclusive gender trials</p> <code>None</code> <code>pilot</code> <code>bool</code> <p>True: use pilot data</p> <code>PILOT</code> <code>return_path</code> <code>bool</code> <p>True: return path to the weights-file</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray | tuple[ndarray, str]</code> <p>sorted weights</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def load_spose_weights(\n    session: str, gender: str | None = None, pilot: bool = params.PILOT, return_path: bool = False, **hp_kwargs\n) -&gt; np.ndarray | tuple[np.ndarray, str]:\n    \"\"\"\n    Load weights for the `SPoSE` implementation by the `Vision and Computational Cognition Group` by Martin N. Hebart.\n\n    :param session: '2D', OR '3D'\n    :param gender: provide gender ('female', 'male') for model training on exclusive gender trials\n    :param pilot: True: use pilot data\n    :param return_path: True: return path to the weights-file\n    :return: sorted weights\n    \"\"\"\n    gender = check_gender_arg(gender=gender)\n\n    # Load sorted weights\n    fn = \"weights_sorted.npy\"\n    parent_dir = Path(paths.results.pilot.v2.spose if pilot else paths.results.main.spose, session.upper())\n\n    if len(hp_kwargs) == 0:\n        p2_weights = list(parent_dir.glob(f\"**/{fn}\"))\n\n    else:  # hp is not None:\n        # Check if hp is valid\n        if \"lambda\" not in hp_kwargs:\n            msg = \"Currently only hyperparameter 'lambda' can be processed in load_spose_weights()!\"\n            raise NotImplementedError(msg)\n\n        p2_weights = list(\n            Path(parent_dir).glob(str(Path(\"\".join([f\"**/*{value}*/**/\" for key, value in hp_kwargs.items()]), fn)))\n        )\n        if gender is None:\n            p2_weights = [p for p in p2_weights if \"male/\" not in str(p)]\n        else:\n            p2_weights = [p for p in p2_weights if f\"/{gender}\" in str(p)]\n\n    if len(p2_weights) == 1:\n        p2_weights = p2_weights.pop()\n\n    if isinstance(p2_weights, list):\n        cprint(string=\"Choose one weight file ...\", col=\"b\")\n        parent_dir = os.path.commonpath(p2_weights)  # find shared parent directory in the list of files\n        p2_weights = browse_files(initialdir=parent_dir, filetypes=\"*.npy\")\n\n    # Load weights\n    weights = np.load(file=p2_weights)\n\n    if return_path:\n        return weights, p2_weights\n    return weights\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.load_vice_similarity_matrix","title":"load_vice_similarity_matrix","text":"<pre><code>load_vice_similarity_matrix(\n    session: str,\n    gender: str | None = None,\n    hp_search: bool = False,\n    pilot: bool = PILOT,\n    verbose: bool = False,\n) -&gt; ndarray\n</code></pre> <p>Load the similarity matrix of the <code>VICE</code> model.</p> Note <p>Computing the matrix is rapid. So loading might not be necessary.</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def load_vice_similarity_matrix(\n    session: str, gender: str | None = None, hp_search: bool = False, pilot: bool = params.PILOT, verbose: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Load the similarity matrix of the `VICE` model.\n\n    ??? note\n        Computing the matrix is rapid. So loading might not be necessary.\n    \"\"\"\n    gender = check_gender_arg(gender=gender)\n\n    # Get the best hyperparameters for VICE\n    best_hp_vice = get_best_hp_vice(hp_search=True, print_n=int(verbose))[session]  # take the best from the hp search\n    if not hp_search:\n        # However, we remove the hyperparameter percentage, since we want the results from the main run\n        best_hp_vice.pop(\"hp_perc\")\n\n    # Generate the path to similarity matrix from the hyperparameter settings\n    path_to_vice_sim_mat = create_path_from_vice_params(params_dict=best_hp_vice, gender=gender, pilot=pilot)\n\n    try:\n        vice_sim_mat = load_obj(name=\"similarity_matrix\", folder=path_to_vice_sim_mat)\n    except FileNotFoundError:\n        cprint(string=\"VICE similarity matrix was not found. Computing it now ...\", col=\"y\")\n        vice_sim_mat = compute_vice_similarity_matrix(\n            session=session,\n            gender=gender,\n            hp_search=hp_search,\n            pilot=pilot,\n            save=True,\n            verbose=verbose,\n        )\n\n    return vice_sim_mat\n</code></pre>"},{"location":"reference/modeling/prep_computational_choice_model/#facesim3d.modeling.prep_computational_choice_model.load_vice_weights","title":"load_vice_weights","text":"<pre><code>load_vice_weights(\n    session: str,\n    pilot: bool = PILOT,\n    pruned: bool = True,\n    return_path: bool = False,\n    param_path: str | None = \"\",\n) -&gt; tuple[ndarray, ndarray, str] | tuple[ndarray, ndarray]\n</code></pre> <p>Load parameters for <code>VICE</code>.</p> <p>!!! note: \"How embedding matrices are sorted\"     \"pruned parameter matrices are sorted according to their overall importance.\"</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>pilot</code> <code>bool</code> <p>True: use pilot data (v2)</p> <code>PILOT</code> <code>pruned</code> <code>bool</code> <p>True: return the pruned parameters</p> <code>True</code> <code>return_path</code> <code>bool</code> <p>True: return the path to the parameter file</p> <code>False</code> <code>param_path</code> <code>str | None</code> <p>path to the weights-file, defined by the corresponding VICE params (after /[session]/..)</p> <code>''</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, str] | tuple[ndarray, ndarray]</code> <p>parameters (pruned parameters are sorted)</p> Source code in <code>code/facesim3d/modeling/prep_computational_choice_model.py</code> <pre><code>def load_vice_weights(\n    session: str,\n    pilot: bool = params.PILOT,\n    pruned: bool = True,\n    return_path: bool = False,\n    param_path: str | None = \"\",\n) -&gt; tuple[np.ndarray, np.ndarray, str] | tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Load parameters for `VICE`.\n\n    !!! note: \"How embedding matrices are sorted\"\n        \"pruned parameter matrices are sorted according to their overall importance.\"\n\n    :param session: '2D', OR '3D'\n    :param pilot: True: use pilot data (v2)\n    :param pruned: True: return the pruned parameters\n    :param return_path: True: return the path to the parameter file\n    :param param_path: path to the weights-file, defined by the corresponding VICE params (after /[session]/..)\n    :return: parameters (pruned parameters are sorted)\n    \"\"\"\n    # Load (sorted) parameters\n    parent_dir = Path(paths.results.pilot.v2.vice if pilot else paths.results.main.vice) / session.upper() / param_path\n    fn_param = \"pruned_params.npz\" if pruned else \"parameters.npz\"\n\n    p2_params = find(fname=fn_param, folder=str(parent_dir), typ=\"file\", abs_path=True, exclusive=False, verbose=False)\n\n    if isinstance(p2_params, list):\n        cprint(string=f\"Choose a parameter file ('{fn_param}') ...\", col=\"b\")\n        p2_params = browse_files(initialdir=parent_dir, filetypes=\"*.npz\")\n\n    vice_params = np.load(file=p2_params)\n\n    # Unpack the compressed parameter object\n    loc_params = vice_params[\"pruned_loc\" if pruned else \"loc\"]\n    scale_params = vice_params[\"pruned_scale\" if pruned else \"scale\"]\n\n    if return_path:\n        return loc_params, scale_params, p2_params\n    return loc_params, scale_params\n</code></pre>"},{"location":"reference/modeling/rsa/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> rsa","text":""},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa","title":"rsa","text":"<p>Compute RDMs of face similarity judgments and run RSA for the pilot study (version 2) &amp; the main experiment.</p> <p>Run the script with (see also <code>./results/main/rsa/run_rsa.sh</code>):</p> <pre><code>for metric in cosine euclidean\ndo\n    python -m facesim3d.modeling.rsa --metric ${metric} --save_corr --plot --save_plots --logger_overwrite -v\ndone\n</code></pre> Need information about input arguments? <p>See the help section: </p><pre><code>python -m facesim3d.modeling.rsa --help\n</code></pre> <p>Functions:</p> Name Description <code>aggregate_judgments_in_session</code> <p>Aggregate similarity judgments across participants in the given session (2D, 3D).</p> <code>aggregate_judgments_in_session_by_gender</code> <p>Aggregate similarity judgments across participants in the given session (2D, 3D).</p> <code>check_gender</code> <p>Check whether the <code>gender</code> argument is valid.</p> <code>compute_flame_feature_similarity_matrix</code> <p>Compute a similarity matrix from (e.g., shape) parameters of the <code>FLAME</code>-fitted heads.</p> <code>compute_physical_attr_similarity_matrix</code> <p>Compute a similarity matrix from physical face attributes (<code>PFA</code>).</p> <code>compute_similarity_matrix_from_human_judgments</code> <p>Compute a behavioral face similarity matrix (<code>BSM</code>) from the given a trial results table.</p> <code>compute_similarity_matrix_from_vgg_face_human_judgment_model</code> <p>Compute a face similarity matrix from decisions of the <code>VGGFaceHumanjudgment[FrozenCore]</code> model.</p> <code>compute_spose_feature_map_similarity_matrix</code> <p>Compute a similarity matrix from <code>SPoSE</code> feature maps (embedding matrix).</p> <code>compute_vgg_feature_map_similarity_matrix</code> <p>Compute a similarity matrix from <code>VGGFace</code> feature maps.</p> <code>compute_vgg_human_judgment_feature_map_similarity_matrix</code> <p>Compute a similarity matrix from feature maps of the <code>vgg_core_bridge</code> layer in <code>VGGFaceHumanjudgment[FrozenCore]</code>.</p> <code>compute_vice_feature_map_similarity_matrix</code> <p>Compute a similarity matrix from <code>VICE</code> feature maps (embedding matrix).</p> <code>extract_exclusive_gender_trials</code> <p>Extract triplets that contain only the faces of one gender.</p> <code>extract_set_of_heads</code> <p>Extract the set of heads from a trial-results table.</p> <code>get_corr_df_rsm</code> <p>Get the correlation dataframe for representational similarity matrices (<code>RSM</code>).</p> <code>get_model_hps</code> <p>Get model hyperparameters.</p> <code>main</code> <p>Run the main function of the <code>rsa.py</code> script.</p> <code>plot_rsa_corr_df</code> <p>Plot the RSA correlation dataframe.</p> <code>plot_vgg_correlations</code> <p>Plot correlations between similarity matrices.</p> <code>similarity_judgments_of_single_participant</code> <p>Compute a face similarity matrix from a single participant's behavioral data.</p> <code>vectorize_similarity_matrix</code> <p>Take the upper triangle of a given similarity matrix and return it as vector.</p> <code>visualise_matrix</code> <p>Visualize face similarity judgments.</p>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.aggregate_judgments_in_session","title":"aggregate_judgments_in_session","text":"<pre><code>aggregate_judgments_in_session(\n    session: str,\n    pilot: bool = PILOT,\n    recalculate: bool = False,\n    verbose: bool = False,\n) -&gt; ndarray\n</code></pre> <p>Aggregate similarity judgments across participants in the given session (2D, 3D).</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>pilot</code> <code>bool</code> <p>True: use pilot data</p> <code>PILOT</code> <code>recalculate</code> <code>bool</code> <p>do not use cached table, recalculate similarity judgments instead</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>verbose or not</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>matrix with normalized judgments</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def aggregate_judgments_in_session(\n    session: str, pilot: bool = params.PILOT, recalculate: bool = False, verbose: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Aggregate similarity judgments across participants in the given session (2D, 3D).\n\n    :param session: '2D', OR '3D'\n    :param pilot: True: use pilot data\n    :param recalculate: do not use cached table, recalculate similarity judgments instead\n    :param verbose: verbose or not\n    :return: matrix with normalized judgments\n    \"\"\"\n    # Define path to cached similarity judgments matrix\n    path_to_cached_matrix = Path(\n        paths.results.pilot.v2.rsa if pilot else paths.results.main.rsa,\n        f\"cached_{session}_similarity_judgments_matrix.npy\",\n    )\n\n    # Check, if cached similarity judgments matrix exists, and if it should be recalculated\n    load_cached_matrix: bool = path_to_cached_matrix.is_file() and not recalculate\n\n    if load_cached_matrix:\n        if verbose:\n            cprint(string=f\"Loading cached similarity judgments matrix from {path_to_cached_matrix} ... \", col=\"b\")\n        sim_mat: np.ndarray = np.load(path_to_cached_matrix)\n\n    else:\n        if pilot:\n            tab = read_pilot_data(clean_trials=True, verbose=verbose)\n        else:\n            tab = read_trial_results_of_session(\n                session=session, clean_trials=True, drop_subsamples=True, verbose=verbose\n            )\n\n        sim_mat: np.ndarray = compute_similarity_matrix_from_human_judgments(\n            trial_results_table=tab, pilot=pilot, split_return=False\n        )\n\n        if verbose:\n            cprint(string=f\"Saving similarity judgments matrix to {path_to_cached_matrix} ... \", col=\"b\")\n        np.save(path_to_cached_matrix, sim_mat)\n\n    return sim_mat\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.aggregate_judgments_in_session_by_gender","title":"aggregate_judgments_in_session_by_gender  <code>cached</code>","text":"<pre><code>aggregate_judgments_in_session_by_gender(\n    session: str,\n    gender: str,\n    pilot: bool = PILOT,\n    recalculate: bool = False,\n    verbose: bool = False,\n) -&gt; ndarray\n</code></pre> <p>Aggregate similarity judgments across participants in the given session (2D, 3D).</p> <p>Take only trials of triplets that exclusively contain the faces of the given gender.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>gender</code> <code>str</code> <p>'female', OR 'male'</p> required <code>pilot</code> <code>bool</code> <p>True: use pilot data</p> <code>PILOT</code> <code>recalculate</code> <code>bool</code> <p>do not use cached table, recalculate similarity judgments instead</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>verbose or not</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>matrix with normalized judgments</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>@lru_cache\ndef aggregate_judgments_in_session_by_gender(\n    session: str, gender: str, pilot: bool = params.PILOT, recalculate: bool = False, verbose: bool = False\n) -&gt; np.ndarray:\n    \"\"\"\n    Aggregate similarity judgments across participants in the given session (2D, 3D).\n\n    Take only trials of triplets that exclusively contain the faces of the given gender.\n\n    :param session: '2D', OR '3D'\n    :param gender: 'female', OR 'male'\n    :param pilot: True: use pilot data\n    :param recalculate: do not use cached table, recalculate similarity judgments instead\n    :param verbose: verbose or not\n    :return: matrix with normalized judgments\n    \"\"\"\n    gender = check_gender(gender=gender)\n\n    # Define path to cached similarity judgments matrix\n    path_to_cached_matrix = Path(\n        paths.results.pilot.v2.rsa if pilot else paths.results.main.rsa,\n        f\"cached_{session}_{gender}_similarity_judgments_matrix.npy\",\n    )\n\n    # Check, if cached similarity judgments matrix exists, and if it should be recalculated\n    load_cached_matrix: bool = path_to_cached_matrix.is_file() and not recalculate\n\n    if load_cached_matrix:\n        if verbose:\n            cprint(string=f\"Loading cached similarity judgments matrix from {path_to_cached_matrix} ... \", col=\"b\")\n        sim_mat: np.ndarray = np.load(path_to_cached_matrix)\n    else:\n        if pilot:\n            tab = read_pilot_data(clean_trials=True, verbose=verbose)\n            msg = \"Not implemented for pilot data.\"\n            raise NotImplementedError(msg)\n        else:  # noqa: RET506\n            tab = read_trial_results_of_session(\n                session=session, clean_trials=True, drop_subsamples=True, verbose=verbose\n            )\n            tab = extract_exclusive_gender_trials(trial_results_table=tab, gender=gender, verbose=verbose)\n        sim_mat = compute_similarity_matrix_from_human_judgments(\n            trial_results_table=tab, pilot=pilot, split_return=False\n        )\n\n        # Reduce matrix to gender of interest\n        n_gender = (12 if gender == \"female\" else 13) if pilot else (params.main.n_faces // 2)\n        sim_mat = sim_mat[:n_gender, :n_gender] if gender == \"female\" else sim_mat[n_gender:, n_gender:]  # or \"male\"\n\n        if verbose:\n            cprint(string=f\"Saving similarity judgments matrix to {path_to_cached_matrix} ... \", col=\"b\")\n        np.save(path_to_cached_matrix, sim_mat)\n\n    return sim_mat\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.check_gender","title":"check_gender","text":"<pre><code>check_gender(gender: str) -&gt; str\n</code></pre> <p>Check whether the <code>gender</code> argument is valid.</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def check_gender(gender: str) -&gt; str:\n    \"\"\"Check whether the `gender` argument is valid.\"\"\"\n    gender = gender.lower()\n    if gender not in params.GENDERS:\n        msg = f\"gender must be in {params.GENDERS}\"\n        raise ValueError(msg)\n    return gender\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.compute_flame_feature_similarity_matrix","title":"compute_flame_feature_similarity_matrix","text":"<pre><code>compute_flame_feature_similarity_matrix(\n    pca: bool | float = False,\n    param: str = \"shape\",\n    model: str = \"deca\",\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n) -&gt; NDArray[float64]\n</code></pre> <p>Compute a similarity matrix from (e.g., shape) parameters of the <code>FLAME</code>-fitted heads.</p> <p>Parameters:</p> Name Type Description Default <code>pca</code> <code>bool | float</code> <p>False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that pca [float] *100 % of variance is explained</p> <code>False</code> <code>param</code> <code>str</code> <p>which parameter to use (e.g., 'shape', 'exp' [expression], 'pose', ...)</p> <code>'shape'</code> <code>model</code> <code>str</code> <p>which model to use: 'deca' OR 'flame'</p> <code>'deca'</code> <code>gender</code> <code>str | None</code> <p>for exclusively within-gender feature comparison</p> <code>None</code> <code>pilot_version</code> <code>int | None</code> <p>None for main experiment,OR pilot 1, OR 2.</p> <code>None</code> <code>metric</code> <code>str</code> <p>similarity metric to use (cosine, Euclidean)</p> <code>'cosine'</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>similarity matrix based on shape parameters of the FLAME-fitted heads</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def compute_flame_feature_similarity_matrix(\n    pca: bool | float = False,\n    param: str = \"shape\",\n    model: str = \"deca\",\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n) -&gt; npt.NDArray[np.float64]:\n    \"\"\"\n    Compute a similarity matrix from (e.g., shape) parameters of the `FLAME`-fitted heads.\n\n    :param pca: False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that\n                pca [float] *100 % of variance is explained\n    :param param: which parameter to use (e.g., 'shape', 'exp' [expression], 'pose', ...)\n    :param model: which model to use: 'deca' OR 'flame'\n    :param gender: for exclusively within-gender feature comparison\n    :param pilot_version: None for main experiment,OR pilot 1, OR 2.\n    :param metric: similarity metric to use (cosine, Euclidean)\n    :return: similarity matrix based on shape parameters of the FLAME-fitted heads\n    \"\"\"\n    # Get head mapping table\n    head_map = heads_naming_converter_table(pilot_version=pilot_version)\n\n    # Filter for gender if requested\n    if gender is not None:\n        gender = check_gender(gender=gender)\n        head_map = head_map[head_map.Model.str.contains(\"WF\" if gender == \"female\" else \"WM\")]\n\n    # Get table with FLAME shape parameters for all heads\n    feat_tab = get_flame_params(list_of_head_nrs=head_map.head_nr, param=param, model=model)\n\n    return compute_feature_similarity_matrix(feature_table=feat_tab, pca=pca, metric=metric, z_score=True)\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.compute_physical_attr_similarity_matrix","title":"compute_physical_attr_similarity_matrix","text":"<pre><code>compute_physical_attr_similarity_matrix(\n    pca: bool | float = False,\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n) -&gt; NDArray[float64]\n</code></pre> <p>Compute a similarity matrix from physical face attributes (<code>PFA</code>).</p> <p>Expected performance of the <code>PFA</code> model</p> <p>Jozwik et al. (2022) report \"poor performance of configural models\", which are similar to <code>PFA</code> here, however, good performance of the active appearance model (<code>AAM</code>) which is based on similar features.</p> <p>Parameters:</p> Name Type Description Default <code>pca</code> <code>bool | float</code> <p>False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that pca [float] *100 % of variance is explained</p> <code>False</code> <code>gender</code> <code>str | None</code> <p>for within-gender feature comparison</p> <code>None</code> <code>pilot_version</code> <code>int | None</code> <p>None for the main experiment, OR pilot 1, OR 2.</p> <code>None</code> <code>metric</code> <code>str</code> <p>similarity metric to use (cosine, Euclidean)</p> <code>'cosine'</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>similarity matrix of physical face attributes</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def compute_physical_attr_similarity_matrix(\n    pca: bool | float = False,\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n) -&gt; npt.NDArray[np.float64]:\n    \"\"\"\n    Compute a similarity matrix from physical face attributes (`PFA`).\n\n    !!! note \"Expected performance of the `PFA` model\"\n        Jozwik et al. (2022) report \"poor performance of configural models\", which are similar to `PFA` here,\n        however, good performance of the active appearance model (`AAM`) which is based on similar features.\n\n    :param pca: False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that\n                pca [float] *100 % of variance is explained\n    :param gender: for within-gender feature comparison\n    :param pilot_version: None for the main experiment, OR pilot 1, OR 2.\n    :param metric: similarity metric to use (cosine, Euclidean)\n    :return: similarity matrix of physical face attributes\n\n    \"\"\"\n    # Get head mapping table\n    head_map = heads_naming_converter_table(pilot_version=pilot_version)\n    list_of_models = head_map.Model\n\n    # Filter for gender if requested\n    if gender is not None:\n        gender = check_gender(gender)\n        list_of_models = head_map.Model[head_map.Model.str.contains(\"WF\" if \"female\" in gender else \"WM\")]\n\n    # Get table with physical attributes/features\n    feat_tab = get_cfd_features_for_models(list_of_models=list_of_models, physical_attr_only=True)\n\n    return compute_feature_similarity_matrix(feature_table=feat_tab, pca=pca, metric=metric, z_score=True)\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.compute_similarity_matrix_from_human_judgments","title":"compute_similarity_matrix_from_human_judgments","text":"<pre><code>compute_similarity_matrix_from_human_judgments(\n    trial_results_table: DataFrame,\n    pilot: bool = PILOT,\n    split_return: bool = False,\n    n_faces: int | None = None,\n    multi_triplet_mode: str = \"majority\",\n    verbose: bool = True,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Compute a behavioral face similarity matrix (<code>BSM</code>) from the given a trial results table.</p> <p>Parameters:</p> Name Type Description Default <code>trial_results_table</code> <code>DataFrame</code> <p>table with trial results</p> required <code>pilot</code> <code>bool</code> <p>True: use the pilot-data</p> <code>PILOT</code> <code>split_return</code> <code>bool</code> <p>split the return in judgments and counts for aggregation across the participants</p> <code>False</code> <code>n_faces</code> <code>int | None</code> <p>number of faces in the experiment</p> <code>None</code> <code>multi_triplet_mode</code> <code>str</code> <p>how to handle trials with multiple samples</p> <code>'majority'</code> <code>verbose</code> <code>bool</code> <p>be verbose</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray | tuple[ndarray, ndarray]</code> <p>either aggregated matrix of similarity judgments OR split in judgments and counts</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def compute_similarity_matrix_from_human_judgments(\n    trial_results_table: pd.DataFrame,\n    pilot: bool = params.PILOT,\n    split_return: bool = False,\n    n_faces: int | None = None,\n    multi_triplet_mode: str = \"majority\",\n    verbose: bool = True,\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute a behavioral face similarity matrix (`BSM`) from the given a trial results table.\n\n    :param trial_results_table: table with trial results\n    :param pilot: True: use the pilot-data\n    :param split_return: split the return in judgments and counts for aggregation across the participants\n    :param n_faces: number of faces in the experiment\n    :param multi_triplet_mode: how to handle trials with multiple samples\n    :param verbose: be verbose\n    :return: either aggregated matrix of similarity judgments OR split in judgments and counts\n    \"\"\"\n    # Remove training trials\n    trial_results_table = trial_results_table.sort_values(\n        by=list(set(trial_results_table.columns).intersection([\"ppid_session_dataname\", \"trial_num\"]))\n    ).reset_index(drop=True)\n\n    # For triplet-IDs with multiple samples, we should take the majority vote or sample randomly\n    tr_tab_tripl_val_ctn = trial_results_table.triplet_id.value_counts()\n    if tr_tab_tripl_val_ctn.nunique() &gt; 1:  # noqa: PD101\n        multi_triplet_mode = multi_triplet_mode.lower()\n        if multi_triplet_mode not in {\"majority\", \"random\", \"ignore\"}:\n            msg = \"multi_triplet_mode must be 'majority' OR 'random'!\"\n            raise ValueError(msg)\n\n        min_n_samples = tr_tab_tripl_val_ctn.min()\n        if multi_triplet_mode == \"majority\":\n            # Per triplet ID keep only the majority vote\n            for t_id in tqdm(\n                tr_tab_tripl_val_ctn[tr_tab_tripl_val_ctn &gt; min_n_samples].index,\n                desc=\"Clean table from multiple samples of triplet IDs via majority vote.\",\n                total=(tr_tab_tripl_val_ctn &gt; min_n_samples).sum(),\n            ):\n                # Head odd counts\n                head_odd_counts = trial_results_table[trial_results_table.triplet_id == t_id].head_odd.value_counts()\n\n                # Find head which was chosen most often\n                head_odd_majority = head_odd_counts[head_odd_counts == head_odd_counts.max()].sample(1).index[0]\n                # we sample here, since there might be multiple odd heads with the same count (i.e., max)\n\n                # Find indices of the current triplet-ID and the majority head\n                indices_to_kick = list(\n                    trial_results_table.loc[\n                        (\n                            (trial_results_table.triplet_id == t_id)\n                            &amp; (trial_results_table.head_odd == head_odd_majority)\n                        )\n                    ].index\n                )\n                # Kick one random index to keep it in the table\n                np.random.shuffle(indices_to_kick)\n                indices_to_kick.pop()\n                # Add the indices of the other heads to kick\n                indices_to_kick += list(\n                    trial_results_table.loc[\n                        (\n                            (trial_results_table.triplet_id == t_id)\n                            &amp; (trial_results_table.head_odd != head_odd_majority)\n                        )\n                    ].index\n                )\n\n                trial_results_table = trial_results_table.drop(index=indices_to_kick)\n\n        elif multi_triplet_mode == \"random\":\n            trial_results_table = trial_results_table.groupby(\"triplet_id\").sample(n=min_n_samples)\n\n        else:  # multi_triplet_mode == \"ignore\"\n            val_ctn_more_than_min = tr_tab_tripl_val_ctn[tr_tab_tripl_val_ctn &gt; min_n_samples]\n            if verbose:\n                cprint(\n                    string=f\"Ignoring {sum(val_ctn_more_than_min)} multiple samples of \"\n                    f\"{len(val_ctn_more_than_min)} triplet IDs!\",\n                    col=\"y\",\n                )\n\n    if n_faces is None:\n        n_faces = params.pilot.v2.n_faces if pilot else params.main.n_faces  # 25 or 100\n    list_of_heads_in_table = extract_set_of_heads(trial_results_table)\n\n    prev_indexing = False\n    if len(list_of_heads_in_table) != n_faces:\n        cprint(\n            string=f\"Not all {n_faces} faces are present in the trial data.\\n\"\n            f\"Consider passing n_faces={len(list_of_heads_in_table)} as kwarg!\",\n            col=\"r\",\n        )\n        prev_indexing = True\n\n    face_sim_mat = np.identity(n_faces)\n    face_ctn_mat = np.zeros(shape=(n_faces, n_faces))\n\n    # Extract data\n    # rt = tab[\"response_time\"]  # keep for potential later usage ...  # noqa: ERA001\n    judge = trial_results_table[[\"head1\", \"head2\", \"head3\", \"head_odd\"]]\n\n    nan_trials = 0\n    miss_trials = 0\n    for _row_i, (h1, h2, h3, odd) in judge.iterrows():  # row_i is not used here\n        if np.isnan((h1, h2, h3, odd)).any():\n            nan_trials += 1\n            continue\n\n        if odd == 0:\n            miss_trials += 1\n            continue\n\n        # Determine indices of face-pairs in the similarity matrix\n        for heads_combi in itertools.combinations((h1, h2, h3), r=2):\n            h_i, h_ii = heads_combi  # save which heads are in combo\n\n            if prev_indexing:\n                indices = np.array(heads_combi).astype(int)\n\n                if pilot:\n                    # In pilot (v2) female faces have head number 1 to 12, and male faces 51 to 63,\n                    # we want to map this to the indices female: 0-11, male: 12-24\n                    for i, fidx in enumerate(indices):\n                        indices[i] = head_nr_to_pilot_matrix_index(head_id=fidx, pilot_version=2)\n                else:  # main experiment\n                    indices -= 1\n\n                indices = tuple(indices)\n            else:\n                indices = list_of_heads_in_table.index(h_i), list_of_heads_in_table.index(h_ii)  # tuple\n\n            # Count comparisons\n            face_ctn_mat[indices] += 1\n            face_ctn_mat[indices[::-1]] += 1  # fill symmetrically\n\n            # Fill judgments\n            similar = int(odd not in {h_i, h_ii})\n            face_sim_mat[indices] += similar\n            face_sim_mat[indices[::-1]] += similar  # fill symmetrically\n\n    if split_return:\n        # For aggregation across participants\n        return face_sim_mat, face_ctn_mat\n\n    # Average across trials\n    face_ctn_mat[np.where(face_ctn_mat == 0)] = np.nan\n    return face_sim_mat / face_ctn_mat\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.compute_similarity_matrix_from_vgg_face_human_judgment_model","title":"compute_similarity_matrix_from_vgg_face_human_judgment_model","text":"<pre><code>compute_similarity_matrix_from_vgg_face_human_judgment_model(\n    session: str,\n    model_name: str | None = None,\n    split_return: bool = False,\n    n_faces: int | None = None,\n    exclusive_gender_trials: str | None = None,\n    verbose: bool = True,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Compute a face similarity matrix from decisions of the <code>VGGFaceHumanjudgment[FrozenCore]</code> model.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>model_name</code> <code>str | None</code> <p>name of the model to use</p> <code>None</code> <code>split_return</code> <code>bool</code> <p>split the return in the decisions, and the count</p> <code>False</code> <code>n_faces</code> <code>int | None</code> <p>the number of faces in the experiment</p> <code>None</code> <code>exclusive_gender_trials</code> <code>str | None</code> <p>use exclusive gender trials ['female' OR 'male'], OR None for all samples.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>be verbose</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray | tuple[ndarray, ndarray]</code> <p>either matrix of similarity decisions OR split in decisions and counts</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def compute_similarity_matrix_from_vgg_face_human_judgment_model(\n    session: str,\n    model_name: str | None = None,\n    split_return: bool = False,\n    n_faces: int | None = None,\n    exclusive_gender_trials: str | None = None,\n    verbose: bool = True,\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute a face similarity matrix from decisions of the `VGGFaceHumanjudgment[FrozenCore]` model.\n\n    :param session: '2D', OR '3D'\n    :param model_name: name of the model to use\n    :param split_return: split the return in the decisions, and the count\n    :param n_faces: the number of faces in the experiment\n    :param exclusive_gender_trials: use exclusive gender trials ['female' OR 'male'], OR None for all samples.\n    :param verbose: be verbose\n    :return: either matrix of similarity decisions OR split in decisions and counts\n    \"\"\"\n    # Check input\n    session = session.upper()\n    if session not in params.SESSIONS:\n        msg = f\"session must be in {params.SESSIONS}\"\n        raise ValueError(msg)\n\n    # Get model information\n    vgg_hj_perform_info = get_model_hps(\n        session=session, model_name=model_name, exclusive_gender_trials=exclusive_gender_trials\n    )\n    vgg_hj_model_name = vgg_hj_perform_info.model_name\n    vgg_hj_data_mode = vgg_hj_perform_info.data_mode\n\n    # Set path to decision table\n    g_sfx = \"\" if exclusive_gender_trials is None else f\"_{exclusive_gender_trials.lower()}_only\"\n\n    p2_model_decisions = Path(paths.results.main.VGG, session, f\"{vgg_hj_model_name}{g_sfx}_decisions\").with_suffix(\n        \".csv\"\n    )\n\n    # Get / compute decision table for all participants\n    if p2_model_decisions.exists():\n        trial_results_table = pd.read_csv(p2_model_decisions)\n\n    else:\n        import torch\n\n        # Get model\n        vgg_hj_model = load_trained_vgg_face_human_judgment_model(\n            session=session,\n            model_name=vgg_hj_model_name,\n            exclusive_gender_trials=exclusive_gender_trials,\n            device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n        )\n        vgg_hj_model.eval()\n\n        # Get model data\n        full_dataset_dl, _, _ = prepare_data_for_human_judgment_model(\n            session=session,\n            frozen_core=vgg_hj_model.freeze_vgg_core,\n            data_mode=vgg_hj_data_mode,\n            last_core_layer=vgg_hj_model.last_core_layer,\n            split_ratio=(1.0, 0.0, 0.0),  # push all data in one set\n            batch_size=1,\n            num_workers=1,\n            dtype=torch.float32,\n            exclusive_gender_trials=exclusive_gender_trials,\n        )\n\n        # Init table\n        trial_results_table = pd.DataFrame(\n            columns=[\"head1\", \"head2\", \"head3\", \"head_odd_human_choice\", \"head_odd_model_choice\"]\n        )\n\n        # Fill table with model decisions\n        with torch.no_grad():\n            for i, model_input in tqdm(\n                enumerate(full_dataset_dl),\n                desc=f\"Get decisions of '{vgg_hj_model_name}'\",\n                total=len(full_dataset_dl),\n                colour=\"#57965D\",\n            ):\n                ipt1, ipt2, ipt3, _, idx = model_input.values()  # _ == choice\n                i_decision = vgg_hj_model(ipt1, ipt2, ipt3).argmax().item()\n\n                trial_results_table.loc[i, :] = (\n                    full_dataset_dl.dataset.dataset.session_data.iloc[idx.item()].to_list()  # noqa: RUF005\n                    + [None]\n                )\n                trial_results_table.loc[i, \"head_odd_model_choice\"] = trial_results_table.loc[\n                    i, [\"head1\", \"head2\", \"head3\"]\n                ][i_decision]\n\n        # Save decision_table\n        p2_model_decisions.parent.mkdir(parents=True, exist_ok=True)\n        trial_results_table.to_csv(p2_model_decisions, index=False)\n\n    if verbose:\n        perc_match = (trial_results_table.head_odd_human_choice == trial_results_table.head_odd_model_choice).mean()\n        cprint(string=f\"\\n\\t{perc_match:.2%} of '{vgg_hj_model_name}' decisions match human judgements.\\n\", col=\"g\")\n\n    # Determine the number of faces\n    if n_faces is None:\n        n_faces = params.main.n_faces\n    list_of_heads_in_table = extract_set_of_heads(trial_results_table)\n\n    prev_indexing = False\n    if len(list_of_heads_in_table) != n_faces:\n        cprint(\n            string=f\"Not all {n_faces} faces are present in the trial data.\\n\"\n            f\"Consider passing n_faces={len(list_of_heads_in_table)} as kwarg!\",\n            col=\"r\",\n        )\n        prev_indexing = True\n\n    # Init similarity matrix\n    face_sim_mat = np.identity(n_faces)\n    face_ctn_mat = np.zeros(shape=(n_faces, n_faces))\n\n    # Extract data\n    judge = trial_results_table[[\"head1\", \"head2\", \"head3\", \"head_odd_model_choice\"]]\n    for _, (h1, h2, h3, odd) in tqdm(\n        judge.iterrows(),\n        desc=f\"Compute similarity matrix from decisions of '{vgg_hj_model_name}'\",\n        total=len(judge),\n        colour=\"#F98382\",\n    ):\n        # Determine indices of face-pairs in the similarity matrix\n        for heads_combi in itertools.combinations((h1, h2, h3), r=2):\n            h_i, h_ii = heads_combi  # save which heads are in combo\n\n            if prev_indexing:\n                indices = np.array(heads_combi).astype(int)\n                indices -= 1  # 1 == judge.min().min()\n                indices = tuple(indices)\n            else:\n                indices = list_of_heads_in_table.index(h_i), list_of_heads_in_table.index(h_ii)  # tuple\n\n            # Count comparisons\n            face_ctn_mat[indices] += 1\n            face_ctn_mat[indices[::-1]] += 1  # fill symmetrically\n\n            # Fill judgments\n            similar = int(odd not in {h_i, h_ii})\n            face_sim_mat[indices] += similar\n            face_sim_mat[indices[::-1]] += similar  # fill symmetrically\n\n    if split_return:\n        # For aggregation across participants\n        return face_sim_mat, face_ctn_mat\n\n    # Average across trials\n    face_ctn_mat[np.where(face_ctn_mat == 0)] = np.nan\n    return face_sim_mat / face_ctn_mat\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.compute_spose_feature_map_similarity_matrix","title":"compute_spose_feature_map_similarity_matrix","text":"<pre><code>compute_spose_feature_map_similarity_matrix(\n    session: str,\n    pca: bool | float = False,\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n) -&gt; NDArray[float64]\n</code></pre> <p>Compute a similarity matrix from <code>SPoSE</code> feature maps (embedding matrix).</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>pca</code> <code>bool | float</code> <p>False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that pca [float] *100 % of variance is explained</p> <code>False</code> <code>gender</code> <code>str | None</code> <p>for exclusively within-gender feature comparison</p> <code>None</code> <code>pilot_version</code> <code>int | None</code> <p>None for the main experiment, OR pilot 1, OR 2.</p> <code>None</code> <code>metric</code> <code>str</code> <p>similarity metric to use (cosine, Euclidean)</p> <code>'cosine'</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>similarity matrix of SPoSE feature maps</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def compute_spose_feature_map_similarity_matrix(\n    session: str,\n    pca: bool | float = False,\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n) -&gt; npt.NDArray[np.float64]:\n    \"\"\"\n    Compute a similarity matrix from `SPoSE` feature maps (embedding matrix).\n\n    :param session: '2D', OR '3D'\n    :param pca: False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that\n                pca [float] *100 % of variance is explained\n    :param gender: for exclusively within-gender feature comparison\n    :param pilot_version: None for the main experiment, OR pilot 1, OR 2.\n    :param metric: similarity metric to use (cosine, Euclidean)\n    :return: similarity matrix of SPoSE feature maps\n    \"\"\"\n    if pilot_version == 1:\n        msg = \"SPoSE feature maps are not available for pilot 1.\"\n        raise ValueError(msg)\n\n    # Get head mapping table\n    head_map = heads_naming_converter_table(pilot_version=pilot_version)\n    list_of_models = head_map.Model\n\n    # Filter for gender if requested\n    if gender is not None:\n        # Prepare gender-only features if requested\n        gender = check_gender(gender)\n        list_of_models = head_map.Model[head_map.Model.str.contains(\"WF\" if \"female\" in gender else \"WM\")]\n\n    # Get the weights of the best hyperparameters for SPoSE\n    spose_weights = load_spose_weights(\n        session=session,\n        gender=gender,\n        pilot=pilot_version is not None,\n        return_path=False,\n        **BEST_HP_SPOSE[session],\n    )\n\n    feat_tab = pd.DataFrame(index=list_of_models, columns=[f\"D{i:03d}\" for i in range(spose_weights.shape[1])])\n    feat_tab.loc[:, :] = spose_weights\n\n    # with z_score=False: == compute_spose_similarity_matrix\n    return compute_feature_similarity_matrix(feature_table=feat_tab, pca=pca, metric=metric, z_score=False)\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.compute_vgg_feature_map_similarity_matrix","title":"compute_vgg_feature_map_similarity_matrix","text":"<pre><code>compute_vgg_feature_map_similarity_matrix(\n    layer_name: str,\n    pca: bool | float = False,\n    data_mode: str = \"3d-reconstructions\",\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n    extract_feat_maps: bool = False,\n) -&gt; NDArray[float64]\n</code></pre> <p>Compute a similarity matrix from <code>VGGFace</code> feature maps.</p> <p>To extend computational efficiency</p> <p>Intermediate results are saved to disk, such that they do not have to be recomputed each time (time-consuming).</p> <p>Parameters:</p> Name Type Description Default <code>pca</code> <code>bool | float</code> <p>False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that pca [float] *100 % of variance is explained</p> <code>False</code> <code>layer_name</code> <code>str</code> <p>name of the VGG layer to use</p> required <code>data_mode</code> <code>str</code> <p>the path to the \"2d-original\", \"3d-reconstructions\", or \"3d-perspectives\"</p> <code>'3d-reconstructions'</code> <code>gender</code> <code>str | None</code> <p>define gender if requested</p> <code>None</code> <code>pilot_version</code> <code>int | None</code> <p>None for main experiment, OR pilot 1, OR 2</p> <code>None</code> <code>metric</code> <code>str</code> <p>similarity metric to use (\"cosine\", \"euclidean\")</p> <code>'cosine'</code> <code>extract_feat_maps</code> <code>bool</code> <p>whether to extract feature maps from VGGFace</p> <code>False</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>similarity matrix based on VGGFace feature maps</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def compute_vgg_feature_map_similarity_matrix(\n    layer_name: str,\n    pca: bool | float = False,\n    data_mode: str = \"3d-reconstructions\",\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n    extract_feat_maps: bool = False,\n) -&gt; npt.NDArray[np.float64]:\n    \"\"\"\n    Compute a similarity matrix from `VGGFace` feature maps.\n\n    !!! note \"To extend computational efficiency\"\n        Intermediate results are saved to disk, such that they do not have to be recomputed each time (time-consuming).\n\n    :param pca: False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that\n                pca [float] *100 % of variance is explained\n    :param layer_name: name of the VGG layer to use\n    :param data_mode: the path to the \"2d-original\", \"3d-reconstructions\", or \"3d-perspectives\"\n    :param gender: define gender if requested\n    :param pilot_version: None for main experiment, OR pilot 1, OR 2\n    :param metric: similarity metric to use (\"cosine\", \"euclidean\")\n    :param extract_feat_maps: whether to extract feature maps from VGGFace\n    :return: similarity matrix based on VGGFace feature maps\n    \"\"\"\n    # Get head mapping table\n    head_map = heads_naming_converter_table(pilot_version=pilot_version)\n\n    # Prepare gender-only features if requested\n    gender_suffix: str = \"\"\n    if gender is not None:\n        gender = check_gender(gender=gender)\n        gender_suffix = f\"_{gender}_only\"\n\n    # Set path to feature matrix\n    data_mode = data_mode.lower()\n    data_mode_suffix = \"original\" if \"orig\" in data_mode else \"3D-recon\" if \"3d-recon\" in data_mode else \"3D-persp\"\n    p2_feat_mat = Path(\n        paths.results.heads.vggface,\n        f\"VGGface_feature_maps_{data_mode_suffix}_{f'PCA-{pca:.2f}_' if pca else ''}{layer_name}.pd.pickle\",\n    )\n\n    p2_feat_sim_mat = Path(str(p2_feat_mat).replace(\".pd.pickle\", f\"_{metric}-similarity-matrix{gender_suffix}.npy\"))\n    p2_feat_mat.parent.mkdir(parents=True, exist_ok=True)  # create directory if not exists\n\n    # Get the table with VGG-Face activations maps of all layers elicited by each head\n    if (p2_feat_sim_mat.is_file() and extract_feat_maps) or not p2_feat_sim_mat.is_file():\n        # optional when similarity matrix is already computed\n        if p2_feat_mat.is_file():\n            feat_tab = pd.read_pickle(p2_feat_mat)\n        else:\n            feat_tab = get_vgg_activation_maps(\n                list_of_head_nrs=head_map.head_nr,\n                layer_name=layer_name,\n                data_mode=data_mode,\n            )\n            # we compute this for all heads irrespective of gender-only similarity matrices\n\n            # Save feature table\n            cprint(string=\"Saving feature table ...\", col=\"b\")\n            feat_tab.to_pickle(p2_feat_mat)  # save feature table as pickle (pd.DataFrame) [fast &amp; small]\n\n    # Compute similarity matrix\n    cprint(string=f\"Computing similarity matrix for '{p2_feat_mat.name.split('.')[0]}' ...\", col=\"b\")\n    if p2_feat_sim_mat.is_file():\n        feat_sim_mat = np.load(file=p2_feat_sim_mat, allow_pickle=True)\n        if pilot_version is not None:\n            msg = \"Cropping feat. sim. mat. for pilot is not implemented yet.\"\n            raise NotImplementedError(msg)\n    else:\n        # Filter for gender if requested\n        if gender is not None:\n            feat_tab = feat_tab.loc[\n                head_map[head_map.Model.str.contains(\"WF\" if gender == \"female\" else \"WM\")].head_nr\n            ]\n\n        feat_sim_mat = compute_feature_similarity_matrix(feature_table=feat_tab, pca=pca, metric=metric, z_score=False)\n\n        # Save similarity matrix\n        cprint(string=\"Saving similarity matrix ...\", col=\"b\")\n        np.save(file=p2_feat_sim_mat, arr=feat_sim_mat, allow_pickle=True)  # save similarity matrix\n        # for z_scored is True: matrices are in \"./results/heads/VGGface_zscored\"\n\n    return feat_sim_mat\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.compute_vgg_human_judgment_feature_map_similarity_matrix","title":"compute_vgg_human_judgment_feature_map_similarity_matrix","text":"<pre><code>compute_vgg_human_judgment_feature_map_similarity_matrix(\n    session: str,\n    model_name: str | None = None,\n    pca: bool | float = False,\n    data_mode: str = \"3d-reconstructions\",\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n    extract_feat_maps: bool = False,\n) -&gt; NDArray[float64]\n</code></pre> <p>Compute a similarity matrix from feature maps of the <code>vgg_core_bridge</code> layer in <code>VGGFaceHumanjudgment[FrozenCore]</code>.</p> <p>To extend computational efficiency</p> <p>Intermediate results are saved to disk, such that they do not have to be recomputed each time (time-consuming).</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>model_name</code> <code>str | None</code> <p>name of the model to use (if None, use the best model)</p> <code>None</code> <code>pca</code> <code>bool | float</code> <p>False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that pca [float] *100 % of variance is explained</p> <code>False</code> <code>data_mode</code> <code>str</code> <p>the path to the \"2d-original\", \"3d-reconstructions\", or \"3d-perspectives\"</p> <code>'3d-reconstructions'</code> <code>gender</code> <code>str | None</code> <p>define gender if requested</p> <code>None</code> <code>pilot_version</code> <code>int | None</code> <p>None for main experiment, OR pilot 1, OR 2</p> <code>None</code> <code>metric</code> <code>str</code> <p>similarity metric to use (\"cosine\", \"euclidean\")</p> <code>'cosine'</code> <code>extract_feat_maps</code> <code>bool</code> <p>whether to extract feature maps from VGGFace</p> <code>False</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>similarity matrix based on VGGFace feature maps</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def compute_vgg_human_judgment_feature_map_similarity_matrix(\n    session: str,\n    model_name: str | None = None,\n    pca: bool | float = False,\n    data_mode: str = \"3d-reconstructions\",\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n    extract_feat_maps: bool = False,\n) -&gt; npt.NDArray[np.float64]:\n    \"\"\"\n    Compute a similarity matrix from feature maps of the `vgg_core_bridge` layer in `VGGFaceHumanjudgment[FrozenCore]`.\n\n    !!! note \"To extend computational efficiency\"\n        Intermediate results are saved to disk, such that they do not have to be recomputed each time (time-consuming).\n\n    :param session: '2D', OR '3D'\n    :param model_name: name of the model to use (if None, use the best model)\n    :param pca: False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that\n                pca [float] *100 % of variance is explained\n    :param data_mode: the path to the \"2d-original\", \"3d-reconstructions\", or \"3d-perspectives\"\n    :param gender: define gender if requested\n    :param pilot_version: None for main experiment, OR pilot 1, OR 2\n    :param metric: similarity metric to use (\"cosine\", \"euclidean\")\n    :param extract_feat_maps: whether to extract feature maps from VGGFace\n    :return: similarity matrix based on VGGFace feature maps\n    \"\"\"\n    # Get model information\n    model_info = get_model_hps(\n        session=session,\n        model_name=model_name,\n        data_mode=data_mode,\n        exclusive_gender_trials=gender,\n    )\n\n    # Get head mapping table\n    head_map = heads_naming_converter_table(pilot_version=pilot_version)\n\n    # Prepare gender-only features if requested\n    gender_suffix: str = \"\"\n    if gender is not None:\n        gender = check_gender(gender=gender)\n        gender_suffix = f\"_{gender}_only\"\n\n    # Set path to feature matrix\n    data_mode_suffix = (\n        \"original\"\n        if \"orig\" in model_info.data_mode\n        else \"3D-recon\"\n        if \"3d-recon\" in model_info.data_mode\n        else \"3D-persp\"\n    )\n    p2_feat_mat = Path(\n        paths.results.main.vgg.feature_maps.format(session=session),\n        f\"{model_info.model_name}_feature_maps_{data_mode_suffix}_\"\n        f\"{f'PCA-{pca:.2f}_' if pca else ''}vgg_core_bridge.pd.pickle\",\n    )\n    p2_feat_sim_mat = Path(str(p2_feat_mat).replace(\".pd.pickle\", f\"_{metric}-similarity-matrix{gender_suffix}.npy\"))\n    p2_feat_mat.parent.mkdir(parents=True, exist_ok=True)  # create directory if not exists\n\n    # Get the table with VGGFaceHumanjudgment[FrozenCore] activations maps of all layers elicited by each head\n    if not p2_feat_sim_mat.is_file() or extract_feat_maps:\n        # optional when similarity matrix is already computed\n        if p2_feat_mat.is_file():\n            feat_tab = pd.read_pickle(p2_feat_mat)\n        else:\n            feat_tab = get_vgg_human_judgment_activation_maps(\n                list_of_head_nrs=head_map.head_nr,\n                session=session,\n                model_name=model_info.model_name,\n                data_mode=model_info.data_mode,\n                exclusive_gender_trials=gender,\n            )\n            # compute this for all heads irrespective of gender-only similarity matrices\n\n            # Save feature table\n            cprint(string=\"Saving feature table ...\", col=\"b\")\n            feat_tab.to_pickle(p2_feat_mat)  # save feature table as pickle (pd.DataFrame) [fast &amp; small]\n\n    # Compute similarity matrix\n    cprint(string=f\"Computing similarity matrix for '{p2_feat_mat.name.split('.')[0]}' ...\", col=\"b\")\n    if p2_feat_sim_mat.is_file():\n        feat_sim_mat = np.load(file=p2_feat_sim_mat, allow_pickle=True)\n        if pilot_version is not None:\n            msg = \"Cropping feat. sim. mat. for pilot is not implemented yet.\"\n            raise NotImplementedError(msg)\n    else:\n        # Filter for gender if requested\n        if gender is not None:\n            feat_tab = feat_tab.loc[\n                head_map[head_map.Model.str.contains(\"WF\" if gender == \"female\" else \"WM\")].head_nr\n            ]\n\n        feat_sim_mat = compute_feature_similarity_matrix(feature_table=feat_tab, pca=pca, metric=metric, z_score=False)\n\n        # Save similarity matrix\n        cprint(string=\"Saving similarity matrix ...\", col=\"b\")\n        np.save(file=p2_feat_sim_mat, arr=feat_sim_mat, allow_pickle=True)  # save similarity matrix\n\n    return feat_sim_mat\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.compute_vice_feature_map_similarity_matrix","title":"compute_vice_feature_map_similarity_matrix","text":"<pre><code>compute_vice_feature_map_similarity_matrix(\n    session: str,\n    pca: bool | float = False,\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n) -&gt; NDArray[float64]\n</code></pre> <p>Compute a similarity matrix from <code>VICE</code> feature maps (embedding matrix).</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>pca</code> <code>bool | float</code> <p>False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that pca [float] *100 % of variance is explained</p> <code>False</code> <code>gender</code> <code>str | None</code> <p>for exclusively within-gender feature comparison</p> <code>None</code> <code>pilot_version</code> <code>int | None</code> <p>None for the main experiment, OR pilot 1, OR 2.</p> <code>None</code> <code>metric</code> <code>str</code> <p>similarity metric to use (cosine, Euclidean)</p> <code>'cosine'</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>similarity matrix of VICE feature maps</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def compute_vice_feature_map_similarity_matrix(\n    session: str,\n    pca: bool | float = False,\n    gender: str | None = None,\n    pilot_version: int | None = None,\n    metric: str = \"cosine\",\n) -&gt; npt.NDArray[np.float64]:\n    \"\"\"\n    Compute a similarity matrix from `VICE` feature maps (embedding matrix).\n\n    :param session: '2D', OR '3D'\n    :param pca: False OR provide (0.&lt; pca &lt; 1.) if PCA should be run on feature table with n components such that\n                pca [float] *100 % of variance is explained\n    :param gender: for exclusively within-gender feature comparison\n    :param pilot_version: None for the main experiment, OR pilot 1, OR 2.\n    :param metric: similarity metric to use (cosine, Euclidean)\n    :return: similarity matrix of VICE feature maps\n    \"\"\"\n    if pilot_version == 1:\n        msg = \"VICE feature maps are not available for pilot 1.\"\n        raise ValueError(msg)\n\n    # Get head mapping table\n    head_map = heads_naming_converter_table(pilot_version=pilot_version)\n    list_of_models = head_map.Model\n\n    # Filter for gender if requested\n    if gender is not None:\n        # Prepare gender-only features if requested\n        gender = check_gender(gender)\n        list_of_models = head_map.Model[head_map.Model.str.contains(\"WF\" if \"female\" in gender else \"WM\")]\n\n    # Get the weights of the best hyperparameters for VICE\n    best_hp_vice = get_best_hp_vice(hp_search=True, print_n=0, from_config=True)[session]\n    best_hp_vice.pop(\"hp_perc\")  # we want params of the main run\n    path_to_vice_sim_mat = create_path_from_vice_params(\n        params_dict=best_hp_vice, gender=gender, pilot=pilot_version is not None\n    )\n    # Cut Path at VICE/\n    param_path_vice = str(path_to_vice_sim_mat).split(f\"VICE/{session}/\")[-1]\n\n    vice_weights = load_vice_weights(\n        session=session,\n        pilot=pilot_version is not None,\n        pruned=True,\n        return_path=False,\n        param_path=param_path_vice,\n    )[0]  # take only loc_params\n\n    feat_tab = pd.DataFrame(index=list_of_models, columns=[f\"D{i:03d}\" for i in range(vice_weights.shape[1])])\n    feat_tab.loc[:, :] = vice_weights\n\n    # R(vice~BSM) of compute_vice_similarity_matrix &gt; compute_vice_feature_map_similarity_matrix(..., z_score=True).\n    # The Difference is that we do not z-score the features (dims) in the former case.\n    # And we do not normalize the similarity matrix (0,1), however, this should have no effect on the R-value.\n    # Z-scoring performs (probably) worse here, since we weaken the relevance of the sparse model dimensions,\n    # in terms of its order.\n    # Later dimensions are less important and represent stimulus similarities less well.\n    # With z-scoring, we make differences between stimuli in these late dimensions as pronounced as in the first,\n    # i.e., more relevant dimensions.\n    # With z_score=False: == compute_vice_similarity_matrix,\n    return compute_feature_similarity_matrix(feature_table=feat_tab, pca=pca, metric=metric, z_score=False)\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.extract_exclusive_gender_trials","title":"extract_exclusive_gender_trials","text":"<pre><code>extract_exclusive_gender_trials(\n    trial_results_table: DataFrame,\n    gender: str,\n    verbose: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Extract triplets that contain only the faces of one gender.</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def extract_exclusive_gender_trials(\n    trial_results_table: pd.DataFrame, gender: str, verbose: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"Extract triplets that contain only the faces of one gender.\"\"\"\n    gender = check_gender(gender)\n\n    # Define drop condition for index\n    # (could be done for both genders at once ...)\n    n_all_trials = len(trial_results_table)\n    n_one_gender = 50\n    drop_cond = (lambda x: (x &gt; n_one_gender).any()) if gender == \"female\" else (lambda x: (x &lt;= n_one_gender).any())\n    drop_indices = []  # init list for indices to drop\n    for i, tr_row in tqdm(\n        trial_results_table[[\"head1\", \"head2\", \"head3\"]].iterrows(),\n        desc=f\"Filter table for {gender} only trials\",\n        total=n_all_trials,\n    ):\n        if drop_cond(tr_row):\n            drop_indices.append(i)\n    trial_results_table = trial_results_table.drop(index=drop_indices, inplace=False).reset_index(drop=True)\n    if verbose:\n        cprint(string=f\"From initial {n_all_trials} trials, {len(trial_results_table)} trials remain.\", col=\"b\")\n\n    return trial_results_table\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.extract_set_of_heads","title":"extract_set_of_heads","text":"<pre><code>extract_set_of_heads(\n    trial_results_table: DataFrame,\n) -&gt; list\n</code></pre> <p>Extract the set of heads from a trial-results table.</p> <p>That is, the heads that appeared in the experiment.</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def extract_set_of_heads(trial_results_table: pd.DataFrame) -&gt; list:\n    \"\"\"\n    Extract the set of heads from a trial-results table.\n\n    That is, the heads that appeared in the experiment.\n    \"\"\"\n    if \"head1\" in trial_results_table.columns:\n        return sorted(np.unique(trial_results_table[[\"head1\", \"head2\", \"head3\"]].to_numpy().flatten()))\n\n    if \"triplet\" in trial_results_table.columns:\n        heads = trial_results_table.triplet.apply(lambda x: x.split(\"_\"))\n        return sorted(np.unique(np.concatenate(heads.to_numpy()).astype(int)))\n\n    msg = \"Unknown trial results table format. 'head1', 'head2', 'head3' AND/OR 'triplet' are expected as columns.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.get_corr_df_rsm","title":"get_corr_df_rsm","text":"<pre><code>get_corr_df_rsm(corr_name: str, metric: str) -&gt; DataFrame\n</code></pre> <p>Get the correlation dataframe for representational similarity matrices (<code>RSM</code>).</p> <p>Parameters:</p> Name Type Description Default <code>corr_name</code> <code>str</code> <p>name of correlation to use (\"Pearson\", \"Spearman\")</p> required <code>metric</code> <code>str</code> <p>similarity metric to use (\"cosine\", \"euclidean\")</p> required Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def get_corr_df_rsm(corr_name: str, metric: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Get the correlation dataframe for representational similarity matrices (`RSM`).\n\n    :param corr_name: name of correlation to use (\"Pearson\", \"Spearman\")\n    :param metric: similarity metric to use (\"cosine\", \"euclidean\")\n    \"\"\"\n    return pd.read_csv(Path(paths.results.main.rsa, f\"{corr_name.title()}_{metric.lower()}.csv\"), index_col=0)\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.get_model_hps","title":"get_model_hps  <code>cached</code>","text":"<pre><code>get_model_hps(\n    session: str,\n    model_name: str | None = None,\n    data_mode: str | None = None,\n    exclusive_gender_trials: str | None = None,\n) -&gt; Series\n</code></pre> <p>Get model hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>2D, OR 3D</p> required <code>model_name</code> <code>str | None</code> <p>name of the model</p> <code>None</code> <code>data_mode</code> <code>str | None</code> <p>'2d-original', '3d-reconstructions', OR '3d-perspectives'</p> <code>None</code> <code>exclusive_gender_trials</code> <code>str | None</code> <p>'female', 'male', OR None for all trials.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>model-specific hyperparameters</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>@lru_cache(maxsize=6)\ndef get_model_hps(\n    session: str,\n    model_name: str | None = None,\n    data_mode: str | None = None,\n    exclusive_gender_trials: str | None = None,\n) -&gt; pd.Series:\n    \"\"\"\n    Get model hyperparameters.\n\n    :param session: 2D, OR 3D\n    :param model_name: name of the model\n    :param data_mode: '2d-original', '3d-reconstructions', OR '3d-perspectives'\n    :param exclusive_gender_trials: 'female', 'male', OR None for all trials.\n    :return: model-specific hyperparameters\n    \"\"\"\n    # Get the model performance table\n    model_table = get_vgg_performance_table(\n        sort_by_acc=True, hp_search=False, exclusive_gender_trials=exclusive_gender_trials\n    )\n\n    # Filter for session\n    model_table = model_table[model_table.session == session]\n\n    # Filter for data mode\n    if data_mode is not None:\n        data_mode = data_mode.lower()\n        model_table = model_table[model_table.data_mode == data_mode]\n\n    # Filter for the model\n    if model_name is None:\n        # Take the best model\n        model_hp_row = model_table[model_table.model_name == model_table.model_name].iloc[0]\n    else:\n        # Take the requested model\n        model_hp_row = model_table[model_table.model_name == model_name].iloc[0]\n\n    return model_hp_row\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Run the main function of the <code>rsa.py</code> script.</p> <p>This script computes the correlation between the similarity judgments of the 2D and 3D sessions. Moreover, it runs RSA on different similarity matrices and creates plots.</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"\n    Run the main function of the `rsa.py` script.\n\n    This script computes the correlation between the similarity judgments of the 2D and 3D sessions.\n    Moreover, it runs RSA on different similarity matrices and creates plots.\n    \"\"\"\n    # Set correlation function name\n    corr_name = \"Spearman\" if FLAGS.spearman else \"Pearson\"  # OR: corr_func.__name__[:-1].title()\n\n    # Set logger\n    logger = logging.getLogger(__name__)  # get predefined logger\n    logger_filename = (\n        Path(paths.results.pilot.v2.rsa if FLAGS.pilot else paths.results.main.rsa)\n        / f\"logs/rsa_{corr_name}_{FLAGS.metric}.log\"\n    )\n    if FLAGS.logger_overwrite and logger_filename.is_file():\n        logger_filename.unlink()\n\n    logger = update_logger_configs(new_logger_name=\"RSA\", new_logger_filename=logger_filename, logger=logger)\n\n    # %% Init correlation table (RSA)\n    rsa_corr_df = pd.DataFrame(\n        index=[sess + \"_BSM\" for sess in params.SESSIONS]\n        + [sess + \"_BSM_\" + g for g in params.GENDERS for sess in params.SESSIONS]  # gender slices but mixed trials\n        + [sess + \"_BSM_\" + f\"{g}_only\" for g in params.GENDERS for sess in params.SESSIONS]\n        # exclusive gender trials\n    )\n\n    # %% Compute aggregated similarity judgments (behavioral similarity matrix, BSM)\n    sim_mat_all_2d = aggregate_judgments_in_session(session=\"2D\", pilot=FLAGS.pilot, verbose=FLAGS.verbose)\n    sim_mat_all_3d = aggregate_judgments_in_session(session=\"3D\", pilot=FLAGS.pilot, verbose=FLAGS.verbose)\n\n    # %% Compute BSMs with exclusive gender trials\n    sim_mat_all_2d_female = aggregate_judgments_in_session_by_gender(\n        session=\"2D\", gender=\"female\", pilot=FLAGS.pilot, verbose=FLAGS.verbose\n    )\n    sim_mat_all_2d_male = aggregate_judgments_in_session_by_gender(\n        session=\"2D\", gender=\"male\", pilot=FLAGS.pilot, verbose=FLAGS.verbose\n    )\n\n    sim_mat_all_3d_female = aggregate_judgments_in_session_by_gender(\n        session=\"3D\", gender=\"female\", pilot=FLAGS.pilot, verbose=FLAGS.verbose\n    )\n    sim_mat_all_3d_male = aggregate_judgments_in_session_by_gender(\n        session=\"3D\", gender=\"male\", pilot=FLAGS.pilot, verbose=FLAGS.verbose\n    )\n\n    sim_mats_by_exclusive_gender_dict = {\n        \"2D\": {\"female\": sim_mat_all_2d_female, \"male\": sim_mat_all_2d_male},\n        \"3D\": {\"female\": sim_mat_all_3d_female, \"male\": sim_mat_all_3d_male},\n    }\n    # Note: These contain per-gender-only-triplets which exclusively consist of the respective gender\n\n    # %% Visualize the BSMs\n    n_female: int = 12 if FLAGS.pilot else 50\n    if FLAGS.plot:\n        for session, sim_mat_all in zip(params.SESSIONS, [sim_mat_all_2d, sim_mat_all_3d], strict=True):\n            visualise_matrix(\n                face_sim_mat=sim_mat_all,\n                session=session,\n                pilot=FLAGS.pilot,\n                use_rsatoolbox=FLAGS.rsa_toolbox,\n                save=FLAGS.save_plots,\n            )\n            # Plot for each gender\n            for gender, slice_it in zip(\n                params.GENDERS, [slice(0, n_female, 1), slice(n_female, None, 1)], strict=True\n            ):\n                visualise_matrix(\n                    face_sim_mat=sim_mat_all[slice_it, slice_it],\n                    session=session,\n                    pilot=FLAGS.pilot,\n                    fig_name=f\"{gender}_{session}\",\n                    use_rsatoolbox=FLAGS.rsa_toolbox,\n                    save=FLAGS.save_plots,\n                )\n\n        # Plot exclusive gender trials\n        for session in params.SESSIONS:\n            for gender in params.GENDERS:\n                visualise_matrix(\n                    face_sim_mat=sim_mats_by_exclusive_gender_dict[session][gender],\n                    session=session,\n                    pilot=FLAGS.pilot,\n                    use_rsatoolbox=FLAGS.rsa_toolbox,\n                    fig_name=f\"{gender}-only_{session}\",\n                    save=FLAGS.save_plots,\n                )\n\n    # %% Compute the correlation between similarity judgments (BSMs) of both conditions (2D &amp; 3D)\n    corr_func = spearmanr if FLAGS.spearman else pearsonr\n\n    r, p = corr_func(vectorize_similarity_matrix(sim_mat_all_2d), vectorize_similarity_matrix(sim_mat_all_3d))\n    log_msg = (\n        f\"{corr_name} correlation between similarity judgments of 2D &amp; 3D: R={r:.3f}, p&lt;={p:.5g}.\"\n        f\"\\t{1 - r**2:.2%} of variance in one condition remains unexplained by the other.\"\n    )\n    logger.info(msg=log_msg)\n\n    # Save results\n    rsa_corr_df.loc[\"2D_BSM\", \"3D_BSM_r\"] = r\n    rsa_corr_df.loc[\"2D_BSM\", \"3D_BSM_p\"] = p\n    rsa_corr_df.loc[\"3D_BSM\", \"2D_BSM_r\"] = r\n    rsa_corr_df.loc[\"3D_BSM\", \"2D_BSM_p\"] = p\n\n    # Compute correlations of BSMs for exclusive and non-exclusive gender trials\n    for gender, slice_it in zip(params.GENDERS, [slice(0, n_female, 1), slice(n_female, None, 1)], strict=True):\n        # Compute for non-exclusive gender trials\n        r, p = corr_func(\n            vectorize_similarity_matrix(sim_mat_all_2d[slice_it, slice_it]),\n            vectorize_similarity_matrix(sim_mat_all_3d[slice_it, slice_it]),\n        )\n        log_msg = (\n            f\"{corr_name} correlation between similarity judgments of 2D &amp; 3D within {gender} (non-exclusive): \"\n            f\"r={r:.3f}, p&lt;={p:.5g}.\"\n            f\"\\t{1 - r**2:.2%} of variance in one condition remains unexplained by the other.\"\n        )\n        logger.info(msg=log_msg)\n\n        rsa_corr_df.loc[f\"2D_BSM_{gender}\", f\"3D_BSM_{gender}_r\"] = r\n        rsa_corr_df.loc[f\"2D_BSM_{gender}\", f\"3D_BSM_{gender}_p\"] = p\n        rsa_corr_df.loc[f\"3D_BSM_{gender}\", f\"2D_BSM_{gender}_r\"] = r\n        rsa_corr_df.loc[f\"3D_BSM_{gender}\", f\"2D_BSM_{gender}_p\"] = p\n\n        # Compute for exclusive gender trials\n        r, p = corr_func(\n            vectorize_similarity_matrix(sim_mats_by_exclusive_gender_dict[\"2D\"][gender]),\n            vectorize_similarity_matrix(sim_mats_by_exclusive_gender_dict[\"3D\"][gender]),\n        )\n\n        log_msg = (\n            f\"{corr_name} correlation between similarity judgments of 2D &amp; 3D within {gender} only: \"\n            f\"r={r:.3f}, p&lt;={p:.5g}.\"\n            f\"\\t{1 - r**2:.2%} of variance in one condition remains unexplained by the other.\"\n        )\n        logger.info(msg=log_msg)\n\n        rsa_corr_df.loc[f\"2D_BSM_{gender}_only\", f\"3D_BSM_{gender}_only_r\"] = r\n        rsa_corr_df.loc[f\"2D_BSM_{gender}_only\", f\"3D_BSM_{gender}_only_p\"] = p\n        rsa_corr_df.loc[f\"3D_BSM_{gender}_only\", f\"2D_BSM_{gender}_only_r\"] = r\n        rsa_corr_df.loc[f\"3D_BSM_{gender}_only\", f\"2D_BSM_{gender}_only_p\"] = p\n\n    # Sanity check: correlation between exclusive gender trials (female ~ male; should be low -&gt; 0)\n    for (sess_1, gender_1), (sess_2, gender_2) in itertools.combinations(\n        itertools.product(params.SESSIONS, params.GENDERS), r=2\n    ):\n        if gender_1 == gender_2:\n            # Do not compare within gender across sessions (e.g., \"male-2D\" vs \"male-3D\"), since this is done elsewhere\n            continue\n\n        r, p = corr_func(\n            vectorize_similarity_matrix(sim_mats_by_exclusive_gender_dict[sess_1][gender_1]),\n            vectorize_similarity_matrix(sim_mats_by_exclusive_gender_dict[sess_2][gender_2]),\n        )\n        log_msg = (\n            f\"{corr_name} correlation between similarity judgments of {sess_1} &amp; {sess_2} between gender-only \"\n            f\"({gender_1}~{gender_2}): r={r:.3f}, p&lt;={p:.5g}.\"\n            f\"\\t{1 - r**2:.2%} of variance in one condition remains unexplained by the other.\"\n        )\n        logger.info(msg=log_msg)\n\n        # Save results\n        rsa_corr_df.loc[f\"{sess_1}_BSM_{gender_1}_only\", f\"{sess_2}_BSM_{gender_2}_only_r\"] = r\n        rsa_corr_df.loc[f\"{sess_1}_BSM_{gender_1}_only\", f\"{sess_2}_BSM_{gender_2}_only_p\"] = p\n        rsa_corr_df.loc[f\"{sess_2}_BSM_{gender_2}_only\", f\"{sess_1}_BSM_{gender_1}_only_r\"] = r\n        rsa_corr_df.loc[f\"{sess_2}_BSM_{gender_2}_only\", f\"{sess_1}_BSM_{gender_1}_only_p\"] = p\n\n    # %% Compute (cosine/euclidean) similarity of (physical and computational) face features, also with PCA version\n    # Compute similarity matrices based on CFD physical face features (PFF)\n    feature_dict = {\"CFD_PFF\": compute_physical_attr_similarity_matrix}\n    # Compute for exclusive gender trials\n    for gender in params.GENDERS:\n        feature_dict[f\"CFD_PFF_{gender}_only\"] = partial(compute_physical_attr_similarity_matrix, gender=gender)\n\n    # Compute similarity matrices based on FLAME/DECA features\n    list_of_flame_params = latent_flame_code + ([\"detail\"] if FLAGS.flame_model == \"deca\" else [])\n    for feat in list_of_flame_params:  # add FLAME features\n        feature_dict[f\"{FLAGS.flame_model.upper()}_{feat.upper()}\"] = partial(\n            compute_flame_feature_similarity_matrix,\n            param=feat,\n            model=FLAGS.flame_model,\n            pilot_version=2 if FLAGS.pilot else None,\n        )\n        # Compute for within gender stimuli\n        for gender in params.GENDERS:\n            feature_dict[f\"{FLAGS.flame_model.upper()}_{feat.upper()}_{gender}_only\"] = partial(\n                compute_flame_feature_similarity_matrix,\n                param=feat,\n                model=FLAGS.flame_model,\n                gender=gender,\n                pilot_version=2 if FLAGS.pilot else None,\n            )\n\n    # Compute similarity matrices based on VGG-Face features\n    for data_mode in [\"2d-original\", \"3d-reconstructions\"]:  # add VGG feature maps\n        data_mode_suffix = \"org\" if \"orig\" in data_mode else \"3D-recon\" if \"3d-recon\" in data_mode else \"3D-persp\"\n        for layer_name in get_vgg_face_model(save_layer_output=False).layer_names:\n            vgg_feat_name = f\"VGG_{data_mode_suffix}_{layer_name.upper()}\"\n            feature_dict[vgg_feat_name] = partial(\n                compute_vgg_feature_map_similarity_matrix,\n                layer_name=layer_name,\n                data_mode=data_mode,\n                pilot_version=2 if FLAGS.pilot else None,\n            )\n            # Compute for within gender stimuli\n            for gender in params.GENDERS:\n                feature_dict[f\"{vgg_feat_name}_{gender}_only\"] = partial(\n                    compute_vgg_feature_map_similarity_matrix,\n                    layer_name=layer_name,\n                    data_mode=data_mode,\n                    gender=gender,\n                    pilot_version=2 if FLAGS.pilot else None,\n                )\n\n    # Compute similarity matrices based on SPoSe &amp; VICE (i.e., sparse) embeddings\n    for session in params.SESSIONS:\n        for sparse_model_name, compute_sparse_feature_map_similarity_matrix in zip(\n            [\"SPoSE\", \"VICE\"],\n            [compute_spose_feature_map_similarity_matrix, compute_vice_feature_map_similarity_matrix],\n            strict=True,\n        ):\n            # Fill function as for other models compute_*_feature_map_similarity_matrix()\n            feature_dict[f\"{sparse_model_name}_{session}\"] = partial(\n                compute_sparse_feature_map_similarity_matrix,\n                session=session,\n                pilot_version=2 if FLAGS.pilot else None,\n            )\n\n            # Compute within exclusive gender stimuli\n            for gender in params.GENDERS:\n                # Fill function as for other models compute_*_feature_map_similarity_matrix()\n                feature_dict[f\"{sparse_model_name}_{session}_{gender}_only\"] = partial(\n                    compute_sparse_feature_map_similarity_matrix,\n                    session=session,\n                    gender=gender,\n                    pilot_version=2 if FLAGS.pilot else None,\n                )\n\n    # Compute similarity matrices based on VGGFaceHumanJudgment[FrozenCore] embeddings\n    for session in params.SESSIONS:\n        model_name_session = get_model_hps(session=session, model_name=None, exclusive_gender_trials=None).model_name\n        # 1. Compute similarity based on embeddings (i.e., feature maps of vgg_core_bridge), similar to VGGface above\n        feature_dict[f\"{model_name_session}_{session}_embedding\"] = partial(  # pca &amp; metric fill be passed below\n            compute_vgg_human_judgment_feature_map_similarity_matrix,\n            session=session,\n            model_name=model_name_session,\n            data_mode=data_mode,\n            gender=None,\n            pilot_version=2 if FLAGS.pilot else None,\n        )\n\n        # Compute within exclusive gender stimuli\n        for gender in params.GENDERS:\n            model_name_session_gender = get_model_hps(\n                session=session, model_name=None, exclusive_gender_trials=gender\n            ).model_name\n            feature_dict[f\"{model_name_session_gender}_{session}_{gender}_only\"] = (\n                partial(  # pca &amp; metric fill be passed below\n                    compute_vgg_human_judgment_feature_map_similarity_matrix,\n                    session=session,\n                    model_name=model_name_session_gender,\n                    data_mode=data_mode,\n                    gender=gender,\n                    pilot_version=2 if FLAGS.pilot else None,\n                )\n            )\n\n        # 2. Compute similarity based on VGGFaceHumanjudgment[FrozenCore] decisions, similar to BSMs\n        feature_dict[f\"{model_name_session}_{session}_decision\"] = (\n            compute_similarity_matrix_from_vgg_face_human_judgment_model(\n                session=session,\n                model_name=model_name_session,\n                split_return=False,\n                n_faces=None,\n                exclusive_gender_trials=None,\n                verbose=True,\n            )\n        )\n\n        # Compute within exclusive gender stimuli\n        for gender in params.GENDERS:\n            model_name_session_gender = get_model_hps(\n                session=session, model_name=None, exclusive_gender_trials=gender\n            ).model_name\n\n            feature_dict[f\"{model_name_session}_{session}_decision_{gender}_only\"] = (\n                compute_similarity_matrix_from_vgg_face_human_judgment_model(\n                    session=session,\n                    model_name=model_name_session_gender,\n                    split_return=False,\n                    n_faces=params.main.n_faces // 2,\n                    exclusive_gender_trials=gender,\n                    verbose=True,\n                )\n            )\n\n    # %% Run through all physical &amp; computational face features, and compute their correlations with BSMs and plot them\n    for feature, feat_vals in feature_dict.items():\n        cprint(string=f\"\\n{feature}\", col=\"b\", fm=\"ul\")\n        gender_feat: bool = \"_only\" in feature\n\n        # Compute cosine similarity of face features, with and without a PCA version\n        if callable(feat_vals):\n            feat_sim_mat = feat_vals(pca=False, metric=FLAGS.metric)\n            pca_feat_sim_mat = feat_vals(pca=FLAGS.pca_fraction, metric=FLAGS.metric)\n            extra_sim_case = False\n        else:\n            feat_sim_mat = feat_vals\n            pca_feat_sim_mat = None\n            extra_sim_case = True\n\n        # Plot similarity matrices based on face features\n        if FLAGS.plot:\n            visualise_matrix(\n                face_sim_mat=feat_sim_mat,\n                session=\"rsatoolbox\" if FLAGS.rsa_toolbox else \"\",\n                pilot=FLAGS.pilot,\n                use_rsatoolbox=FLAGS.rsa_toolbox,\n                # vmin=feat_sim_mat.min().round(3), vmax=1,\n                fig_name=f\"Similarity ({'extra' if extra_sim_case else FLAGS.metric}) of {feature} face features\",\n                save=FLAGS.save_plots,\n            )\n\n            if pca_feat_sim_mat is not None:\n                visualise_matrix(\n                    face_sim_mat=pca_feat_sim_mat,\n                    session=\"pca_rsatoolbox\" if FLAGS.rsa_toolbox else \"\",\n                    pilot=FLAGS.pilot,\n                    use_rsatoolbox=FLAGS.rsa_toolbox,  # vmax=pca_feat_sim_mat.max(),\n                    fig_name=f\"Similarity ({FLAGS.metric}) of {feature} face features ({FLAGS.pca_fraction:.0%}-PCA)\",\n                    save=FLAGS.save_plots,\n                )\n\n        # Plot for each gender (for non-exclusive gender features)\n        if FLAGS.plot and not gender_feat:\n            for gender, slice_it in zip(\n                params.GENDERS, [slice(0, n_female, 1), slice(n_female, None, 1)], strict=True\n            ):\n                visualise_matrix(\n                    face_sim_mat=feat_sim_mat[slice_it, slice_it],\n                    session=\"\",\n                    pilot=FLAGS.pilot,\n                    fig_name=f\"{gender} ({'extra' if extra_sim_case else FLAGS.metric}) {feature} feats\",\n                    use_rsatoolbox=FLAGS.rsa_toolbox,\n                    save=FLAGS.save_plots,\n                )\n\n                if pca_feat_sim_mat is not None:\n                    visualise_matrix(\n                        face_sim_mat=pca_feat_sim_mat[slice_it, slice_it],\n                        session=\"\",\n                        pilot=FLAGS.pilot,\n                        fig_name=f\"{gender} ({FLAGS.metric}) {feature} feats ({FLAGS.pca_fraction:.0%}-PCA)\",\n                        use_rsatoolbox=FLAGS.rsa_toolbox,\n                        save=FLAGS.save_plots,\n                    )\n\n        # Compute correlation of physical or computational face features with behavioral similarity judgments (BSM)\n        for session, sim_mat_all in zip(params.SESSIONS, [sim_mat_all_2d, sim_mat_all_3d], strict=True):\n            cprint(session, fm=\"ul\")\n            if not gender_feat:  # Compute correlation face features with BSM for all trials\n                r, p = corr_func(vectorize_similarity_matrix(sim_mat_all), vectorize_similarity_matrix(feat_sim_mat))\n                msg = (\n                    f\"{corr_name} correlation between similarity judgments of {session} &amp; {feature} features: \"\n                    f\"r={r:.3f}, p&lt;={p:.5g}\"\n                )\n                logger.info(msg=msg)\n\n                # Save results to file\n                rsa_corr_df.loc[f\"{session}_BSM\", f\"{feature}_r\"] = r\n                rsa_corr_df.loc[f\"{session}_BSM\", f\"{feature}_p\"] = p\n\n                if pca_feat_sim_mat is not None:\n                    r, p = corr_func(\n                        vectorize_similarity_matrix(sim_mat_all), vectorize_similarity_matrix(pca_feat_sim_mat)\n                    )\n                    msg = (\n                        f\"{corr_name} correlation between similarity judgments of {session} &amp; \"\n                        f\"{FLAGS.pca_fraction:.0%}-PCA-{feature} features: r={r:.3f}, p&lt;={p:.5g}\"\n                    )\n                    logger.info(msg=msg)\n\n                    # Save results to file\n                    rsa_corr_df.loc[f\"{session}_BSM\", f\"{FLAGS.pca_fraction:.0%}-PCA-{feature}_r\"] = r\n                    rsa_corr_df.loc[f\"{session}_BSM\", f\"{FLAGS.pca_fraction:.0%}-PCA-{feature}_p\"] = p\n            else:  # Compute correlation face features with BSM for exclusive gender trials\n                for gender in params.GENDERS:\n                    if f\"_{gender}_\" not in feature:\n                        continue\n\n                    r, p = corr_func(\n                        vectorize_similarity_matrix(sim_mats_by_exclusive_gender_dict[session][gender]),\n                        vectorize_similarity_matrix(feat_sim_mat),\n                    )\n                    msg = (\n                        f\"{corr_name} correlation between similarity judgments of {session} within {gender} only &amp; \"\n                        f\"{feature} features: r={r:.3f}, p&lt;={p:.5g}\"\n                    )\n                    logger.info(msg=msg)\n\n                    # Save results to file\n                    rsa_corr_df.loc[f\"{session}_BSM_{gender}_only\", f\"{feature}_r\"] = r\n                    rsa_corr_df.loc[f\"{session}_BSM_{gender}_only\", f\"{feature}_p\"] = p\n\n                    if pca_feat_sim_mat is not None:\n                        r, p = corr_func(\n                            vectorize_similarity_matrix(sim_mats_by_exclusive_gender_dict[session][gender]),\n                            vectorize_similarity_matrix(pca_feat_sim_mat),\n                        )\n                        msg = (\n                            f\"{corr_name} correlation between similarity judgments of {session} within {gender} only \"\n                            f\"&amp; {FLAGS.pca_fraction:.0%}-PCA-{feature} features: r={r:.3f}, p&lt;={p:.5g}\"\n                        )\n                        logger.info(msg=msg)\n\n                        # Save results to file\n                        rsa_corr_df.loc[\n                            f\"{session}_BSM_{gender}_only\", f\"{FLAGS.pca_fraction:.0%}-PCA-{feature}_r\"\n                        ] = r\n                        rsa_corr_df.loc[\n                            f\"{session}_BSM_{gender}_only\", f\"{FLAGS.pca_fraction:.0%}-PCA-{feature}_p\"\n                        ] = p\n\n            # Correlations within gender (non-exclusive trials)\n            for gender, slice_it in zip(\n                params.GENDERS, [slice(0, n_female, 1), slice(n_female, None, 1)], strict=True\n            ):\n                r, p = corr_func(\n                    vectorize_similarity_matrix(sim_mat_all[slice_it, slice_it]),\n                    vectorize_similarity_matrix(feat_sim_mat if gender_feat else feat_sim_mat[slice_it, slice_it]),\n                )\n                msg = (\n                    f\"{corr_name} correlation between similarity judgments of {session} &amp; {feature} features in \"\n                    f\"{gender}s: r={r:.3f}, p&lt;={p:.5g}\"\n                )\n                logger.info(msg=msg)\n\n                # Save results to file\n                rsa_corr_df.loc[f\"{session}_BSM_{gender}\", f\"{feature}_r\"] = r\n                rsa_corr_df.loc[f\"{session}_BSM_{gender}\", f\"{feature}_p\"] = p\n\n                if pca_feat_sim_mat is not None:\n                    r, p = corr_func(\n                        vectorize_similarity_matrix(sim_mat_all[slice_it, slice_it]),\n                        vectorize_similarity_matrix(\n                            pca_feat_sim_mat if gender_feat else pca_feat_sim_mat[slice_it, slice_it]\n                        ),\n                    )\n                    msg = (\n                        f\"{corr_name} correlation between similarity judgments of {session} &amp; \"\n                        f\"{FLAGS.pca_fraction:.0%}-PCA-{feature} features in {gender}s: r={r:.3f}, p&lt;={p:.5g}\"\n                    )\n                    logger.info(msg=msg)\n\n                    # Save results to file\n                    rsa_corr_df.loc[f\"{session}_BSM_{gender}\", f\"{FLAGS.pca_fraction:.0%}-PCA-{feature}_r\"] = r\n                    rsa_corr_df.loc[f\"{session}_BSM_{gender}\", f\"{FLAGS.pca_fraction:.0%}-PCA-{feature}_p\"] = p\n\n    # Save correlation results to file\n    if FLAGS.save_corr:\n        p2_corr_df = Path(paths.results.main.rsa, f\"{corr_name}_{FLAGS.metric}.csv\")\n        if p2_corr_df.is_file():\n            cprint(string=f\"Correlation file '{p2_corr_df}' already exists. It will be overwritten ...\", col=\"y\")\n        rsa_corr_df.to_csv(p2_corr_df, float_format=\"%.6g\")\n\n    # Plot RSA correlation dataframe\n    if FLAGS.plot:\n        plot_rsa_corr_df(corr_name=corr_name, metric=FLAGS.metric, corr_df=rsa_corr_df, save=FLAGS.save_plots)\n        plot_vgg_correlations(\n            corr_name=corr_name, metric=FLAGS.metric, data_mode=\"3d-reconstructions\", max_r=1.0, save=FLAGS.save_plots\n        )\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.plot_rsa_corr_df","title":"plot_rsa_corr_df","text":"<pre><code>plot_rsa_corr_df(\n    corr_name: str,\n    metric: str,\n    corr_df: DataFrame | None,\n    save: bool = False,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Plot the RSA correlation dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>corr_name</code> <code>str</code> <p>name of correlation to use (\"Pearson\", \"Spearman\")</p> required <code>metric</code> <code>str</code> <p>similarity metric to use (\"cosine\", \"euclidean\")</p> required <code>corr_df</code> <code>DataFrame | None</code> <p>RSA correlation dataframe</p> required <code>save</code> <code>bool</code> <p>whether to save the plot</p> <code>False</code> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def plot_rsa_corr_df(corr_name: str, metric: str, corr_df: pd.DataFrame | None, save: bool = False, **kwargs) -&gt; None:\n    \"\"\"\n    Plot the RSA correlation dataframe.\n\n    :param corr_name: name of correlation to use (\"Pearson\", \"Spearman\")\n    :param metric: similarity metric to use (\"cosine\", \"euclidean\")\n    :param corr_df: RSA correlation dataframe\n    :param save: whether to save the plot\n    \"\"\"\n    if corr_df is None:\n        corr_df = get_corr_df_rsm(corr_name=corr_name, metric=metric)\n\n    # Get max empirical R\n    mer = (\n        pd.read_csv(\n            paths.results.main.noise_ceiling.r_table.format(corr_name=corr_name.lower()),\n            index_col=[\"session\", \"sample_type\"],\n        )\n        .loc[(\"both\", \"multi-sub-sample\")]\n        .mean_r\n    )\n\n    # Plot correlations between BSMs and physical / computational face features\n    additional_sim_mat = kwargs.pop(\"additional_sim_mat\", False)\n    for bsm_name, corr_row in corr_df.iterrows():\n        if not additional_sim_mat and str(bsm_name).endswith(\"_sim_mat\"):\n            continue\n        sess = str(bsm_name).split(\"_\")[0]\n        other_sess = [s for s in params.SESSIONS if s != sess].pop()\n        exclusive = \"only\" in bsm_name\n        gender = \"female\" if \"female\" in bsm_name else \"male\" if \"_male\" in bsm_name else None\n        tmp_corr_row = corr_row.dropna()\n\n        # Filter columns / variables to plot\n        #   Remove PCA columns &amp; SPoSE columns\n        r_cols = [c for c in tmp_corr_row.index if c.endswith(\"_r\") and \"PCA\" not in c and \"SPoSE\" not in c]\n\n        #   Remove other-session columns but not other BSM\n        r_cols = [c for c in r_cols if \"BSM\" in c or f\"_{other_sess}_\" not in c]\n\n        #   Filter for exclusive gender trials if required\n        r_cols = [c for c in r_cols if \"only\" in c] if exclusive else [c for c in r_cols if \"only\" not in c]\n        if exclusive:\n            r_cols = [c for c in r_cols if f\"_{gender}_\" in c]\n\n        #   Filter for VGG columns based on 3D-reconstructions\n        r_cols = [c for c in r_cols if \"VGG_org\" not in c]\n\n        #   Find three VGG columns with the highest correlation\n        top_3_vgg = tmp_corr_row[[c for c in r_cols if c.startswith(\"VGG_\")]].sort_values(ascending=False)[:3]\n        #   Remove other VGG columns from r_cols\n        r_cols = [c for c in r_cols if (not c.startswith(\"VGG_\") or c in top_3_vgg)]\n\n        #   Separate VGGFaceHumanjudgmentFrozenCore columns\n        r_vgghj_cols = [c for c in r_cols if \"VGGFaceHumanjudgment\" in c]\n        r_vgghj_cols_replace = [\"VGG HJ\" + c.split(\"_VGGFaceHumanjudgmentFrozenCore\")[1] for c in r_vgghj_cols]\n\n        plt.figure(figsize=(10, 8))\n        h = tmp_corr_row[r_cols].plot(\n            kind=\"bar\",\n            title=bsm_name,\n            color=[\"#081C22\"]  # BSM\n            + [\"#1E7872\"]  # CFD PFF\n            + [\"#F4C096\"] * 6  # DECA\n            + [\"#EE2E33\"] * 3  # VGG-Face\n            + [\"#6C4179\"] * 1  # VICE\n            + [\"#008080\"] * 2,  # VGGFaceHumanjudgmentFrozenCore\n        )\n\n        h.set_xticklabels(\n            labels=[\n                c.replace(r_vgghj_cols[0], r_vgghj_cols_replace[0])\n                .replace(r_vgghj_cols[1], r_vgghj_cols_replace[1])\n                .removesuffix(\"_r\")\n                .replace(\"_\", \" \")\n                .replace(\" EXP\", \" EXPRESSION\")\n                .replace(\" CAM\", \" CAMERA\")\n                .replace(\" TEX\", \" TEXTURE\")\n                .replace(\"3D-recon\", \"\")\n                .replace(\" inner\", \"\")  # in case of PLOT_VICE_INNER\n                for c in r_cols\n            ],\n            rotation=30,\n            ha=\"right\",\n            fontdict={\"size\": 12},\n        )\n        ticks_loc = h.get_yticks().tolist()\n        h.yaxis.set_major_locator(FixedLocator(ticks_loc))\n        h.set_yticklabels(labels=[f\"{t:.1f}\" for t in h.get_yticks()], fontdict={\"size\": 12})\n        h.set_ylim(0, 1)\n        h.set_ylabel(\"Correlation R\", fontdict={\"size\": 14})\n        h.set_title(\n            f\"Correlation between {str(bsm_name).replace('_', ' ')} &amp; other face features\", fontdict={\"size\": 16}\n        )\n        # Add horizontal line for max empirical R\n        h.axhline(y=mer, color=\"r\", linestyle=\"--\", alpha=0.5)  # , label=\"Max empirical R\"\n        plt.tight_layout()\n\n        # Save figure\n        if save:\n            for ext in [\".png\", \".svg\"]:\n                plt.savefig(\n                    Path(paths.results.main.rsa, f\"{corr_name.title()}_{bsm_name}-FaceFeats_{metric}\").with_suffix(ext)\n                )\n            plt.close()\n        else:\n            plt.show()\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.plot_vgg_correlations","title":"plot_vgg_correlations","text":"<pre><code>plot_vgg_correlations(\n    corr_name: str,\n    metric: str,\n    data_mode: str,\n    max_r: float | None = None,\n    save: bool = True,\n) -&gt; None\n</code></pre> <p>Plot correlations between similarity matrices.</p> <p>Parameters:</p> Name Type Description Default <code>corr_name</code> <code>str</code> <p>name of correlation to use (\"Pearson\", \"Spearman\")</p> required <code>metric</code> <code>str</code> <p>similarity metric to use (\"cosine\", \"euclidean\")</p> required <code>data_mode</code> <code>str</code> <p>data mode (\"2d-original\", \"3d-reconstructions\", \"3d-perspectives\")</p> required <code>max_r</code> <code>float | None</code> <p>maximum correlation value for limit of y-axis</p> <code>None</code> <code>save</code> <code>bool</code> <p>whether to save the plot</p> <code>True</code> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def plot_vgg_correlations(\n    corr_name: str, metric: str, data_mode: str, max_r: float | None = None, save: bool = True\n) -&gt; None:\n    \"\"\"\n    Plot correlations between similarity matrices.\n\n    :param corr_name: name of correlation to use (\"Pearson\", \"Spearman\")\n    :param metric: similarity metric to use (\"cosine\", \"euclidean\")\n    :param data_mode: data mode (\"2d-original\", \"3d-reconstructions\", \"3d-perspectives\")\n    :param max_r: maximum correlation value for limit of y-axis\n    :param save: whether to save the plot\n    \"\"\"\n    corr_name = corr_name.lower()\n    metric = metric.lower()\n    data_mode = data_mode.lower()\n    if \"3d-persp\" in data_mode:\n        msg = \"3D-persp data mode is not implemented yet.\"\n        raise NotImplementedError(msg)\n\n    corr_df = get_corr_df_rsm(corr_name=corr_name, metric=metric)\n\n    if max_r is None:\n        max_r = corr_df.loc[:, [c for c in corr_df.columns if \"_p\" not in c and \"VGG_\" in c]].max().max()\n\n    # Filter VGG columns\n    ipt = \"org_\" if \"original\" in data_mode else \"3D-recon_\" if \"3d-recon\" in data_mode else \"3D-persp_\"\n    non_ipt = \"3D-recon_\" if \"original\" in data_mode else \"org_\"\n    vgg_cols = [c for c in corr_df.columns if (\"VGG_\" in c and \"PCA\" not in c and \"_p\" not in c and non_ipt not in c)]\n\n    for bsm_name, other_sim_mat in corr_df.iterrows():\n        if str(bsm_name).endswith(\"D_BSM\"):\n            # All trials (i.e., no gender-only trials)\n            tmp_vgg_cols = [c for c in vgg_cols if \"male_\" not in c]\n            gender_filter = \"\"\n        else:\n            gender_filter = \"_\" + str(bsm_name).split(\"BSM_\")[-1]\n            gender_filter += \"_only\" if not gender_filter.endswith(\"_only\") else \"\"\n            tmp_vgg_cols = [c for c in vgg_cols if gender_filter in c]\n\n        sub_df = other_sim_mat[tmp_vgg_cols].copy()\n\n        fig, ax = plt.subplots(figsize=(12, 8))\n        h = sns.barplot(\n            x=tmp_vgg_cols,\n            y=sub_df.values,\n            ax=ax,\n            palette=sns.color_palette(palette=\"flare\", as_cmap=False, n_colors=len(tmp_vgg_cols)),  # \"dark:#5A9_r\"\n        )\n\n        h.set_xticklabels(\n            labels=[c.removeprefix(f\"VGG_{ipt}\").removesuffix(f\"{gender_filter}_r\") for c in tmp_vgg_cols],\n            rotation=45,\n            ha=\"right\",\n        )\n        h.set_ylim(0, max_r)\n        h.set_ylabel(\"Correlation R\", fontsize=14)\n        h.set_xlabel(\"VGGFace layers\", fontsize=14)\n        h.set_title(f\"Correlation between {bsm_name} and VGGFace feature maps (input: {ipt.removesuffix('_')})\")\n        fig.tight_layout()\n\n        if save:\n            for ext in [\".png\", \".svg\"]:\n                fig.savefig(\n                    Path(\n                        paths.results.main.rsa,\n                        f\"VGG_{ipt.removesuffix('_')}_feat-{metric}-sim_{bsm_name}_{corr_name.lower()}_corr\",\n                    ).with_suffix(ext)\n                )\n            plt.close()\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.similarity_judgments_of_single_participant","title":"similarity_judgments_of_single_participant","text":"<pre><code>similarity_judgments_of_single_participant(\n    ppid: str,\n    pilot: bool = PILOT,\n    split_return: bool = False,\n) -&gt; ndarray | tuple[ndarray, ndarray]\n</code></pre> <p>Compute a face similarity matrix from a single participant's behavioral data.</p> <p>Parameters:</p> Name Type Description Default <code>ppid</code> <code>str</code> <p>ID of participant</p> required <code>pilot</code> <code>bool</code> <p>True: use the pilot-data</p> <code>PILOT</code> <code>split_return</code> <code>bool</code> <p>split the return in judgments and counts for aggregation across participants</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray | tuple[ndarray, ndarray]</code> <p>either aggregated matrix of similarity judgments OR split in judgments and counts</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def similarity_judgments_of_single_participant(\n    ppid: str, pilot: bool = params.PILOT, split_return: bool = False\n) -&gt; np.ndarray | tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute a face similarity matrix from a single participant's behavioral data.\n\n    :param ppid: ID of participant\n    :param pilot: True: use the pilot-data\n    :param split_return: split the return in judgments and counts for aggregation across participants\n    :return: either aggregated matrix of similarity judgments OR split in judgments and counts\n    \"\"\"\n    if pilot:\n        tab = read_pilot_data(clean_trials=True, verbose=False)\n        tab = tab.loc[tab.ppid == ppid]  # reduce table to given PID\n    else:\n        tab = read_trial_results_of_participant(ppid=ppid, clean_trials=True, verbose=False)\n\n    return compute_similarity_matrix_from_human_judgments(\n        trial_results_table=tab, pilot=pilot, split_return=split_return\n    )\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.vectorize_similarity_matrix","title":"vectorize_similarity_matrix","text":"<pre><code>vectorize_similarity_matrix(\n    face_sim_mat: ndarray,\n) -&gt; ndarray\n</code></pre> <p>Take the upper triangle of a given similarity matrix and return it as vector.</p> Ways to vectorize a matrix <pre><code>    a = [[1 2 3]\n         [4 5 6]\n         [7 8 9]]\n\n    # This is how the diagonal can be excluded (as it is required here)\n    print(a[np.triu_indices(n=3, k=1)])\n    # &gt; array([2, 3, 6])\n\n    # In contrast, see how the diagonal can be included (as it is not done here):\n    print(a[np.triu_indices(n=3, k=0)])\n    # &gt; array([1, 2, 3, 5, 6, 9])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>face_sim_mat</code> <code>ndarray</code> <p>face similarity matrix</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>1d vector of upper triangle</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def vectorize_similarity_matrix(face_sim_mat: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Take the upper triangle of a given similarity matrix and return it as vector.\n\n    ??? note \"Ways to vectorize a matrix\"\n        ```python\n            a = [[1 2 3]\n                 [4 5 6]\n                 [7 8 9]]\n\n            # This is how the diagonal can be excluded (as it is required here)\n            print(a[np.triu_indices(n=3, k=1)])\n            # &gt; array([2, 3, 6])\n\n            # In contrast, see how the diagonal can be included (as it is not done here):\n            print(a[np.triu_indices(n=3, k=0)])\n            # &gt; array([1, 2, 3, 5, 6, 9])\n        ```\n\n    :param face_sim_mat: face similarity matrix\n    :return: 1d vector of upper triangle\n    \"\"\"\n    return face_sim_mat[np.triu_indices(n=face_sim_mat.shape[0], k=1)]\n</code></pre>"},{"location":"reference/modeling/rsa/#facesim3d.modeling.rsa.visualise_matrix","title":"visualise_matrix","text":"<pre><code>visualise_matrix(\n    face_sim_mat: ndarray,\n    session: str,\n    ppid: str | None = None,\n    pilot: bool = PILOT,\n    use_rsatoolbox: bool = False,\n    save: bool = False,\n    **kwargs\n) -&gt; str | Figure\n</code></pre> <p>Visualize face similarity judgments.</p> <p>Parameters:</p> Name Type Description Default <code>face_sim_mat</code> <code>ndarray</code> <p>matrix of face similarity judgments of given participant</p> required <code>ppid</code> <code>str | None</code> <p>ID of participant OR 'all'</p> <code>None</code> <code>pilot</code> <code>bool</code> <p>True: use pilot data</p> <code>PILOT</code> <code>session</code> <code>str</code> <p>'2D', OR '3D'</p> required <code>use_rsatoolbox</code> <code>bool</code> <p>plot with rsatoolbox</p> <code>False</code> <code>save</code> <code>bool</code> <p>save figure</p> <code>False</code> <p>Returns:</p> Type Description <code>str | Figure</code> <p>None</p> Source code in <code>code/facesim3d/modeling/rsa.py</code> <pre><code>def visualise_matrix(\n    face_sim_mat: np.ndarray,\n    session: str,\n    ppid: str | None = None,\n    pilot: bool = params.PILOT,\n    use_rsatoolbox: bool = False,\n    save: bool = False,\n    **kwargs,\n) -&gt; str | plt.Figure:\n    \"\"\"\n    Visualize face similarity judgments.\n\n    :param face_sim_mat: matrix of face similarity judgments of given participant\n    :param ppid: ID of participant OR 'all'\n    :param pilot: True: use pilot data\n    :param session: '2D', OR '3D'\n    :param use_rsatoolbox: plot with rsatoolbox\n    :param save: save figure\n    :return: None\n    \"\"\"\n    # Plot matrix\n    if ppid is None:\n        fig_name = kwargs.pop(\"fig_name\", f\"Aggregated similarity judgments in {session}-session\")\n    else:\n        fig_name = kwargs.pop(\"fig_name\", f\"Similarity judgments of PID {ppid} in {session}-session\")\n\n    # Compute size of the figure\n    figsize = kwargs.pop(\n        \"figsize\",\n        (\n            round(face_sim_mat.shape[1] / min(face_sim_mat.shape) * 10),  # keep x-axis longer since we add colorbar\n            round(face_sim_mat.shape[0] / min(face_sim_mat.shape) * 9),\n        ),\n    )\n\n    if use_rsatoolbox:\n        # Explore rsatoolbox:\n        #  data = rsatoolbox.data.Dataset(np.random.rand(10, 5))  # noqa: ERA001\n        #  rdms = rsatoolbox.rdm.calc_rdm(data)  # noqa: ERA001\n        # This is not ideal for our case, since it works with data with shape of (observations x channels).\n        # With the following vectorized version, the visualization works ...\n        rdms = rsatoolbox.rdm.RDMs(dissimilarities=vectorize_similarity_matrix(face_sim_mat=face_sim_mat))\n\n        # TODO: Set labels (this is not functional yet)  # noqa: FIX002\n        if \"pattern_descriptor\" in kwargs:\n            rdms.pattern_descriptors.update(\n                {\"labels\": heads_naming_converter_table(pilot_version=2 if pilot else None).head_nr.to_list()}\n            )\n            rdms.pattern_descriptors.update(\n                {\"index\": np.arange(params.pilot.v2.n_faces if pilot else params.main.n_faces)}\n            )\n\n        fig, ax_array, _ = rsatoolbox.vis.show_rdm(\n            rdm=rdms,\n            show_colorbar=\"panel\",\n            vmin=kwargs.pop(\"vmin\", 0.0),\n            vmax=kwargs.pop(\"vmax\", 1.0),\n            figsize=figsize,\n            rdm_descriptor=fig_name,\n            num_pattern_groups=face_sim_mat.shape[0] / 2 if face_sim_mat.shape[0] % 2 == 0 else None,\n            pattern_descriptor=kwargs.pop(\"pattern_descriptor\", None),  # labels OR index\n        )  # cmap=\"viridis\"\n        # plt.tight_layout()  # noqa: ERA001\n\n        # Set labels and title\n        msg = \"Not implemented for rsatoolbox. Use its pattern_descriptor instead.\"\n        if \"xticklabels\" in kwargs:\n            raise NotImplementedError(msg)\n        if \"yticklabels\" in kwargs:\n            raise NotImplementedError(msg)\n        if \"xlabel\" in kwargs:\n            ax_array[0][0].set_xlabel(kwargs.pop(\"xlabel\"))\n        if \"ylabel\" in kwargs:\n            ax_array[0][0].set_ylabel(kwargs.pop(\"ylabel\"))\n        if \"title\" in kwargs:\n            ax_array[0][0].set_title(kwargs.pop(\"title\"), pad=10)\n\n    else:\n        fig, ax1 = plt.subplots(num=fig_name, figsize=figsize, ncols=1)\n        pos = ax1.imshow(face_sim_mat, cmap=kwargs.pop(\"cmap\", None))\n        # cmap='magma' OR 'inferno', interpolation='none')\n        fig.colorbar(pos, ax=ax1)\n\n        # Set labels and title\n        if \"xticklabels\" in kwargs:\n            ax1.set_xticks(range(len(kwargs[\"xticklabels\"])))\n            ax1.set_xticklabels(kwargs.pop(\"xticklabels\"), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n        if \"yticklabels\" in kwargs:\n            ax1.set_yticks(range(len(kwargs[\"yticklabels\"])))\n            ax1.set_yticklabels(kwargs.pop(\"yticklabels\"), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n        if \"xlabel\" in kwargs:\n            ax1.set_xlabel(kwargs.pop(\"xlabel\"))\n        if \"ylabel\" in kwargs:\n            ax1.set_ylabel(kwargs.pop(\"ylabel\"))\n        if \"title\" in kwargs:\n            ax1.set_title(kwargs.pop(\"title\"), pad=10)\n\n    if save:\n        # Save figure\n        p2save = Path(kwargs.pop(\"save_path\", paths.results.pilot.v2.rdms if pilot else paths.results.main.rdms))\n        p2save.mkdir(parents=True, exist_ok=True)\n        for ext in [\"png\", \"svg\"]:\n            p2_save_file = p2save / f\"{fig_name}{'_rsatoolbox' if use_rsatoolbox else ''}.{ext}\"\n            cprint(string=f\"Saving figure in {p2_save_file} ... \", col=\"b\")\n            plt.savefig(fname=p2_save_file, dpi=300, format=ext)\n        plt.close()\n        return str(p2_save_file)\n\n    plt.show(block=False)\n    return fig\n</code></pre>"},{"location":"reference/modeling/FLAME/","title":"Index","text":""},{"location":"reference/modeling/FLAME/#facesim3d.modeling.FLAME","title":"FLAME","text":"<p>Init for FLAME model params.</p> <p>Modules:</p> Name Description <code>extract_flame_params</code> <p>Extract FLAME parameters from 3D face models.</p>"},{"location":"reference/modeling/FLAME/extract_flame_params/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> extract_flame_params","text":""},{"location":"reference/modeling/FLAME/extract_flame_params/#facesim3d.modeling.FLAME.extract_flame_params","title":"extract_flame_params","text":"<p>Extract FLAME parameters from 3D face models.</p> <p>FLAME model</p> <p>Check out the interactive FLAME model viewer online:</p> <p>https://flame.is.tue.mpg.de/interactivemodelviewer.html</p> <p>Functions:</p> Name Description <code>get_flame_params</code> <p>Get FLAME parameters (e.g., shape) from the list of head models.</p> <code>load_deca_params</code> <p>Load estimated FLAME parameters of the DECA model.</p> <code>load_flame_params</code> <p>Load FLAME parameters of the original FLAME implementation.</p>"},{"location":"reference/modeling/FLAME/extract_flame_params/#facesim3d.modeling.FLAME.extract_flame_params.get_flame_params","title":"get_flame_params","text":"<pre><code>get_flame_params(\n    list_of_head_nrs: list[str | int],\n    param: str,\n    model: str = \"deca\",\n) -&gt; DataFrame\n</code></pre> <p>Get FLAME parameters (e.g., shape) from the list of head models.</p> Source code in <code>code/facesim3d/modeling/FLAME/extract_flame_params.py</code> <pre><code>def get_flame_params(list_of_head_nrs: list[str | int], param: str, model: str = \"deca\") -&gt; pd.DataFrame:\n    \"\"\"Get FLAME parameters (e.g., shape) from the list of head models.\"\"\"\n    model = model.lower()\n    assert model in [\"deca\", \"flame\"], \"Model must be either 'deca' or 'flame'.\"  # noqa: S101\n    assert param in latent_flame_code + (  # noqa: S101\n        [\"detail\"] if model == \"deca\" else []\n    ), f\"Parameter must be one of {latent_flame_code}.\"\n\n    param_loader = load_deca_params if model == \"deca\" else load_flame_params  # select loader\n    param_length = param_loader(\"Head1\")[param].shape[-1]  # get length of parameter from first head\n    df_param = pd.DataFrame(\n        index=list_of_head_nrs, columns=[f\"{param.title()[0]}{i:03d}\" for i in range(param_length)]\n    )\n\n    # Fill df with FLAME shape params into df\n    for head_nr in list_of_head_nrs:\n        df_param.loc[head_nr, :] = param_loader(head_nr)[param]\n\n    return df_param\n</code></pre>"},{"location":"reference/modeling/FLAME/extract_flame_params/#facesim3d.modeling.FLAME.extract_flame_params.load_deca_params","title":"load_deca_params","text":"<pre><code>load_deca_params(head_idx_or_nr: str | int)\n</code></pre> <p>Load estimated FLAME parameters of the DECA model.</p> Source code in <code>code/facesim3d/modeling/FLAME/extract_flame_params.py</code> <pre><code>def load_deca_params(head_idx_or_nr: str | int):\n    \"\"\"Load estimated FLAME parameters of the DECA model.\"\"\"\n    head_idx = head_nr_to_index(head_idx_or_nr) if str(head_idx_or_nr).startswith(\"Head\") else head_idx_or_nr\n    return np.load(\n        Path(paths.data.facemodel.deca, f\"{head_idx}_inputs_codedict_orig_numpy.npy\"), allow_pickle=True\n    ).item()\n</code></pre>"},{"location":"reference/modeling/FLAME/extract_flame_params/#facesim3d.modeling.FLAME.extract_flame_params.load_flame_params","title":"load_flame_params","text":"<pre><code>load_flame_params(head_idx_or_nr: str | int)\n</code></pre> <p>Load FLAME parameters of the original FLAME implementation.</p> Source code in <code>code/facesim3d/modeling/FLAME/extract_flame_params.py</code> <pre><code>def load_flame_params(head_idx_or_nr: str | int):\n    \"\"\"Load FLAME parameters of the original FLAME implementation.\"\"\"\n    head_idx = head_nr_to_index(head_idx_or_nr) if str(head_idx_or_nr).startswith(\"Head\") else head_idx_or_nr\n    return np.load(Path(paths.data.facemodel.flame, f\"{head_idx}_inputs_FLAMEfit.npy\"), allow_pickle=True).item()\n</code></pre>"},{"location":"reference/modeling/VGG/","title":"Index","text":""},{"location":"reference/modeling/VGG/#facesim3d.modeling.VGG","title":"VGG","text":"<p>Init of the <code>VGG-Face</code>-based modeling pipeline.</p> <p>Modules:</p> Name Description <code>convert_weights</code> <p>Convert <code>VGG-Face</code> weights from <code>LuaTorch</code> to <code>PyTorch</code>.</p> <code>extract_vgg_feature_maps</code> <p>Extract feature maps from <code>VGG-Face</code> elicited by the original and 3D-reconstructed heads in the <code>CFD</code>.</p> <code>models</code> <p>Collection comprising adaptations of the <code>VGG-Face</code> model.</p> <code>prepare_data</code> <p>Prepare data for <code>VGG</code> models.</p> <code>vgg_predict</code> <p>Adapt <code>VGG-Face</code> as the core model to predict human judgments in the face similarity task.</p>"},{"location":"reference/modeling/VGG/convert_weights/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> convert_weights","text":""},{"location":"reference/modeling/VGG/convert_weights/#facesim3d.modeling.VGG.convert_weights","title":"convert_weights","text":"<p>Convert <code>VGG-Face</code> weights from <code>LuaTorch</code> to <code>PyTorch</code>.</p> <p>Note</p> <p>This is an adaptation of: https://github.com/chi0tzp/PyVGGFace/blob/master/convert_weights.py.</p> <p>Author: Simon M. Hofmann Years: 2023</p> <p>Functions:</p> Name Description <code>convert_weights</code> <p>Convert <code>LuaTorch</code> weights and load them into a <code>PyTorch</code> model.</p> <code>download_torch_weights</code> <p>Download <code>torch</code> weights for <code>VGG-Face</code> if necessary.</p>"},{"location":"reference/modeling/VGG/convert_weights/#facesim3d.modeling.VGG.convert_weights.convert_weights","title":"convert_weights","text":"<pre><code>convert_weights(\n    path_to_weights: str | Path, model: VGGFace\n) -&gt; VGGFace\n</code></pre> <p>Convert <code>LuaTorch</code> weights and load them into a <code>PyTorch</code> model.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_weights</code> <code>str | Path</code> <p>filename of the pre-trained <code>LuaTorch</code> weights file [str]</p> required <code>model</code> <code>VGGFace</code> <p><code>VGGFace</code> model</p> required <p>Returns:</p> Type Description <code>VGGFace</code> <p><code>VGGFace</code> model with loaded weights</p> Source code in <code>code/facesim3d/modeling/VGG/convert_weights.py</code> <pre><code>def convert_weights(path_to_weights: str | Path, model: VGGFace) -&gt; VGGFace:\n    \"\"\"\n    Convert `LuaTorch` weights and load them into a `PyTorch` model.\n\n    :param path_to_weights: filename of the pre-trained `LuaTorch` weights file [str]\n    :param model: `VGGFace` model\n    :return: `VGGFace` model with loaded weights\n    \"\"\"\n    torch_model = torchfile.load(path_to_weights)\n    counter = 1\n    block = 1\n    block_size = [2, 2, 3, 3, 3]\n    block_cut: int = 5\n    for _i, layer in enumerate(torch_model.modules):\n        if layer.weight is not None:\n            if block &lt;= block_cut:\n                self_layer = model.features[f\"conv_{block}_{counter}\"]\n                counter += 1\n                if counter &gt; block_size[block - 1]:\n                    counter = 1\n                    block += 1\n                self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n                self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n            else:\n                self_layer = model.fc[f\"fc{block}\"]\n                block += 1\n                self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n                self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n    return model\n</code></pre>"},{"location":"reference/modeling/VGG/convert_weights/#facesim3d.modeling.VGG.convert_weights.download_torch_weights","title":"download_torch_weights","text":"<pre><code>download_torch_weights() -&gt; Path\n</code></pre> <p>Download <code>torch</code> weights for <code>VGG-Face</code> if necessary.</p> Source code in <code>code/facesim3d/modeling/VGG/convert_weights.py</code> <pre><code>def download_torch_weights() -&gt; Path:\n    \"\"\"Download `torch` weights for `VGG-Face` if necessary.\"\"\"\n    # Create output directory (if necessary)\n    p2_model = Path(paths.data.models.vggface).parent\n    p2_model.mkdir(parents=True, exist_ok=True)\n\n    p2_torch_weights = Path(paths.data.models.vggface, WEIGHT_FILE)\n\n    if not p2_torch_weights.is_file():\n        torch_tar_file = Path(paths.data.models.vggface).parent / \"vgg_face_torch.tar.gz\"\n        # Download tar.gz file\n        cprint(string=\"Downloading VGG-Face torch weights 'vgg_face_torch.tar.gz'...\", col=\"b\")\n        urllib.request.urlretrieve(\n            url=\"http://www.robots.ox.ac.uk/~vgg/software/vgg_face/src/vgg_face_torch.tar.gz\", filename=torch_tar_file\n        )\n\n        # Extract weight file from tar.gz\n        if not p2_torch_weights.is_file():\n            cprint(string=\"Extracting weights from 'vgg_face_torch.tar.gz'...\", col=\"b\")\n            with tarfile.open(torch_tar_file) as tar_file:\n                tar_file.extract(member=f\"vgg_face_torch/{WEIGHT_FILE}\", path=p2_model)\n            torch_tar_file.unlink()\n\n    return p2_torch_weights\n</code></pre>"},{"location":"reference/modeling/VGG/extract_vgg_feature_maps/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> extract_vgg_feature_maps","text":""},{"location":"reference/modeling/VGG/extract_vgg_feature_maps/#facesim3d.modeling.VGG.extract_vgg_feature_maps","title":"extract_vgg_feature_maps","text":"<p>Extract feature maps from <code>VGG-Face</code> elicited by the original and 3D-reconstructed heads in the <code>CFD</code>.</p> <p>Functions:</p> Name Description <code>extract_activation_maps</code> <p>Extract activation map(s) from model layer(s).</p> <code>extract_vgg_human_judgment_activation_maps_in_core_bridge</code> <p>Extract activation map(s) in the <code>VGG-core-bridge</code> from a <code>VGGFaceHumanjudgment[FrozenCore]</code> model.</p> <code>get_vgg_activation_maps</code> <p>Get activation maps of the <code>VGG-Face</code> model for a list of head models.</p> <code>get_vgg_human_judgment_activation_maps</code> <p>Get activation maps of <code>VGGFaceHumanjudgment[FrozenCore]</code> for a list of head models.</p> <code>plot_activation_maps</code> <p>Plot activation maps.</p>"},{"location":"reference/modeling/VGG/extract_vgg_feature_maps/#facesim3d.modeling.VGG.extract_vgg_feature_maps.extract_activation_maps","title":"extract_activation_maps","text":"<pre><code>extract_activation_maps(\n    model: VGGFace,\n    image_path: str,\n    layer_name: str | None = None,\n    **kwargs\n) -&gt; ndarray | list[ndarray]\n</code></pre> <p>Extract activation map(s) from model layer(s).</p> Source code in <code>code/facesim3d/modeling/VGG/extract_vgg_feature_maps.py</code> <pre><code>def extract_activation_maps(\n    model: VGGFace, image_path: str, layer_name: str | None = None, **kwargs\n) -&gt; np.ndarray | list[np.ndarray]:\n    \"\"\"Extract activation map(s) from model layer(s).\"\"\"\n    model.eval()\n    image = load_image_for_model(image_path=image_path, dtype=torch.double, **kwargs)\n\n    # Forward image through VGGFace\n    with torch.no_grad():\n        _ = model(image)\n\n    if layer_name is None:\n        return [l_out.data.cpu().numpy() for l_out in model.layer_output]\n\n    layer_name = layer_name.lower()\n    if layer_name not in model.layer_names:\n        msg = f\"Layer name '{layer_name}' not in model.layer_names !\"\n        raise ValueError(msg)\n    return model.layer_output[model.layer_names.index(layer_name)].data.cpu().numpy()\n</code></pre>"},{"location":"reference/modeling/VGG/extract_vgg_feature_maps/#facesim3d.modeling.VGG.extract_vgg_feature_maps.extract_vgg_human_judgment_activation_maps_in_core_bridge","title":"extract_vgg_human_judgment_activation_maps_in_core_bridge","text":"<pre><code>extract_vgg_human_judgment_activation_maps_in_core_bridge(\n    model: (\n        VGGFaceHumanjudgment\n        | VGGFaceHumanjudgmentFrozenCore\n    ),\n    model_input: Tensor,\n) -&gt; ndarray | list[ndarray]\n</code></pre> <p>Extract activation map(s) in the <code>VGG-core-bridge</code> from a <code>VGGFaceHumanjudgment[FrozenCore]</code> model.</p> Source code in <code>code/facesim3d/modeling/VGG/extract_vgg_feature_maps.py</code> <pre><code>def extract_vgg_human_judgment_activation_maps_in_core_bridge(\n    model: VGGFaceHumanjudgment | VGGFaceHumanjudgmentFrozenCore,\n    model_input: torch.Tensor,\n) -&gt; np.ndarray | list[np.ndarray]:\n    \"\"\"Extract activation map(s) in the `VGG-core-bridge` from a `VGGFaceHumanjudgment[FrozenCore]` model.\"\"\"\n    model.eval()\n\n    # Forward image through VGGFaceHumanjudgment[FrozenCore]\n    with torch.no_grad():\n        if model.parallel_bridge:\n            # Push the same image through all three parts of the bridge\n            out = [\n                model.forward_vgg(x=model_input, bridge_idx=1).data.cpu().numpy(),\n                model.forward_vgg(x=model_input, bridge_idx=2).data.cpu().numpy(),\n                model.forward_vgg(x=model_input, bridge_idx=3).data.cpu().numpy(),\n            ]\n        else:\n            out = model.forward_vgg(x=model_input, bridge_idx=None).data.cpu().numpy()\n\n    return out\n</code></pre>"},{"location":"reference/modeling/VGG/extract_vgg_feature_maps/#facesim3d.modeling.VGG.extract_vgg_feature_maps.get_vgg_activation_maps","title":"get_vgg_activation_maps","text":"<pre><code>get_vgg_activation_maps(\n    list_of_head_nrs: list[str | int],\n    layer_name: str,\n    data_mode: str,\n) -&gt; DataFrame\n</code></pre> <p>Get activation maps of the <code>VGG-Face</code> model for a list of head models.</p> Source code in <code>code/facesim3d/modeling/VGG/extract_vgg_feature_maps.py</code> <pre><code>def get_vgg_activation_maps(list_of_head_nrs: list[str | int], layer_name: str, data_mode: str) -&gt; pd.DataFrame:\n    \"\"\"Get activation maps of the `VGG-Face` model for a list of head models.\"\"\"\n    layer_name = layer_name.lower()\n    vgg_model = get_vgg_face_model(save_layer_output=True)\n    vgg_model.eval()\n    if layer_name not in vgg_model.layer_names:\n        msg = f\"Layer name '{layer_name}' not in model.\"\n        raise ValueError(msg)\n\n    df_activation_maps = None  # init\n    for head_nr in tqdm(\n        list_of_head_nrs, desc=f\"Extracting activation maps in layer '{layer_name}'\", colour=\"#F79F09\"\n    ):\n        p2_img = face_image_path(head_id=head_nr, data_mode=data_mode, return_head_id=False)\n        act_map = extract_activation_maps(model=vgg_model, image_path=p2_img, layer_name=layer_name)\n        act_map = act_map.flatten()\n\n        if df_activation_maps is None:\n            m_len = len(act_map)\n            df_activation_maps = pd.DataFrame(\n                index=list_of_head_nrs, columns=[f\"{layer_name.upper()}-{i: 0{oom(m_len) + 1}d}\" for i in range(m_len)]\n            )\n\n        df_activation_maps.loc[head_nr, :] = act_map.astype(\"float32\")\n\n    return df_activation_maps\n</code></pre>"},{"location":"reference/modeling/VGG/extract_vgg_feature_maps/#facesim3d.modeling.VGG.extract_vgg_feature_maps.get_vgg_human_judgment_activation_maps","title":"get_vgg_human_judgment_activation_maps","text":"<pre><code>get_vgg_human_judgment_activation_maps(\n    list_of_head_nrs: list[str | int] | Series,\n    session: str,\n    model_name: str,\n    data_mode: str,\n    exclusive_gender_trials: str | None = None,\n) -&gt; DataFrame\n</code></pre> <p>Get activation maps of <code>VGGFaceHumanjudgment[FrozenCore]</code> for a list of head models.</p> Source code in <code>code/facesim3d/modeling/VGG/extract_vgg_feature_maps.py</code> <pre><code>def get_vgg_human_judgment_activation_maps(\n    list_of_head_nrs: list[str | int] | pd.Series,\n    session: str,\n    model_name: str,\n    data_mode: str,\n    exclusive_gender_trials: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Get activation maps of `VGGFaceHumanjudgment[FrozenCore]` for a list of head models.\"\"\"\n    # Get model\n    vgg_hj_model = load_trained_vgg_face_human_judgment_model(\n        session=session,\n        model_name=model_name,\n        exclusive_gender_trials=exclusive_gender_trials,\n        device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n    )\n    vgg_hj_model.eval()\n\n    # Get model data\n    full_dataset_dl, _, _ = prepare_data_for_human_judgment_model(\n        session=session,\n        frozen_core=vgg_hj_model.freeze_vgg_core,\n        data_mode=data_mode,\n        last_core_layer=vgg_hj_model.last_core_layer,\n        split_ratio=(1.0, 0.0, 0.0),  # push all data in one set\n        batch_size=1,\n        num_workers=1,\n        dtype=torch.float32,\n        exclusive_gender_trials=exclusive_gender_trials,\n    )\n\n    df_activation_maps = None  # init\n    for i, i_data in tqdm(\n        enumerate(full_dataset_dl),\n        desc=\"Extracting activation maps in layer 'vgg_core_bridge'\",\n        total=len(full_dataset_dl),\n        colour=\"#F79F09\",\n    ):\n        ipt_1, ipt_2, ipt_3, _, idx = i_data.values()  # _ == choice\n        head_nr_1, head_nr_2, head_nr_3 = (\n            f\"Head{nr}\"\n            for nr in full_dataset_dl.dataset.dataset.session_data.iloc[idx.item()][[\"head1\", \"head2\", \"head3\"]]\n        )\n\n        for head_nr, model_input in zip([head_nr_1, head_nr_2, head_nr_3], [ipt_1, ipt_2, ipt_3], strict=True):\n            if head_nr not in (\n                list_of_head_nrs.to_numpy() if hasattr(list_of_head_nrs, \"to_numpy\") else list_of_head_nrs\n            ):\n                # Skip if the head is not in the list\n                continue\n\n            if df_activation_maps is not None and not df_activation_maps.loc[head_nr].isna().any():\n                # Skip if this is already computed\n                continue\n\n            act_map = extract_vgg_human_judgment_activation_maps_in_core_bridge(\n                model=vgg_hj_model, model_input=model_input\n            )\n            if isinstance(act_map, list):\n                act_map = np.concatenate(act_map)\n            act_map = act_map.flatten()\n            if df_activation_maps is None:\n                m_len = len(act_map)\n                df_activation_maps = pd.DataFrame(\n                    index=list_of_head_nrs, columns=[f\"VGG_CORE_BRIDGE-{i: 0{oom(m_len) + 1}d}\" for i in range(m_len)]\n                )\n\n            df_activation_maps.loc[head_nr, :] = act_map.astype(\"float32\")\n\n    return df_activation_maps\n</code></pre>"},{"location":"reference/modeling/VGG/extract_vgg_feature_maps/#facesim3d.modeling.VGG.extract_vgg_feature_maps.plot_activation_maps","title":"plot_activation_maps","text":"<pre><code>plot_activation_maps(\n    activation_maps: ndarray | list[ndarray],\n    layer_names: str | list[str],\n)\n</code></pre> <p>Plot activation maps.</p> Source code in <code>code/facesim3d/modeling/VGG/extract_vgg_feature_maps.py</code> <pre><code>def plot_activation_maps(activation_maps: np.ndarray | list[np.ndarray], layer_names: str | list[str]):\n    \"\"\"Plot activation maps.\"\"\"\n    # Convert to list (if necessary)\n    if isinstance(activation_maps, np.ndarray):\n        activation_maps = [activation_maps]\n    if isinstance(layer_names, str):\n        layer_names = [layer_names]\n\n    # Pre-process activation maps for plotting\n    processed_layer_outputs = []\n    for lout in activation_maps:\n        feature_map = lout.squeeze(0)\n        gray_scale = np.sum(feature_map, axis=0)\n        gray_scale /= feature_map.shape[0]\n        processed_layer_outputs.append(gray_scale)\n\n    # Plot activation maps\n    n_plots = len([ln for ln in layer_names if \"fc\" not in ln])\n    if n_plots &lt; len(layer_names):\n        print(f\"Some layers are fully connected (fc) layers (n={len(layer_names) - n_plots}) &amp; will not be plotted.\")\n    size_r, size_c = get_n_cols_and_rows(n_plots=n_plots, square=True)\n    fig = plt.figure(figsize=(9, 9))\n    for i, p_lout in enumerate(processed_layer_outputs):\n        if \"fc\" in layer_names[i]:\n            continue\n        a = fig.add_subplot(size_r, size_c, i + 1)\n        _ = plt.imshow(p_lout)\n        a.axis(\"off\")\n        a.set_title(layer_names[i], fontsize=12)\n    plt.tight_layout()\n\n    # TODO: Save figure  # noqa: FIX002\n    pass\n</code></pre>"},{"location":"reference/modeling/VGG/models/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> models","text":""},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models","title":"models","text":"<p>Collection comprising adaptations of the <code>VGG-Face</code> model.</p> <p>Model sources</p> <p>\u25b8 https://www.robots.ox.ac.uk/~vgg/software/vgg_face/ (original model weights but in LuaTorch)</p> <p>\u25b8 https://github.com/chi0tzp/PyVGGFace (most is adopted from here)</p> <p>\u25b8 https://modelzoo.co/model/facenet-pytorch</p> <p>\u25b8 https://www.kaggle.com/code/shubhendumishra/recognizing-faces-in-the-wild-vggface-pytorch</p> <p>\u25b8 Note there is also a second version of VGGFace https://github.com/ox-vgg/vgg_face2</p> <p>Classes:</p> Name Description <code>VGGFace</code> <p>VGGFace class.</p> <code>VGGFaceHumanjudgment</code> <p>An adaptation of the <code>VGG-Face</code> model for human similarity judgments.</p> <code>VGGFaceHumanjudgmentBase</code> <p>Base class for the <code>VGG-Face</code> model for human similarity judgments.</p> <code>VGGFaceHumanjudgmentFrozenCore</code> <p>An adaptation of the <code>VGG-Face</code> model for human similarity judgments, where the <code>VGG core</code> is frozen.</p> <code>VGGFaceHumanjudgmentFrozenCoreOld</code> <p>Old, that is, deprecated frozen-core <code>VGG-Face</code> model for human similarity judgments.</p> <code>VGGFaceHumanjudgmentFrozenCoreWithLegs</code> <p>A model extension to feed face images to the <code>VGGFaceHumanjudgmentFrozenCore</code>.</p> <code>VGGMultiView</code> <p>Original <code>VGG-Face</code> model retrained to predict face IDs from multiple views.</p> <code>VGGcore</code> <p>The <code>VGGcore</code> class is used to extract a core part of the <code>VGGFace</code> model.</p> <p>Functions:</p> Name Description <code>check_exclusive_gender_trials</code> <p>Check the variable <code>exclusive_gender_trials</code>, which is used in different functions.</p> <code>create_conv_decision_block</code> <p>Build a decision block with convolutional layers only.</p> <code>create_fc_bridge</code> <p>Build a bridge between the <code>VGG core</code> and the decision block with fully connected layers.</p> <code>create_fc_decision_block</code> <p>Build a decision block with fully connected (fc) layers.</p> <code>draw_model</code> <p>Draw the computational graph of a given <code>VGG</code> model variant.</p> <code>get_vgg_face_model</code> <p>Get the originally trained <code>VGGFace</code> model.</p> <code>get_vgg_layer_feature</code> <p>Get the output shape of a given <code>VGG</code> layer.</p> <code>get_vgg_layer_names</code> <p>Return a list of layer names constituting the <code>VGGFace</code> model.</p> <code>get_vgg_performance_table</code> <p>Get the performance table for <code>VGGFace</code> models.</p> <code>h_out</code> <p>Calculate the output height of a convolutional layer.</p> <code>load_trained_vgg_face_human_judgment_model</code> <p>Load a trained <code>VGGFaceHumanjudgment</code> model from a file.</p> <code>load_trained_vgg_weights_into_model</code> <p>Load trained weights into the original <code>VGG-Face</code> model.</p> <code>model_summary</code> <p>Create a <code>Tensorflow</code>-like model summary.</p> <code>read_vgg_layer_table</code> <p>Read the table with <code>VGG</code> layer names and corresponding output shapes, and number of parameters.</p> <code>w_out</code> <p>Calculate the output width of a convolutional layer.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFace","title":"VGGFace","text":"<pre><code>VGGFace(save_layer_output: bool = False)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>VGGFace class.</p> <p>This is an reimplementation of the original <code>VGG-Face</code> model in <code>PyTorch</code>.</p> <p>Source: https://github.com/chi0tzp/PyVGGFace/blob/master/lib/vggface.py.</p> <p>Initialize VGGFace model.</p> <p>Parameters:</p> Name Type Description Default <code>save_layer_output</code> <code>bool</code> <p>If True, save the output of each layer in a list.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Methods:</p> Name Description <code>forward</code> <p>Run forward pass through the model <code>VGGFace</code>.</p> <code>reset_layer_output</code> <p>Reset the layer output list (i.e., set it to an empty list).</p> <p>Attributes:</p> Name Type Description <code>layer_names</code> <p>Return list of layer names in <code>VGGFace</code>.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def __init__(self, save_layer_output: bool = False) -&gt; None:\n    \"\"\"\n    Initialize VGGFace model.\n\n    :param save_layer_output: If True, save the output of each layer in a list.\n    :return: None\n    \"\"\"\n    super().__init__()\n\n    self.save_layer_output = save_layer_output\n    self.layer_output = []\n    self._layer_names = []\n\n    self.features = nn.ModuleDict(\n        OrderedDict(\n            {\n                # === Block 1 ===\n                \"conv_1_1\": nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n                \"relu_1_1\": nn.ReLU(inplace=True),\n                \"conv_1_2\": nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n                \"relu_1_2\": nn.ReLU(inplace=True),\n                \"maxp_1_2\": nn.MaxPool2d(kernel_size=2, stride=2),\n                # === Block 2 ===\n                \"conv_2_1\": nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n                \"relu_2_1\": nn.ReLU(inplace=True),\n                \"conv_2_2\": nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n                \"relu_2_2\": nn.ReLU(inplace=True),\n                \"maxp_2_2\": nn.MaxPool2d(kernel_size=2, stride=2),\n                # === Block 3 ===\n                \"conv_3_1\": nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n                \"relu_3_1\": nn.ReLU(inplace=True),\n                \"conv_3_2\": nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n                \"relu_3_2\": nn.ReLU(inplace=True),\n                \"conv_3_3\": nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n                \"relu_3_3\": nn.ReLU(inplace=True),\n                \"maxp_3_3\": nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n                # === Block 4 ===\n                \"conv_4_1\": nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n                \"relu_4_1\": nn.ReLU(inplace=True),\n                \"conv_4_2\": nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n                \"relu_4_2\": nn.ReLU(inplace=True),\n                \"conv_4_3\": nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n                \"relu_4_3\": nn.ReLU(inplace=True),\n                \"maxp_4_3\": nn.MaxPool2d(kernel_size=2, stride=2),\n                # === Block 5 ===\n                \"conv_5_1\": nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n                \"relu_5_1\": nn.ReLU(inplace=True),\n                \"conv_5_2\": nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n                \"relu_5_2\": nn.ReLU(inplace=True),\n                \"conv_5_3\": nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n                \"relu_5_3\": nn.ReLU(inplace=True),\n                \"maxp_5_3\": nn.MaxPool2d(kernel_size=2, stride=2),\n            }\n        )\n    )\n\n    self.fc = nn.ModuleDict(\n        OrderedDict(\n            {\n                \"fc6\": nn.Linear(in_features=512 * 7 * 7, out_features=4096),\n                \"fc6-relu\": nn.ReLU(inplace=True),\n                \"fc6-dropout\": nn.Dropout(p=0.5),\n                \"fc7\": nn.Linear(in_features=4096, out_features=4096),\n                \"fc7-relu\": nn.ReLU(inplace=True),\n                \"fc7-dropout\": nn.Dropout(p=0.5),\n                \"fc8\": nn.Linear(in_features=4096, out_features=2622),\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFace.layer_names","title":"layer_names  <code>property</code>","text":"<pre><code>layer_names\n</code></pre> <p>Return list of layer names in <code>VGGFace</code>.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFace.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Run forward pass through the model <code>VGGFace</code>.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"Run forward pass through the model `VGGFace`.\"\"\"\n    if self.save_layer_output:\n        self.reset_layer_output()\n\n    # Forward through feature layers\n    for layer in self.features.values():\n        x = layer(x)\n        # Append layer output to list\n        if self.save_layer_output:\n            self.layer_output.append(x)\n\n    # Flatten convolution outputs\n    x = x.view(x.size(0), -1)\n\n    # Forward through FC layers\n    if hasattr(self, \"fc\"):  # check for later cut-off models\n        for layer in self.fc.values():\n            x = layer(x)\n            if self.save_layer_output:\n                self.layer_output.append(x)\n\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFace.reset_layer_output","title":"reset_layer_output","text":"<pre><code>reset_layer_output()\n</code></pre> <p>Reset the layer output list (i.e., set it to an empty list).</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def reset_layer_output(self):\n    \"\"\"Reset the layer output list (i.e., set it to an empty list).\"\"\"\n    self.layer_output = []\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgment","title":"VGGFaceHumanjudgment","text":"<pre><code>VGGFaceHumanjudgment(\n    decision_block: str,\n    freeze_vgg_core: bool,\n    last_core_layer: str,\n    parallel_bridge: bool = False,\n    session: str | None = None,\n)\n</code></pre> <p>               Bases: <code>VGGFaceHumanjudgmentBase</code></p> <p>An adaptation of the <code>VGG-Face</code> model for human similarity judgments.</p> <p>The <code>VGGFaceHumanjudgment</code> model consists of three parallel face models (based on <code>VGGcore</code>):</p> <ul> <li>For each trial, each submodel gets one of the face images which are part of the corresponding triplet.</li> <li>The outputs of the three models are combined with linear layer(s) (<code>FC bridge</code>).</li> <li>Weights are shared between the three submodels at the bottom (<code>VGGcore</code> + <code>bridge</code>).</li> <li>Then the concatenated feature maps are pushed through a <code>decision block</code> to predict human choices in a trial.</li> </ul> <p>Compare to: https://github.com/pytorch/examples/blob/main/siamese_network/main.py</p> <p>Initialize the <code>VGGFaceHumanjudgment</code> model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Run the forward pass through the whole model.</p> <code>forward_vgg</code> <p>Run the forward pass through the <code>VGGcore</code> and then through layers of the <code>VGGFaceHumanjudgmentBase</code>.</p> <code>init_weights</code> <p>Initialize weights.</p> <code>requires_grad</code> <p>Return whether a layer requires a gradient flow.</p> <p>Attributes:</p> Name Type Description <code>decision_block</code> <code>ModuleDict</code> <p>Return the decision block of the model.</p> <code>decision_block_mode</code> <code>str</code> <p>Return the decision block mode.</p> <code>last_core_layer</code> <code>str</code> <p>Return the cut layer of the <code>VGGcore</code>, i.e., the last layer before the <code>bridge</code> is attached.</p> <code>layer_names</code> <code>list[str]</code> <p>Return a list of layer names in the model.</p> <code>vgg_core_bridge</code> <p>Return <code>VGG core bridge</code>.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def __init__(\n    self,\n    decision_block: str,\n    freeze_vgg_core: bool,\n    last_core_layer: str,\n    parallel_bridge: bool = False,\n    session: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the `VGGFaceHumanjudgment` model.\"\"\"\n    super().__init__(\n        decision_block=decision_block,\n        freeze_vgg_core=freeze_vgg_core,\n        last_core_layer=last_core_layer,\n        parallel_bridge=parallel_bridge,\n        session=session,\n    )\n    self.vgg_core = VGGcore(freeze_vgg_core=freeze_vgg_core, last_core_layer=last_core_layer, verbose=False)\n    self.vgg_core.float()  # float32 for GPU\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgment.decision_block","title":"decision_block  <code>property</code> <code>writable</code>","text":"<pre><code>decision_block: ModuleDict\n</code></pre> <p>Return the decision block of the model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgment.decision_block_mode","title":"decision_block_mode  <code>property</code> <code>writable</code>","text":"<pre><code>decision_block_mode: str\n</code></pre> <p>Return the decision block mode.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgment.last_core_layer","title":"last_core_layer  <code>property</code> <code>writable</code>","text":"<pre><code>last_core_layer: str\n</code></pre> <p>Return the cut layer of the <code>VGGcore</code>, i.e., the last layer before the <code>bridge</code> is attached.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgment.layer_names","title":"layer_names  <code>property</code>","text":"<pre><code>layer_names: list[str]\n</code></pre> <p>Return a list of layer names in the model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgment.vgg_core_bridge","title":"vgg_core_bridge  <code>property</code> <code>writable</code>","text":"<pre><code>vgg_core_bridge\n</code></pre> <p>Return <code>VGG core bridge</code>.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgment.forward","title":"forward","text":"<pre><code>forward(x1: Tensor, x2: Tensor, x3: Tensor) -&gt; Tensor\n</code></pre> <p>Run the forward pass through the whole model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward(self, x1: torch.Tensor, x2: torch.Tensor, x3: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Run the forward pass through the whole model.\"\"\"\n    x1 = self.forward_vgg(x1, bridge_idx=1 if self.parallel_bridge else None)\n    x2 = self.forward_vgg(x2, bridge_idx=2 if self.parallel_bridge else None)\n    x3 = self.forward_vgg(x3, bridge_idx=3 if self.parallel_bridge else None)\n\n    # Reshape for decision block\n    if self.decision_block_mode == \"fc\":\n        x = torch.stack((x1, x2, x3), dim=1)\n        x = x.view(x.size(0), -1)  # (batch_size, 4096 * 3)\n    elif self.decision_block_mode == \"conv\":\n        x = torch.stack((x1, x2, x1, x3, x2, x3), dim=1)  # all 6 combinations\n        x = x.unsqueeze(1)  # add channel dimensions (batch_size, channel=1, combs=6, x[1|2|3].shape=300|4096)\n    else:\n        msg = f\"Decision block mode '{self.decision_block_mode}' not implemented (yet).\"\n        raise NotImplementedError(msg)\n\n    # Forward through decision layers\n    for layer in self.decision_block.values():\n        x = layer(x)\n\n    # Reshape before release\n    if self.decision_block_mode == \"conv\":\n        x = x.view(x.size(0), -1)\n\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgment.forward_vgg","title":"forward_vgg","text":"<pre><code>forward_vgg(x: Tensor, bridge_idx: int | None) -&gt; Tensor\n</code></pre> <p>Run the forward pass through the <code>VGGcore</code> and then through layers of the <code>VGGFaceHumanjudgmentBase</code>.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward_vgg(self, x: torch.Tensor, bridge_idx: int | None) -&gt; torch.Tensor:\n    \"\"\"Run the forward pass through the `VGGcore` and then through layers of the `VGGFaceHumanjudgmentBase`.\"\"\"\n    x = self.vgg_core(x)\n    x = x.view(x.size(0), -1)  # or ...size()[0], -1), this flattens the tensor\n    # The bridge(s) come(s) from VGGFaceHumanjudgmentBase\n    if self.parallel_bridge:\n        for layer in self.vgg_core_bridge[f\"bridge{bridge_idx}\"].values():\n            x = layer(x)\n    else:\n        for layer in self.vgg_core_bridge.values():\n            x = layer(x)\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgment.init_weights","title":"init_weights  <code>staticmethod</code>","text":"<pre><code>init_weights(m)\n</code></pre> <p>Initialize weights.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>@staticmethod\ndef init_weights(m):\n    \"\"\"Initialize weights.\"\"\"\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        torch.nn.init.constant_(m.bias, 0.01)  # torch.nn.init.zeros_(m.bias)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgment.requires_grad","title":"requires_grad","text":"<pre><code>requires_grad(layer_name: str | None = None) -&gt; None\n</code></pre> <p>Return whether a layer requires a gradient flow.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def requires_grad(self, layer_name: str | None = None) -&gt; None:\n    \"\"\"Return whether a layer requires a gradient flow.\"\"\"\n    found = False  # initialize\n    for param_name, param in self.named_parameters():\n        if layer_name is None or layer_name in param_name:\n            print(f\"'{param_name}' requires grad: {param.requires_grad}\")\n            found = True\n    if not found:\n        cprint(string=f\"There is no layer containing parameters with the name '{layer_name}'.\", col=\"r\")\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentBase","title":"VGGFaceHumanjudgmentBase","text":"<pre><code>VGGFaceHumanjudgmentBase(\n    decision_block: str,\n    freeze_vgg_core: bool,\n    last_core_layer: str,\n    parallel_bridge: bool,\n    session: str | None,\n)\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Base class for the <code>VGG-Face</code> model for human similarity judgments.</p> <ul> <li>the architecture consists of three parallel <code>VGG-Face</code> models.</li> <li>for each trial, each VGG submodel gets one of the faces from the triplet, respectively.</li> <li>weights are shared between the three models</li> <li>we combine the outputs of the three models with linear layer(s) to predict the human choice in the trial</li> </ul> <p>This is similar to: https://github.com/pytorch/examples/blob/main/siamese_network/main.py</p> <p>This base class builds the body for two different variants of the model for human similarity judgments: * VGGFaceHumanjudgment: trained on face images directly. It directs data from VGGcore -&gt; VGGFaceHumanjudgmentBase * VGGFaceHumanjudgmentFrozenCore: trained on activation maps of VGGFace in layer 'maxp_5_3'</p> <p>Initialize VGGFaceHumanjudgmentBase.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Run the forward pass through the whole model.</p> <code>forward_vgg</code> <p>Run the forward pass through <code>VGG core</code> part of the model.</p> <code>init_weights</code> <p>Initialize weights.</p> <code>requires_grad</code> <p>Return whether a layer requires a gradient flow.</p> <p>Attributes:</p> Name Type Description <code>decision_block</code> <code>ModuleDict</code> <p>Return the decision block of the model.</p> <code>decision_block_mode</code> <code>str</code> <p>Return the decision block mode.</p> <code>last_core_layer</code> <code>str</code> <p>Return the cut layer of the <code>VGGcore</code>, i.e., the last layer before the <code>bridge</code> is attached.</p> <code>layer_names</code> <code>list[str]</code> <p>Return a list of layer names in the model.</p> <code>vgg_core_bridge</code> <p>Return <code>VGG core bridge</code>.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def __init__(\n    self,\n    decision_block: str,\n    freeze_vgg_core: bool,\n    last_core_layer: str,\n    parallel_bridge: bool,\n    session: str | None,\n) -&gt; None:\n    \"\"\"Initialize VGGFaceHumanjudgmentBase.\"\"\"\n    super().__init__()\n\n    self._layer_names = []  # initialize\n    self.freeze_vgg_core: bool = freeze_vgg_core\n    self.last_core_layer = last_core_layer\n    self.parallel_bridge = parallel_bridge\n    self.session: str | None = session\n\n    # Replace the last core layer of VGGFace\n    if self.parallel_bridge:\n        self.vgg_core_bridge = nn.ModuleDict(\n            OrderedDict(\n                {\n                    \"bridge1\": create_fc_bridge(last_core_layer=self.last_core_layer),\n                    \"bridge2\": create_fc_bridge(last_core_layer=self.last_core_layer),\n                    \"bridge3\": create_fc_bridge(last_core_layer=self.last_core_layer),\n                }\n            )\n        )\n\n    else:\n        self.vgg_core_bridge = create_fc_bridge(last_core_layer=self.last_core_layer)\n\n    # Create decision block\n    self.decision_block_mode = decision_block\n\n    # Set decision block, too\n    if self.decision_block_mode == \"fc\":\n        self.decision_block = create_fc_decision_block(last_core_layer=self.last_core_layer)\n        self.decision_block.apply(self.init_weights)  # might not be necessary\n    elif self.decision_block_mode == \"conv\":\n        self.decision_block = create_conv_decision_block(last_core_layer=self.last_core_layer)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentBase.decision_block","title":"decision_block  <code>property</code> <code>writable</code>","text":"<pre><code>decision_block: ModuleDict\n</code></pre> <p>Return the decision block of the model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentBase.decision_block_mode","title":"decision_block_mode  <code>property</code> <code>writable</code>","text":"<pre><code>decision_block_mode: str\n</code></pre> <p>Return the decision block mode.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentBase.last_core_layer","title":"last_core_layer  <code>property</code> <code>writable</code>","text":"<pre><code>last_core_layer: str\n</code></pre> <p>Return the cut layer of the <code>VGGcore</code>, i.e., the last layer before the <code>bridge</code> is attached.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentBase.layer_names","title":"layer_names  <code>property</code>","text":"<pre><code>layer_names: list[str]\n</code></pre> <p>Return a list of layer names in the model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentBase.vgg_core_bridge","title":"vgg_core_bridge  <code>property</code> <code>writable</code>","text":"<pre><code>vgg_core_bridge\n</code></pre> <p>Return <code>VGG core bridge</code>.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentBase.forward","title":"forward","text":"<pre><code>forward(x1: Tensor, x2: Tensor, x3: Tensor) -&gt; Tensor\n</code></pre> <p>Run the forward pass through the whole model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward(self, x1: torch.Tensor, x2: torch.Tensor, x3: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Run the forward pass through the whole model.\"\"\"\n    x1 = self.forward_vgg(x1, bridge_idx=1 if self.parallel_bridge else None)\n    x2 = self.forward_vgg(x2, bridge_idx=2 if self.parallel_bridge else None)\n    x3 = self.forward_vgg(x3, bridge_idx=3 if self.parallel_bridge else None)\n\n    # Reshape for decision block\n    if self.decision_block_mode == \"fc\":\n        x = torch.stack((x1, x2, x3), dim=1)\n        x = x.view(x.size(0), -1)  # (batch_size, 4096 * 3)\n    elif self.decision_block_mode == \"conv\":\n        x = torch.stack((x1, x2, x1, x3, x2, x3), dim=1)  # all 6 combinations\n        x = x.unsqueeze(1)  # add channel dimensions (batch_size, channel=1, combs=6, x[1|2|3].shape=300|4096)\n    else:\n        msg = f\"Decision block mode '{self.decision_block_mode}' not implemented (yet).\"\n        raise NotImplementedError(msg)\n\n    # Forward through decision layers\n    for layer in self.decision_block.values():\n        x = layer(x)\n\n    # Reshape before release\n    if self.decision_block_mode == \"conv\":\n        x = x.view(x.size(0), -1)\n\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentBase.forward_vgg","title":"forward_vgg  <code>abstractmethod</code>","text":"<pre><code>forward_vgg(x: Tensor, bridge_idx: int | None) -&gt; Tensor\n</code></pre> <p>Run the forward pass through <code>VGG core</code> part of the model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>@abstractmethod\ndef forward_vgg(self, x: torch.Tensor, bridge_idx: int | None) -&gt; torch.Tensor:\n    \"\"\"Run the forward pass through `VGG core` part of the model.\"\"\"\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentBase.init_weights","title":"init_weights  <code>staticmethod</code>","text":"<pre><code>init_weights(m)\n</code></pre> <p>Initialize weights.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>@staticmethod\ndef init_weights(m):\n    \"\"\"Initialize weights.\"\"\"\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        torch.nn.init.constant_(m.bias, 0.01)  # torch.nn.init.zeros_(m.bias)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentBase.requires_grad","title":"requires_grad","text":"<pre><code>requires_grad(layer_name: str | None = None) -&gt; None\n</code></pre> <p>Return whether a layer requires a gradient flow.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def requires_grad(self, layer_name: str | None = None) -&gt; None:\n    \"\"\"Return whether a layer requires a gradient flow.\"\"\"\n    found = False  # initialize\n    for param_name, param in self.named_parameters():\n        if layer_name is None or layer_name in param_name:\n            print(f\"'{param_name}' requires grad: {param.requires_grad}\")\n            found = True\n    if not found:\n        cprint(string=f\"There is no layer containing parameters with the name '{layer_name}'.\", col=\"r\")\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCore","title":"VGGFaceHumanjudgmentFrozenCore","text":"<pre><code>VGGFaceHumanjudgmentFrozenCore(\n    decision_block: str,\n    last_core_layer: str,\n    parallel_bridge: bool = False,\n    session: str | None = None,\n)\n</code></pre> <p>               Bases: <code>VGGFaceHumanjudgmentBase</code></p> <p>An adaptation of the <code>VGG-Face</code> model for human similarity judgments, where the <code>VGG core</code> is frozen.</p> <p>This model is similar to the <code>VGGFaceHumanjudgment</code> variant, however, the model gets pre-computed activation maps of a given layer (<code>last_core_layer</code>) of <code>VGG-Face</code> as input.</p> <p>These activation maps, from three faces in a trial, are concatenated and pushed through a <code>decision block</code>.</p> <p>Initialize model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Run the forward pass through the whole model.</p> <code>forward_vgg</code> <p>Run the forward pass through <code>VGG bridge(s)</code>.</p> <code>init_weights</code> <p>Initialize weights.</p> <code>requires_grad</code> <p>Return whether a layer requires a gradient flow.</p> <p>Attributes:</p> Name Type Description <code>decision_block</code> <code>ModuleDict</code> <p>Return the decision block of the model.</p> <code>decision_block_mode</code> <code>str</code> <p>Return the decision block mode.</p> <code>last_core_layer</code> <code>str</code> <p>Return the cut layer of the <code>VGGcore</code>, i.e., the last layer before the <code>bridge</code> is attached.</p> <code>layer_names</code> <code>list[str]</code> <p>Return a list of layer names in the model.</p> <code>vgg_core_bridge</code> <p>Return <code>VGG core bridge</code>.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def __init__(\n    self, decision_block: str, last_core_layer: str, parallel_bridge: bool = False, session: str | None = None\n) -&gt; None:\n    \"\"\"Initialize model.\"\"\"\n    super().__init__(\n        decision_block=decision_block,\n        freeze_vgg_core=True,\n        last_core_layer=last_core_layer,\n        parallel_bridge=parallel_bridge,\n        session=session,\n    )\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCore.decision_block","title":"decision_block  <code>property</code> <code>writable</code>","text":"<pre><code>decision_block: ModuleDict\n</code></pre> <p>Return the decision block of the model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCore.decision_block_mode","title":"decision_block_mode  <code>property</code> <code>writable</code>","text":"<pre><code>decision_block_mode: str\n</code></pre> <p>Return the decision block mode.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCore.last_core_layer","title":"last_core_layer  <code>property</code> <code>writable</code>","text":"<pre><code>last_core_layer: str\n</code></pre> <p>Return the cut layer of the <code>VGGcore</code>, i.e., the last layer before the <code>bridge</code> is attached.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCore.layer_names","title":"layer_names  <code>property</code>","text":"<pre><code>layer_names: list[str]\n</code></pre> <p>Return a list of layer names in the model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCore.vgg_core_bridge","title":"vgg_core_bridge  <code>property</code> <code>writable</code>","text":"<pre><code>vgg_core_bridge\n</code></pre> <p>Return <code>VGG core bridge</code>.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCore.forward","title":"forward","text":"<pre><code>forward(x1: Tensor, x2: Tensor, x3: Tensor) -&gt; Tensor\n</code></pre> <p>Run the forward pass through the whole model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward(self, x1: torch.Tensor, x2: torch.Tensor, x3: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Run the forward pass through the whole model.\"\"\"\n    x1 = self.forward_vgg(x1, bridge_idx=1 if self.parallel_bridge else None)\n    x2 = self.forward_vgg(x2, bridge_idx=2 if self.parallel_bridge else None)\n    x3 = self.forward_vgg(x3, bridge_idx=3 if self.parallel_bridge else None)\n\n    # Reshape for decision block\n    if self.decision_block_mode == \"fc\":\n        x = torch.stack((x1, x2, x3), dim=1)\n        x = x.view(x.size(0), -1)  # (batch_size, 4096 * 3)\n    elif self.decision_block_mode == \"conv\":\n        x = torch.stack((x1, x2, x1, x3, x2, x3), dim=1)  # all 6 combinations\n        x = x.unsqueeze(1)  # add channel dimensions (batch_size, channel=1, combs=6, x[1|2|3].shape=300|4096)\n    else:\n        msg = f\"Decision block mode '{self.decision_block_mode}' not implemented (yet).\"\n        raise NotImplementedError(msg)\n\n    # Forward through decision layers\n    for layer in self.decision_block.values():\n        x = layer(x)\n\n    # Reshape before release\n    if self.decision_block_mode == \"conv\":\n        x = x.view(x.size(0), -1)\n\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCore.forward_vgg","title":"forward_vgg","text":"<pre><code>forward_vgg(x: Tensor, bridge_idx: int | None) -&gt; Tensor\n</code></pre> <p>Run the forward pass through <code>VGG bridge(s)</code>.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward_vgg(self, x: torch.Tensor, bridge_idx: int | None) -&gt; torch.Tensor:\n    \"\"\"Run the forward pass through `VGG bridge(s)`.\"\"\"\n    # The bridge(s) come(s) from VGGFaceHumanjudgmentBase\n    if self.parallel_bridge:\n        for layer in self.vgg_core_bridge[f\"bridge{bridge_idx}\"].values():\n            x = layer(x)\n    else:\n        for layer in self.vgg_core_bridge.values():\n            x = layer(x)\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCore.init_weights","title":"init_weights  <code>staticmethod</code>","text":"<pre><code>init_weights(m)\n</code></pre> <p>Initialize weights.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>@staticmethod\ndef init_weights(m):\n    \"\"\"Initialize weights.\"\"\"\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        torch.nn.init.constant_(m.bias, 0.01)  # torch.nn.init.zeros_(m.bias)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCore.requires_grad","title":"requires_grad","text":"<pre><code>requires_grad(layer_name: str | None = None) -&gt; None\n</code></pre> <p>Return whether a layer requires a gradient flow.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def requires_grad(self, layer_name: str | None = None) -&gt; None:\n    \"\"\"Return whether a layer requires a gradient flow.\"\"\"\n    found = False  # initialize\n    for param_name, param in self.named_parameters():\n        if layer_name is None or layer_name in param_name:\n            print(f\"'{param_name}' requires grad: {param.requires_grad}\")\n            found = True\n    if not found:\n        cprint(string=f\"There is no layer containing parameters with the name '{layer_name}'.\", col=\"r\")\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreOld","title":"VGGFaceHumanjudgmentFrozenCoreOld","text":"<pre><code>VGGFaceHumanjudgmentFrozenCoreOld(decision_block: str)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Old, that is, deprecated frozen-core <code>VGG-Face</code> model for human similarity judgments.</p> <p>The model comprises:</p> <ul> <li>The model takes the activation maps of the <code>VGG-Face</code> in layer <code>\"fc7-relu\"</code></li> <li>It gets three activation maps representing three faces, and it feets them to the same <code>\"fc8\"</code> layer</li> <li>Then the output is passed through a decision block.</li> </ul> <p>Initialize the <code>VGGFaceHumanjudgmentFrozenCoreOld</code> model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Run the forward pass.</p> <code>forward_vgg</code> <p>Run the forward pass through the <code>VGG core</code>.</p> <code>init_weights</code> <p>Initialize the weights.</p> <p>Attributes:</p> Name Type Description <code>layer_names</code> <p>Return a list of layer names in the model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def __init__(self, decision_block: str) -&gt; None:\n    \"\"\"Initialize the `VGGFaceHumanjudgmentFrozenCoreOld` model.\"\"\"\n    super().__init__()\n\n    self._layer_names = []\n    self.freeze_vgg_core = True\n    self.vgg_core_fc8 = nn.ModuleDict(\n        OrderedDict(\n            {\n                \"fc8\": nn.Linear(in_features=4096, out_features=300),  # replace last layer\n                \"fc8-relu\": nn.ReLU(inplace=True),\n                \"fc8-dropout\": nn.Dropout(p=0.5),\n            }\n        )\n    )\n    self.vgg_core_fc8.float()  # float32 for GPU, since the input is smaller now, we could use float64\n    self.decision_block_mode = decision_block.lower()\n    if self.decision_block_mode not in {\"fc\", \"conv\"}:\n        msg = \"Decision block mode must be 'fc' or 'conv'.\"\n        raise ValueError(msg)\n    if self.decision_block_mode == \"fc\":\n        self.decision_block = nn.ModuleDict(\n            OrderedDict(\n                {\n                    \"fc_d_9\": nn.Linear(in_features=300 * 3, out_features=300),\n                    \"fc_d_9-relu\": nn.ReLU(inplace=True),\n                    \"fc_d_9-dropout\": nn.Dropout(p=0.5),\n                    \"fc_d_10\": nn.Linear(in_features=300, out_features=3),\n                }\n            )\n        )\n        # Initialize weights\n        self.decision_block.apply(self.init_weights)  # might not be necessary\n\n    elif self.decision_block_mode == \"conv\":\n        self.decision_block = nn.ModuleDict(\n            OrderedDict(\n                {  # in (bs, 1, 6, 300)\n                    \"conv_d_9_1\": nn.Conv2d(\n                        in_channels=1, out_channels=2, kernel_size=(2, 50), padding=0, stride=(2, 1)\n                    ),  # -&gt; (bs, 2, 3, 251)\n                    \"relu_d_9_1\": nn.ReLU(inplace=True),\n                    \"conv_d_9_2\": nn.Conv2d(\n                        in_channels=2, out_channels=2, kernel_size=(3, 100), padding=0\n                    ),  # -&gt; (bs, 2, 1, 152)\n                    \"relu_d_9_2\": nn.ReLU(inplace=True),\n                    \"conv_d_9_3\": nn.Conv2d(\n                        in_channels=2, out_channels=1, kernel_size=(1, 100), padding=0\n                    ),  # -&gt; (bs, 1, 1, 53)\n                    \"relu_d_9_3\": nn.ReLU(inplace=True),\n                    \"conv_d_10_1\": nn.Conv2d(\n                        in_channels=1, out_channels=1, kernel_size=(1, 51), padding=0\n                    ),  # -&gt; (bs, 1, 1, 3)\n                }\n            )\n        )\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreOld.layer_names","title":"layer_names  <code>property</code>","text":"<pre><code>layer_names\n</code></pre> <p>Return a list of layer names in the model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreOld.forward","title":"forward","text":"<pre><code>forward(x1, x2, x3)\n</code></pre> <p>Run the forward pass.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward(self, x1, x2, x3):\n    \"\"\"Run the forward pass.\"\"\"\n    x1 = self.forward_vgg(x1)\n    x2 = self.forward_vgg(x2)\n    x3 = self.forward_vgg(x3)\n\n    # Reshape for decision block\n    if self.decision_block_mode == \"fc\":\n        x = torch.stack((x1, x2, x3), dim=1)\n        x = x.view(x.size(0), -1)  # (batch_size, 300 * 3)\n    elif self.decision_block_mode == \"conv\":\n        x = torch.stack((x1, x2, x1, x3, x2, x3), dim=1)  # all 6 combinations\n        x = x.unsqueeze(1)  # add channel dimension,  (batch_size, channel=1, combs=6, 300)\n    else:\n        msg = \"Decision block mode must be 'fc' or 'conv'.\"\n        raise ValueError(msg)\n\n    # Forward through decision layers\n    for layer in self.decision_block.values():\n        x = layer(x)\n\n    if self.decision_block_mode == \"conv\":\n        # Reshape before release\n        x = x.view(x.size(0), -1)\n\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreOld.forward_vgg","title":"forward_vgg","text":"<pre><code>forward_vgg(x)\n</code></pre> <p>Run the forward pass through the <code>VGG core</code>.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward_vgg(self, x):\n    \"\"\"Run the forward pass through the `VGG core`.\"\"\"\n    # Forward through FC layers\n    for layer in self.vgg_core_fc8.values():\n        x = layer(x)\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreOld.init_weights","title":"init_weights  <code>staticmethod</code>","text":"<pre><code>init_weights(m)\n</code></pre> <p>Initialize the weights.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>@staticmethod\ndef init_weights(m):\n    \"\"\"Initialize the weights.\"\"\"\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        torch.nn.init.constant_(m.bias, 0.01)  # torch.nn.init.zeros_(m.bias)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreWithLegs","title":"VGGFaceHumanjudgmentFrozenCoreWithLegs","text":"<pre><code>VGGFaceHumanjudgmentFrozenCoreWithLegs(\n    frozen_top_model: VGGFaceHumanjudgmentFrozenCore,\n)\n</code></pre> <p>               Bases: <code>VGGFaceHumanjudgment</code></p> <p>A model extension to feed face images to the <code>VGGFaceHumanjudgmentFrozenCore</code>.</p> <p>That is, the model is fed with whole images, instead of activation maps from the last cut layer of the <code>VGGFace core</code>.</p> <p>This is used to apply XAI methods upon <code>VGGFaceHumanjudgmentFrozenCore</code> to find relevant areas in the input images that drive the decision of the model (see <code>facesim3d.modeling.VGG.explain.py</code>).</p> <p>Initialize the <code>VGGFaceHumanjudgmentFrozenCoreWithLegs</code> model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Run the forward pass through the whole model.</p> <code>forward_vgg</code> <p>Run the forward pass through the <code>VGGcore</code> and then through layers of the <code>VGGFaceHumanjudgmentBase</code>.</p> <code>init_weights</code> <p>Initialize weights.</p> <code>requires_grad</code> <p>Return whether a layer requires a gradient flow.</p> <p>Attributes:</p> Name Type Description <code>decision_block_mode</code> <code>str</code> <p>Return the decision block mode.</p> <code>frozen_top_model</code> <p>Return the frozen top-model.</p> <code>last_core_layer</code> <code>str</code> <p>Return the cut layer of the <code>VGGcore</code>, i.e., the last layer before the <code>bridge</code> is attached.</p> <code>layer_names</code> <p>Return the layer names of the model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def __init__(self, frozen_top_model: VGGFaceHumanjudgmentFrozenCore) -&gt; None:\n    \"\"\"Initialize the `VGGFaceHumanjudgmentFrozenCoreWithLegs` model.\"\"\"\n    super().__init__(\n        decision_block=frozen_top_model.decision_block_mode,\n        freeze_vgg_core=frozen_top_model.freeze_vgg_core,\n        last_core_layer=frozen_top_model.last_core_layer,\n        session=frozen_top_model.session,\n    )\n    self._frozen_top_model = frozen_top_model\n    self.frozen_top_model_name = frozen_top_model.name\n    self.vgg_core_bridge = frozen_top_model.vgg_core_bridge  # overwrite vgg_core_bridge\n    self.decision_block = frozen_top_model.decision_block  # overwrite decision_block\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreWithLegs.decision_block_mode","title":"decision_block_mode  <code>property</code> <code>writable</code>","text":"<pre><code>decision_block_mode: str\n</code></pre> <p>Return the decision block mode.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreWithLegs.frozen_top_model","title":"frozen_top_model  <code>property</code> <code>writable</code>","text":"<pre><code>frozen_top_model\n</code></pre> <p>Return the frozen top-model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreWithLegs.last_core_layer","title":"last_core_layer  <code>property</code> <code>writable</code>","text":"<pre><code>last_core_layer: str\n</code></pre> <p>Return the cut layer of the <code>VGGcore</code>, i.e., the last layer before the <code>bridge</code> is attached.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreWithLegs.layer_names","title":"layer_names  <code>property</code>","text":"<pre><code>layer_names\n</code></pre> <p>Return the layer names of the model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreWithLegs.forward","title":"forward","text":"<pre><code>forward(x1: Tensor, x2: Tensor, x3: Tensor) -&gt; Tensor\n</code></pre> <p>Run the forward pass through the whole model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward(self, x1: torch.Tensor, x2: torch.Tensor, x3: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Run the forward pass through the whole model.\"\"\"\n    x1 = self.forward_vgg(x1, bridge_idx=1 if self.parallel_bridge else None)\n    x2 = self.forward_vgg(x2, bridge_idx=2 if self.parallel_bridge else None)\n    x3 = self.forward_vgg(x3, bridge_idx=3 if self.parallel_bridge else None)\n\n    # Reshape for decision block\n    if self.decision_block_mode == \"fc\":\n        x = torch.stack((x1, x2, x3), dim=1)\n        x = x.view(x.size(0), -1)  # (batch_size, 4096 * 3)\n    elif self.decision_block_mode == \"conv\":\n        x = torch.stack((x1, x2, x1, x3, x2, x3), dim=1)  # all 6 combinations\n        x = x.unsqueeze(1)  # add channel dimensions (batch_size, channel=1, combs=6, x[1|2|3].shape=300|4096)\n    else:\n        msg = f\"Decision block mode '{self.decision_block_mode}' not implemented (yet).\"\n        raise NotImplementedError(msg)\n\n    # Forward through decision layers\n    for layer in self.decision_block.values():\n        x = layer(x)\n\n    # Reshape before release\n    if self.decision_block_mode == \"conv\":\n        x = x.view(x.size(0), -1)\n\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreWithLegs.forward_vgg","title":"forward_vgg","text":"<pre><code>forward_vgg(x: Tensor, bridge_idx: int | None) -&gt; Tensor\n</code></pre> <p>Run the forward pass through the <code>VGGcore</code> and then through layers of the <code>VGGFaceHumanjudgmentBase</code>.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward_vgg(self, x: torch.Tensor, bridge_idx: int | None) -&gt; torch.Tensor:\n    \"\"\"Run the forward pass through the `VGGcore` and then through layers of the `VGGFaceHumanjudgmentBase`.\"\"\"\n    x = self.vgg_core(x)\n    x = x.view(x.size(0), -1)  # or ...size()[0], -1), this flattens the tensor\n    # The bridge(s) come(s) from VGGFaceHumanjudgmentBase\n    if self.parallel_bridge:\n        for layer in self.vgg_core_bridge[f\"bridge{bridge_idx}\"].values():\n            x = layer(x)\n    else:\n        for layer in self.vgg_core_bridge.values():\n            x = layer(x)\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreWithLegs.init_weights","title":"init_weights  <code>staticmethod</code>","text":"<pre><code>init_weights(m)\n</code></pre> <p>Initialize weights.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>@staticmethod\ndef init_weights(m):\n    \"\"\"Initialize weights.\"\"\"\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        torch.nn.init.constant_(m.bias, 0.01)  # torch.nn.init.zeros_(m.bias)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGFaceHumanjudgmentFrozenCoreWithLegs.requires_grad","title":"requires_grad","text":"<pre><code>requires_grad(layer_name: str | None = None) -&gt; None\n</code></pre> <p>Return whether a layer requires a gradient flow.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def requires_grad(self, layer_name: str | None = None) -&gt; None:\n    \"\"\"Return whether a layer requires a gradient flow.\"\"\"\n    found = False  # initialize\n    for param_name, param in self.named_parameters():\n        if layer_name is None or layer_name in param_name:\n            print(f\"'{param_name}' requires grad: {param.requires_grad}\")\n            found = True\n    if not found:\n        cprint(string=f\"There is no layer containing parameters with the name '{layer_name}'.\", col=\"r\")\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGMultiView","title":"VGGMultiView","text":"<pre><code>VGGMultiView(\n    freeze_vgg_core: bool,\n    last_core_layer: str = \"fc7-relu\",\n    n_face_ids: int = n_faces,\n    verbose: bool = False,\n)\n</code></pre> <p>               Bases: <code>VGGcore</code></p> <p>Original <code>VGG-Face</code> model retrained to predict face IDs from multiple views.</p> <p>Initialize the <code>VGGMultiView</code> model.</p> <p>Methods:</p> Name Description <code>find_output_dims_of_last_core_layer</code> <p>Find the output dimensions of the last core layer.</p> <code>forward</code> <p>Run the forward pass through the model.</p> <code>init_weights</code> <p>Initialize the model weights.</p> <p>Attributes:</p> Name Type Description <code>layer_names</code> <p>Return a list of layer names in the model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def __init__(\n    self,\n    freeze_vgg_core: bool,\n    last_core_layer: str = \"fc7-relu\",\n    n_face_ids: int = params.main.n_faces,  # 100 in the main study\n    verbose: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the `VGGMultiView` model.\"\"\"\n    if \"fc\" not in last_core_layer:\n        msg = f\"Last core layer must be one of the FC layers, not '{last_core_layer}'.\"\n        # Could cut at an earlier layer, too, but this would need to be implemented\n        raise ValueError(msg)\n    super().__init__(freeze_vgg_core=freeze_vgg_core, last_core_layer=last_core_layer, verbose=verbose)\n    self.verbose = verbose\n\n    # Create decision layer above the last core layer of VGGFace\n    self.decision_layer = nn.ModuleDict(\n        OrderedDict(\n            {\n                \"fc_d\": nn.Linear(\n                    in_features=self.find_output_dims_of_last_core_layer(),  # get out dims of the last core layer\n                    out_features=n_face_ids,\n                )\n            }\n        )\n    )\n    self.decision_layer.apply(self.init_weights)  # might not be necessary\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGMultiView.layer_names","title":"layer_names  <code>property</code>","text":"<pre><code>layer_names\n</code></pre> <p>Return a list of layer names in the model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGMultiView.find_output_dims_of_last_core_layer","title":"find_output_dims_of_last_core_layer","text":"<pre><code>find_output_dims_of_last_core_layer()\n</code></pre> <p>Find the output dimensions of the last core layer.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def find_output_dims_of_last_core_layer(self):\n    \"\"\"Find the output dimensions of the last core layer.\"\"\"\n    n_features = None\n    for name, child in reversed(list(self.vgg_core.named_children())):\n        if not isinstance(child, torch.nn.modules.container.ModuleDict):\n            msg = f\"Child '{name}' is not a ModuleDict, but {type(child)}.\"\n            raise TypeError(msg)\n        for layer_name, layer in reversed(list(child.named_children())):\n            if layer_name == self.last_core_layer.split(\"-\")[0]:  # ignore dropout &amp; relu layers\n                n_features = layer.out_features\n                if self.verbose:\n                    cprint(\n                        string=f\"Found output dims ({n_features}, 1) of layer '{self.last_core_layer}' \"\n                        f\"from the {name}-module.\",\n                        col=\"y\",\n                    )\n                break\n        if n_features is not None:\n            break\n    if n_features is None:\n        msg = f\"Could not find output dimensions of layer '{self.last_core_layer}'.\"\n        raise ValueError(msg)\n    return n_features\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGMultiView.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Run the forward pass through the model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"Run the forward pass through the model.\"\"\"\n    x = self.vgg_core(x)\n    for layer in self.decision_layer.values():\n        # keep iteration for future extensions\n        x = layer(x)\n    return x\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGMultiView.init_weights","title":"init_weights  <code>staticmethod</code>","text":"<pre><code>init_weights(m)\n</code></pre> <p>Initialize the model weights.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>@staticmethod\ndef init_weights(m):\n    \"\"\"Initialize the model weights.\"\"\"\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        torch.nn.init.constant_(m.bias, 0.01)  # torch.nn.init.zeros_(m.bias)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGcore","title":"VGGcore","text":"<pre><code>VGGcore(\n    freeze_vgg_core: bool,\n    last_core_layer: str,\n    verbose: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>The <code>VGGcore</code> class is used to extract a core part of the <code>VGGFace</code> model.</p> <p>For this, the original <code>VGGFace</code> is cut off at a given layer.</p> <p>Initialize the <code>VGGcore</code> model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Run the forward pass through the <code>VGGcore</code> model.</p> <p>Attributes:</p> Name Type Description <code>layer_names</code> <p>Return a list of layer names in the model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def __init__(self, freeze_vgg_core: bool, last_core_layer: str, verbose: bool = False) -&gt; None:\n    \"\"\"Initialize the `VGGcore` model.\"\"\"\n    super().__init__()\n    self.vgg_core = get_vgg_face_model(save_layer_output=False)\n    self._layer_names = []\n    # Keep only layers until maxp_5_3\n    self.last_core_layer = last_core_layer\n    self._cut_off(last_core_layer=last_core_layer, verbose=verbose)\n    self.freeze_vgg_core = freeze_vgg_core\n    if self.freeze_vgg_core:\n        self._freeze_vgg_core()\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGcore.layer_names","title":"layer_names  <code>property</code>","text":"<pre><code>layer_names\n</code></pre> <p>Return a list of layer names in the model.</p>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.VGGcore.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Run the forward pass through the <code>VGGcore</code> model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"Run the forward pass through the `VGGcore` model.\"\"\"\n    return self.vgg_core(x)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.check_exclusive_gender_trials","title":"check_exclusive_gender_trials","text":"<pre><code>check_exclusive_gender_trials(\n    exclusive_gender_trials: str | None,\n) -&gt; str | None\n</code></pre> <p>Check the variable <code>exclusive_gender_trials</code>, which is used in different functions.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def check_exclusive_gender_trials(exclusive_gender_trials: str | None) -&gt; str | None:\n    \"\"\"Check the variable `exclusive_gender_trials`, which is used in different functions.\"\"\"\n    if exclusive_gender_trials is not None:\n        msg = f\"exclusive_gender_trials must be in {params.GENDERS} OR None.\"\n        if isinstance(exclusive_gender_trials, str):\n            exclusive_gender_trials = exclusive_gender_trials.lower()\n            if exclusive_gender_trials not in params.GENDERS:\n                raise ValueError(msg)\n        else:\n            raise TypeError(msg)\n    return exclusive_gender_trials\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.create_conv_decision_block","title":"create_conv_decision_block","text":"<pre><code>create_conv_decision_block(\n    last_core_layer: str,\n) -&gt; ModuleDict\n</code></pre> <p>Build a decision block with convolutional layers only.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def create_conv_decision_block(last_core_layer: str) -&gt; nn.ModuleDict:\n    \"\"\"Build a decision block with convolutional layers only.\"\"\"\n    last_core_layer = last_core_layer.lower()\n    # Get layer parameters\n    last_block_nr = next(int(c) for c in last_core_layer if c.isdigit())  # e.g., \"maxp_5_3\" -&gt; 5; \"fc6\" -&gt; 6\n\n    if last_block_nr in range(1, 5 + 1):\n        # the core model was cut off at its conv block\n        return nn.ModuleDict(\n            OrderedDict(\n                {  # in (bs, 1, 6, 4096)\n                    f\"conv_d_{last_block_nr + 2}_1\": nn.Conv2d(\n                        in_channels=1, out_channels=2, kernel_size=(2, 50), padding=0, stride=(2, 1)\n                    ),  # -&gt; (bs, 2, 3, 4047)\n                    f\"relu_d_{last_block_nr + 2}_1\": nn.ReLU(inplace=True),\n                    f\"conv_d_{last_block_nr + 2}_2\": nn.Conv2d(\n                        in_channels=2, out_channels=2, kernel_size=(3, 100), padding=0, stride=(1, 2)\n                    ),  # -&gt; (bs, 2, 1, 1974)\n                    f\"relu_d_{last_block_nr + 2}_2\": nn.ReLU(inplace=True),\n                    f\"conv_d_{last_block_nr + 2}_3\": nn.Conv2d(\n                        in_channels=2, out_channels=1, kernel_size=(1, 100), padding=0, stride=2\n                    ),  # -&gt; (bs, 1, 1, 938)\n                    f\"relu_d_{last_block_nr + 2}_3\": nn.ReLU(inplace=True),\n                    f\"conv_d_{last_block_nr + 2}_4\": nn.Conv2d(\n                        in_channels=1, out_channels=1, kernel_size=(1, 100), padding=0, stride=3\n                    ),  # -&gt; (bs, 1, 1, 280)\n                    f\"relu_d_{last_block_nr + 2}_4\": nn.ReLU(inplace=True),\n                    f\"conv_d_{last_block_nr + 2}_5\": nn.Conv2d(\n                        in_channels=1, out_channels=1, kernel_size=(1, 80), padding=0, stride=3\n                    ),  # -&gt; (bs, 1, 1, 67)\n                    f\"relu_d_{last_block_nr + 2}_5\": nn.ReLU(inplace=True),\n                    f\"conv_d_{last_block_nr + 3}_1\": nn.Conv2d(\n                        in_channels=1, out_channels=1, kernel_size=(1, 65), padding=0, stride=1\n                    ),  # -&gt; (bs, 1, 1, 3); final layer\n                }\n            )\n        )\n    if \"fc\" in last_core_layer:  # in {\"fc6-relu\", \"fc6-dropout\", \"fc7-relu\", \"fc7-dropout\"}\n        return nn.ModuleDict(\n            OrderedDict(\n                {  # in (bs, 1, 6, 300)\n                    f\"conv_d_{last_block_nr + 2}_1\": nn.Conv2d(\n                        in_channels=1, out_channels=2, kernel_size=(2, 50), padding=0, stride=(2, 1)\n                    ),  # -&gt; (bs, 2, 3, 251)\n                    f\"relu_d_{last_block_nr + 2}_1\": nn.ReLU(inplace=True),\n                    f\"conv_d_{last_block_nr + 2}_2\": nn.Conv2d(\n                        in_channels=2, out_channels=2, kernel_size=(3, 100), padding=0\n                    ),  # -&gt; (bs, 2, 1, 152)\n                    f\"relu_d_{last_block_nr + 2}_2\": nn.ReLU(inplace=True),\n                    f\"conv_d_{last_block_nr + 2}_3\": nn.Conv2d(\n                        in_channels=2, out_channels=1, kernel_size=(1, 100), padding=0\n                    ),  # -&gt; (bs, 1, 1, 53)\n                    f\"relu_d_{last_block_nr + 2}_3\": nn.ReLU(inplace=True),\n                    f\"conv_d_{last_block_nr + 3}_1\": nn.Conv2d(\n                        in_channels=1, out_channels=1, kernel_size=(1, 51), padding=0\n                    ),  # -&gt; (bs, 1, 1, 3)\n                }\n            )\n        )\n\n    # Error if layer not implemented\n    msg = f\"Cut layer '{last_core_layer}' not implemented yet for create_conv_decision_block().\"\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.create_fc_bridge","title":"create_fc_bridge","text":"<pre><code>create_fc_bridge(last_core_layer: str) -&gt; ModuleDict | None\n</code></pre> <p>Build a bridge between the <code>VGG core</code> and the decision block with fully connected layers.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def create_fc_bridge(last_core_layer: str) -&gt; nn.ModuleDict | None:\n    \"\"\"Build a bridge between the `VGG core` and the decision block with fully connected layers.\"\"\"\n    # Get layer parameters\n    in_features = np.prod(get_vgg_layer_feature(last_core_layer))  # note, this raises error if layer not found\n    # in_features: for conv layer this is, e.g., (bs, 512, 7, 7) -&gt; 25088 (here: \"maxp_5_3\"); for fc this is -&gt; 4096\n    last_block_nr = next(int(c) for c in last_core_layer if c.isdigit())  # e.g., \"maxp_5_3\" -&gt; 5; \"fc6\" -&gt; 6\n\n    # Create bridge\n    if last_block_nr in range(1, 5 + 1):  # in one of the conv blocks, e.g., \"maxp_5_3\"\n        # For conv blocks\n        return nn.ModuleDict(\n            OrderedDict(\n                {\n                    f\"fc{last_block_nr + 1}\": nn.Linear(in_features=in_features, out_features=4096),\n                    f\"fc{last_block_nr + 1}-relu\": nn.ReLU(inplace=True),\n                    f\"fc{last_block_nr + 1}-dropout\": nn.Dropout(p=0.5, inplace=False),\n                }\n            )\n        )\n\n    # i.e., if \"fc\" in last_core_layer: # in {\"fc6-relu\", \"fc6-dropout\", \"fc7-relu\", \"fc7-dropout\"}\n    # For fc layers\n    return nn.ModuleDict(\n        OrderedDict(\n            {\n                f\"fc{last_block_nr + 1}\": nn.Linear(in_features=in_features, out_features=300),\n                f\"fc{last_block_nr + 1}-relu\": nn.ReLU(inplace=True),\n                f\"fc{last_block_nr + 1}-dropout\": nn.Dropout(p=0.5),\n            }\n        )\n    )\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.create_fc_decision_block","title":"create_fc_decision_block","text":"<pre><code>create_fc_decision_block(\n    last_core_layer: str,\n) -&gt; ModuleDict\n</code></pre> <p>Build a decision block with fully connected (fc) layers.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>@deprecated(message=\"Deprecated: FC decision block does not converge. Use create_conv_decision_block() instead.\")\ndef create_fc_decision_block(last_core_layer: str) -&gt; nn.ModuleDict:\n    \"\"\"Build a decision block with fully connected (fc) layers.\"\"\"\n    # Get layer parameters\n    last_block_nr = next(int(c) for c in last_core_layer if c.isdigit())  # e.g., \"maxp_5_3\" -&gt; 5; fc6 -&gt; 6\n\n    if last_block_nr in range(1, 5 + 1):  # in one of the conv blocks, e.g., \"maxp_5_3\"\n        # For conv blocks\n        return nn.ModuleDict(\n            OrderedDict(\n                {\n                    f\"fc_d_{last_block_nr + 2}\": nn.Linear(in_features=4096 * 3, out_features=1024),\n                    f\"fc_d_{last_block_nr + 2}-relu\": nn.ReLU(inplace=True),\n                    f\"fc_d_{last_block_nr + 2}-dropout\": nn.Dropout(p=0.5, inplace=False),\n                    f\"fc_d_{last_block_nr + 3}\": nn.Linear(in_features=1024, out_features=3),  # final layer\n                }\n            )\n        )\n    if \"fc\" in last_core_layer:  # in {\"fc6-relu\", \"fc6-dropout\", \"fc7-relu\", \"fc7-dropout\"}\n        # For fc layers\n        return nn.ModuleDict(\n            OrderedDict({f\"fc_d_{last_block_nr + 2}\": nn.Linear(in_features=300 * 3, out_features=3)})\n        )\n\n    # Error if layer not implemented\n    msg = f\"Cut layer '{last_core_layer}' not implemented yet for create_fc_decision_block().\"\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.draw_model","title":"draw_model","text":"<pre><code>draw_model(\n    model: (\n        VGGFace\n        | VGGcore\n        | VGGFaceHumanjudgment\n        | VGGFaceHumanjudgmentFrozenCore\n    ),\n    output: Tensor,\n    keep: bool = False,\n) -&gt; None\n</code></pre> <p>Draw the computational graph of a given <code>VGG</code> model variant.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def draw_model(\n    model: VGGFace | VGGcore | VGGFaceHumanjudgment | VGGFaceHumanjudgmentFrozenCore,\n    output: torch.Tensor,\n    keep: bool = False,\n) -&gt; None:\n    \"\"\"Draw the computational graph of a given `VGG` model variant.\"\"\"\n    dot_graph = make_dot(var=output, params=dict(model.named_parameters()), show_attrs=False, show_saved=False)\n\n    graph_name = f\"graph_{type(model).__name__}\"\n    if hasattr(model, \"session\"):\n        graph_name += f\"_session-{model.session}\"\n    model_path = Path(paths.data.CACHE, graph_name)\n    if hasattr(model, \"freeze_vgg_core\"):\n        model_path = model_path.with_name(model_path.name + f\"_frozen_core-{model.freeze_vgg_core}\")\n    if hasattr(model, \"decision_block_mode\"):\n        model_path = model_path.with_name(model_path.name + f\"_db-{model.decision_block_mode}\")\n\n    # Draw\n    dot_graph.view(filename=model_path, cleanup=True)\n\n    if not keep:\n        cinput(string=\"\\nPress any key to close &amp; delete the graph file.\\n\", col=\"y\")\n        model_path.with_suffix(\".pdf\").unlink()\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.get_vgg_face_model","title":"get_vgg_face_model","text":"<pre><code>get_vgg_face_model(save_layer_output: bool) -&gt; VGGFace\n</code></pre> <p>Get the originally trained <code>VGGFace</code> model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def get_vgg_face_model(save_layer_output: bool) -&gt; VGGFace:\n    \"\"\"Get the originally trained `VGGFace` model.\"\"\"\n    vgg_model = VGGFace(save_layer_output=save_layer_output).double()\n    return load_trained_vgg_weights_into_model(vgg_model)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.get_vgg_layer_feature","title":"get_vgg_layer_feature","text":"<pre><code>get_vgg_layer_feature(\n    layer_name: str, feature: str = \"output_shape\"\n) -&gt; list[..., int] | int\n</code></pre> <p>Get the output shape of a given <code>VGG</code> layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>Name of the layer.</p> required <code>feature</code> <code>str</code> <p>Feature to return, either 'output_shape' or 'n_params', or so (see table columns)</p> <code>'output_shape'</code> <p>Returns:</p> Type Description <code>list[..., int] | int</code> <p>layer feature</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def get_vgg_layer_feature(layer_name: str, feature: str = \"output_shape\") -&gt; list[..., int] | int:\n    \"\"\"\n    Get the output shape of a given `VGG` layer.\n\n    :param layer_name: Name of the layer.\n    :param feature: Feature to return, either 'output_shape' or 'n_params', or so (see table columns)\n    :return: layer feature\n    \"\"\"\n    layer_name = layer_name.lower()\n    layer_tab = read_vgg_layer_table()\n    if layer_name not in layer_tab.layer_names.to_numpy():\n        msg = f\"Layer '{layer_name}' not found in {paths.data.models.vgg.output_shapes}.\"\n        raise ValueError(msg)\n    feature = feature.lower()\n    if feature not in layer_tab.columns:\n        msg = f\"Feature '{feature}' not found in {layer_tab.columns.to_list()}.\"\n        raise ValueError(msg)\n    return layer_tab.loc[layer_tab.layer_names == layer_name][feature].to_list()[0]\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.get_vgg_layer_names","title":"get_vgg_layer_names  <code>cached</code>","text":"<pre><code>get_vgg_layer_names() -&gt; list[str]\n</code></pre> <p>Return a list of layer names constituting the <code>VGGFace</code> model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>@lru_cache(maxsize=1)\ndef get_vgg_layer_names() -&gt; list[str]:\n    \"\"\"Return a list of layer names constituting the `VGGFace` model.\"\"\"\n    return VGGFace(save_layer_output=False).layer_names\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.get_vgg_performance_table","title":"get_vgg_performance_table","text":"<pre><code>get_vgg_performance_table(\n    sort_by_acc: bool = True,\n    hp_search: bool = False,\n    exclusive_gender_trials: str | None = None,\n) -&gt; DataFrame\n</code></pre> <p>Get the performance table for <code>VGGFace</code> models.</p> <p>Parameters:</p> Name Type Description Default <code>sort_by_acc</code> <code>bool</code> <p>Sort table by accuracy.</p> <code>True</code> <code>hp_search</code> <code>bool</code> <p>True: Use hyperparameter search table.</p> <code>False</code> <code>exclusive_gender_trials</code> <code>str | None</code> <p>For models trained on exclusive gender trials ['female' OR 'male'], OR None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>VGGface performance table.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def get_vgg_performance_table(\n    sort_by_acc: bool = True, hp_search: bool = False, exclusive_gender_trials: str | None = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Get the performance table for `VGGFace` models.\n\n    :param sort_by_acc: Sort table by accuracy.\n    :param hp_search: True: Use hyperparameter search table.\n    :param exclusive_gender_trials: For models trained on exclusive gender trials ['female' OR 'male'], OR None.\n    :return: VGGface performance table.\n    \"\"\"\n    acc_cols = [\"train_acc\", \"val_acc\", \"test_acc\"]\n\n    exclusive_gender_trials = check_exclusive_gender_trials(exclusive_gender_trials=exclusive_gender_trials)\n    if isinstance(exclusive_gender_trials, str):\n        p2_table = (\n            paths.data.models.behave.hp_search.hp_table_gender\n            if hp_search\n            else paths.data.models.behave.hp_table_gender\n        )\n        p2_table = Path(p2_table.format(gender=exclusive_gender_trials))\n    else:\n        p2_table = Path(\n            paths.data.models.behave.hp_search.hp_table if hp_search else paths.data.models.behave.hp_table\n        )\n\n    if p2_table.exists():\n        hp_tab = pd.read_csv(p2_table)\n        if sort_by_acc:\n            hp_tab = hp_tab.sort_values(by=\"test_acc\", ascending=False)\n    else:\n        # initialize table\n        hp_tab = pd.DataFrame(\n            columns=[\n                \"model_name\",  # str\n                \"session\",  # \"2D\" | \"3D\"\n                \"data_mode\",  # \"2d-original\" | \"3d-reconstructions\" | \"3d-perspectives\"\n                \"freeze_vgg_core\",  # bool\n                \"last_core_layer\",  # str\n                \"parallel_bridge\",  # bool\n                \"dblock\",  # architecture of decision head\n                \"bs\",  # batch size\n                \"epochs\",  # int\n                \"lr\",  # learning rate\n                \"seed\",  # int\n                \"device\",  # \"cpu:i\" | \"cuda:i\"\n                \"n_heads\",  # number of heads (main: 100)\n                \"n_train\",  # int\n                \"n_val\",  # int\n                \"time_taken\",  # pd.Timedelta\n                *acc_cols,  # float (accuracy) * 3\n            ]\n        )\n    return hp_tab\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.h_out","title":"h_out","text":"<pre><code>h_out(h_in, k, s, p, d=1)\n</code></pre> <p>Calculate the output height of a convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>h_in</code> <p>input height</p> required <code>k</code> <p>kernel size (height)</p> required <code>s</code> <p>stride</p> required <code>p</code> <p>padding</p> required <code>d</code> <p>dilation</p> <code>1</code> <p>Returns:</p> Type Description <p>output height.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def h_out(h_in, k, s, p, d=1):\n    \"\"\"\n    Calculate the output height of a convolutional layer.\n\n    :param h_in: input height\n    :param k: kernel size (height)\n    :param s: stride\n    :param p: padding\n    :param d: dilation\n    :return: output height.\n    \"\"\"\n    return int((h_in + 2 * p - d * (k - 1) - 1) / s + 1)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.load_trained_vgg_face_human_judgment_model","title":"load_trained_vgg_face_human_judgment_model","text":"<pre><code>load_trained_vgg_face_human_judgment_model(\n    session: str,\n    model_name: str | None = None,\n    exclusive_gender_trials: str | None = None,\n    device: str | None = None,\n) -&gt; VGGFaceHumanjudgment | VGGFaceHumanjudgmentFrozenCore\n</code></pre> <p>Load a trained <code>VGGFaceHumanjudgment</code> model from a file.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def load_trained_vgg_face_human_judgment_model(\n    session: str,\n    model_name: str | None = None,\n    exclusive_gender_trials: str | None = None,\n    device: str | None = None,\n) -&gt; VGGFaceHumanjudgment | VGGFaceHumanjudgmentFrozenCore:\n    \"\"\"Load a trained `VGGFaceHumanjudgment` model from a file.\"\"\"\n    exclusive_gender_trials = check_exclusive_gender_trials(exclusive_gender_trials=exclusive_gender_trials)\n    g_sfx = \"\" if exclusive_gender_trials is None else f\"{exclusive_gender_trials}_only_trials\"\n    p2_model_root = Path(paths.data.models.vggbehave, g_sfx, session)\n\n    if model_name is None:\n        p2_models = list(Path(p2_model_root).glob(\"*.pth\"))\n        if not p2_models:\n            msg = f\"No model found in '{p2_model_root}'.\"\n            raise FileNotFoundError(msg)\n        if len(p2_models) == 1:\n            p2_model = p2_models.pop()\n        else:\n            p2_model = Path(browse_files(initialdir=p2_model_root, filetypes=\"pth\"))\n        model_name = p2_model.name.removesuffix(\"_final.pth\")\n    else:\n        model_name = model_name.removesuffix(\"_final.pth\")\n        p2_model = p2_model_root / f\"{model_name}_final.pth\"\n\n    # Load hyperparameters from table\n    hp_tab = get_vgg_performance_table(exclusive_gender_trials=exclusive_gender_trials)\n    model_hps = hp_tab[hp_tab.model_name == model_name]\n    decision_block = model_hps.dblock.to_numpy()[0]\n    freeze_vgg_core = model_hps.freeze_vgg_core.to_numpy()[0]\n    last_core_layer = model_hps.last_core_layer.to_numpy()[0]\n    parallel_bridge = model_hps.parallel_bridge.to_numpy()[0]\n\n    # Initialize model\n    if \"FrozenCore\" in model_name:\n        if pd.Timestamp(model_name.split(\"_\")[0]) &lt; pd.Timestamp(\"2023-04-03\"):\n            model = VGGFaceHumanjudgmentFrozenCoreOld(decision_block=decision_block, session=session)\n        else:\n            model = VGGFaceHumanjudgmentFrozenCore(\n                decision_block=decision_block,\n                last_core_layer=last_core_layer,\n                parallel_bridge=parallel_bridge,\n                session=session,\n            )\n\n    else:\n        model = VGGFaceHumanjudgment(\n            decision_block=decision_block,\n            freeze_vgg_core=freeze_vgg_core,\n            last_core_layer=last_core_layer,\n            parallel_bridge=parallel_bridge,\n            session=session,\n        )\n    model.name = model_name\n\n    # Load model weights\n    model_dict = torch.load(p2_model, map_location=lambda storage, loc: storage)  # noqa: ARG005\n    model.load_state_dict(model_dict)\n\n    if device is not None:\n        return model.to(device).float()\n    return model.float()\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.load_trained_vgg_weights_into_model","title":"load_trained_vgg_weights_into_model","text":"<pre><code>load_trained_vgg_weights_into_model(\n    model: VGGFace,\n) -&gt; VGGFace\n</code></pre> <p>Load trained weights into the original <code>VGG-Face</code> model.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def load_trained_vgg_weights_into_model(model: VGGFace) -&gt; VGGFace:\n    \"\"\"Load trained weights into the original `VGG-Face` model.\"\"\"\n    model_dict = torch.load(\n        Path(paths.data.models.vggface, \"vggface.pth\"),\n        map_location=lambda storage, loc: storage,  # noqa: ARG005\n    )\n    model.load_state_dict(model_dict)\n    return model\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.model_summary","title":"model_summary","text":"<pre><code>model_summary(\n    model: nn,\n    input_size: list,\n    batch_size: int = -1,\n    device: str = \"cpu\",\n)\n</code></pre> <p>Create a <code>Tensorflow</code>-like model summary.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def model_summary(model: torch.nn, input_size: list, batch_size: int = -1, device: str = \"cpu\"):\n    \"\"\"Create a `Tensorflow`-like model summary.\"\"\"\n    summary(model=model, input_size=input_size, batch_size=batch_size, device=device)\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.read_vgg_layer_table","title":"read_vgg_layer_table  <code>cached</code>","text":"<pre><code>read_vgg_layer_table() -&gt; DataFrame\n</code></pre> <p>Read the table with <code>VGG</code> layer names and corresponding output shapes, and number of parameters.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>@lru_cache(maxsize=1)\ndef read_vgg_layer_table() -&gt; pd.DataFrame:\n    \"\"\"Read the table with `VGG` layer names and corresponding output shapes, and number of parameters.\"\"\"\n    return pd.read_csv(\n        filepath_or_buffer=paths.data.models.vgg.output_shapes,\n        sep=\"\\t\",\n        header=0,\n        converters={\"output_shape\": literal_eval},\n    )\n</code></pre>"},{"location":"reference/modeling/VGG/models/#facesim3d.modeling.VGG.models.w_out","title":"w_out","text":"<pre><code>w_out(w_in, k, s, p, d=1)\n</code></pre> <p>Calculate the output width of a convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>w_in</code> <p>input width</p> required <code>k</code> <p>kernel size (width)</p> required <code>s</code> <p>stride</p> required <code>p</code> <p>padding</p> required <code>d</code> <p>dilation</p> <code>1</code> <p>Returns:</p> Type Description <p>output width.</p> Source code in <code>code/facesim3d/modeling/VGG/models.py</code> <pre><code>def w_out(w_in, k, s, p, d=1):\n    \"\"\"\n    Calculate the output width of a convolutional layer.\n\n    :param w_in: input width\n    :param k: kernel size (width)\n    :param s: stride\n    :param p: padding\n    :param d: dilation\n    :return: output width.\n    \"\"\"\n    return int((w_in + 2 * p - d * (k - 1) - 1) / s + 1)\n</code></pre>"},{"location":"reference/modeling/VGG/prepare_data/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> prepare_data","text":""},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data","title":"prepare_data","text":"<p>Prepare data for <code>VGG</code> models.</p> <p>Classes:</p> Name Description <code>VGGFaceHumanjudgmentDataset</code> <p>Dataset for the <code>VGG-Face</code> model variant, adapted for human similarity judgments.</p> <code>VGGMultiViewDataset</code> <p>Dataset for the <code>VGG-Multi-View-Face</code> model.</p> <p>Functions:</p> Name Description <code>get_multi_view_data</code> <p>Get the multi-view data.</p> <code>load_image_for_model</code> <p>Load an image for the <code>VGG</code> model.</p> <code>prepare_data_for_human_judgment_model</code> <p>Prepare data for the <code>VGG-Face</code>-model for human similarity judgments.</p> <code>prepare_data_for_multi_view_model</code> <p>Prepare data for the multi-view model.</p> <code>revert_model_image</code> <p>Revert a model-input-image to its original form.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGFaceHumanjudgmentDataset","title":"VGGFaceHumanjudgmentDataset","text":"<pre><code>VGGFaceHumanjudgmentDataset(\n    session: str,\n    frozen_core: bool,\n    data_mode: str = \"2d-original\",\n    last_core_layer: str | None = None,\n    dtype: dtype = float32,\n    size: int | None = None,\n    exclusive_gender_trials: str | None = None,\n    heads: list[int] | ndarray[int] | int | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset for the <code>VGG-Face</code> model variant, adapted for human similarity judgments.</p> <p>Initialize the <code>VGGFaceHumanjudgmentDataset</code>.</p> <p>Methods:</p> Name Description <code>display_triplet</code> <p>Display a triplet of images.</p> <p>Attributes:</p> Name Type Description <code>current_index</code> <p>Return the current index.</p> <code>data_mode</code> <p>Return the data mode.</p> <code>exclusive_gender_trials</code> <code>str | None</code> <p>Return the <code>exclusive_gender_trials</code> configuration.</p> <code>last_core_layer</code> <p>Return the cut layer of the <code>VGG core</code> model.</p> <code>session_data</code> <p>Return the session data.</p> <code>vgg_core_output</code> <p>Return the <code>VGG core</code> output.</p> Source code in <code>code/facesim3d/modeling/VGG/prepare_data.py</code> <pre><code>def __init__(\n    self,\n    session: str,\n    frozen_core: bool,\n    data_mode: str = \"2d-original\",\n    last_core_layer: str | None = None,\n    dtype: torch.dtype = torch.float32,\n    size: int | None = None,\n    exclusive_gender_trials: str | None = None,\n    heads: list[int] | np.ndarray[int] | int | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the `VGGFaceHumanjudgmentDataset`.\"\"\"\n    self.session = session\n    self._size = size\n    self.exclusive_gender_trials = exclusive_gender_trials\n    self._heads = heads\n    self.session_data = read_trial_results_of_session(session=session, clean_trials=True, verbose=False)[\n        [\"head1\", \"head2\", \"head3\", \"head_odd\"]\n    ].astype(int)\n    self.frozen_core = frozen_core\n    self._vgg_core_output = None\n    self.data_mode = data_mode.lower()\n    self.last_core_layer = last_core_layer\n    self._suffix_data_mode = (\n        \"original\" if \"orig\" in self.data_mode else \"3D-recon\" if \"recon\" in self.data_mode else \"3D-persp\"\n    )\n    self.dtype = dtype\n    self._subtract_mean = kwargs.pop(\"subtract_mean\", True)\n    self._current_index = None\n</code></pre>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGFaceHumanjudgmentDataset.current_index","title":"current_index  <code>property</code>","text":"<pre><code>current_index\n</code></pre> <p>Return the current index.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGFaceHumanjudgmentDataset.data_mode","title":"data_mode  <code>property</code> <code>writable</code>","text":"<pre><code>data_mode\n</code></pre> <p>Return the data mode.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGFaceHumanjudgmentDataset.exclusive_gender_trials","title":"exclusive_gender_trials  <code>property</code> <code>writable</code>","text":"<pre><code>exclusive_gender_trials: str | None\n</code></pre> <p>Return the <code>exclusive_gender_trials</code> configuration.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGFaceHumanjudgmentDataset.last_core_layer","title":"last_core_layer  <code>property</code> <code>writable</code>","text":"<pre><code>last_core_layer\n</code></pre> <p>Return the cut layer of the <code>VGG core</code> model.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGFaceHumanjudgmentDataset.session_data","title":"session_data  <code>property</code> <code>writable</code>","text":"<pre><code>session_data\n</code></pre> <p>Return the session data.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGFaceHumanjudgmentDataset.vgg_core_output","title":"vgg_core_output  <code>property</code>","text":"<pre><code>vgg_core_output\n</code></pre> <p>Return the <code>VGG core</code> output.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGFaceHumanjudgmentDataset.display_triplet","title":"display_triplet","text":"<pre><code>display_triplet(\n    idx: int | Tensor,\n    as_seen_by_model: bool = True,\n    verbose: bool = False,\n) -&gt; None\n</code></pre> <p>Display a triplet of images.</p> Source code in <code>code/facesim3d/modeling/VGG/prepare_data.py</code> <pre><code>def display_triplet(self, idx: int | torch.Tensor, as_seen_by_model: bool = True, verbose: bool = False) -&gt; None:\n    \"\"\"Display a triplet of images.\"\"\"\n    if torch.is_tensor(idx):\n        idx = idx.tolist().pop()\n\n    img1, img2, img3, _, _ = self[idx].values()  # _, _ == choice, idx\n    faces_imgs = [img1, img2, img3]\n    min_val = min(img.min() for img in faces_imgs)\n    max_val = max(img.max() for img in faces_imgs)\n    h1, h2, h3, odd = self.session_data.iloc[idx].to_numpy()\n    faces = [f\"Head{h}\" for h in [h1, h2, h3]]\n\n    choice_side = [h1, h2, h3].index(odd)\n\n    # Display faces\n    title = f\"Session: {self.session} | {self.data_mode} | as seen by model: {as_seen_by_model} | idx: {idx}\"\n    color = \"darkorange\" if self.frozen_core and as_seen_by_model else \"royalblue\"\n\n    r, c = 12, 4\n    if self.frozen_core:\n        x, y = dims_to_rectangularize(len(img1))\n        c = round(r / x * y) - 1\n\n    fig, axs = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True, num=title, figsize=(r, c))\n    for i, ax in enumerate(axs.flatten()):\n        if as_seen_by_model:\n            if self.frozen_core:\n                img = rectangularize_1d_array(arr=faces_imgs[i], wide=False)\n                ax.imshow(img, cmap=\"seismic\", vmin=min_val, vmax=max_val)\n\n            else:\n                img = faces_imgs[i].permute(1, 2, 0).to(\"cpu\").numpy()\n                img = (img - img.min()) / (img.max() - img.min()) * 255\n                img = img.astype(np.uint8)\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                ax.imshow(Image.fromarray(img))\n        else:\n            ax.imshow(\n                display_face(\n                    head_id=faces[i], data_mode=self.data_mode, interactive=False, show=False, verbose=verbose\n                )\n            )\n        ax.set_title(faces[i], color=color if i == choice_side else \"black\")\n        if i == choice_side:\n            for spine in ax.spines.values():\n                spine.set_edgecolor(color)\n                spine.set_linewidth(2)\n            ax.set_xticks([])\n            ax.set_yticks([])\n        else:\n            ax.axis(\"off\")\n    fig.suptitle(title)\n    fig.tight_layout()\n    fig.show()\n</code></pre>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGMultiViewDataset","title":"VGGMultiViewDataset","text":"<pre><code>VGGMultiViewDataset(\n    frozen_core: bool,\n    last_core_layer: str | None = None,\n    dtype: dtype = float32,\n    heads: list[int] | ndarray[int] | int | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset for the <code>VGG-Multi-View-Face</code> model.</p> <p>Initialize the <code>VGGMultiViewDataset</code>.</p> <p>Methods:</p> Name Description <code>display_image</code> <p>Display face images with a specific angle.</p> <p>Attributes:</p> Name Type Description <code>current_index</code> <p>Return the current index.</p> <code>last_core_layer</code> <p>Return cut layer of the <code>VGG core</code> model.</p> <code>multi_view_data</code> <p>Return the session data.</p> <code>n_unique_faces</code> <p>Return the number of unique faces.</p> <code>vgg_core_output</code> <p>Return the <code>VGG core</code> output.</p> Source code in <code>code/facesim3d/modeling/VGG/prepare_data.py</code> <pre><code>def __init__(\n    self,\n    frozen_core: bool,\n    last_core_layer: str | None = None,\n    dtype: torch.dtype = torch.float32,\n    heads: list[int] | np.ndarray[int] | int | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the `VGGMultiViewDataset`.\"\"\"\n    self._heads = heads\n    self.multi_view_data = get_multi_view_data()\n    self.frozen_core = frozen_core\n    self._vgg_core_output = None\n    self.last_core_layer = last_core_layer\n    self._suffix_data_mode = \"3D-persp\"\n    self.dtype = dtype\n    self._subtract_mean = kwargs.pop(\"subtract_mean\", True)\n    self._current_index = None\n</code></pre>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGMultiViewDataset.current_index","title":"current_index  <code>property</code>","text":"<pre><code>current_index\n</code></pre> <p>Return the current index.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGMultiViewDataset.last_core_layer","title":"last_core_layer  <code>property</code> <code>writable</code>","text":"<pre><code>last_core_layer\n</code></pre> <p>Return cut layer of the <code>VGG core</code> model.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGMultiViewDataset.multi_view_data","title":"multi_view_data  <code>property</code> <code>writable</code>","text":"<pre><code>multi_view_data\n</code></pre> <p>Return the session data.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGMultiViewDataset.n_unique_faces","title":"n_unique_faces  <code>property</code>","text":"<pre><code>n_unique_faces\n</code></pre> <p>Return the number of unique faces.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGMultiViewDataset.vgg_core_output","title":"vgg_core_output  <code>property</code>","text":"<pre><code>vgg_core_output\n</code></pre> <p>Return the <code>VGG core</code> output.</p>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.VGGMultiViewDataset.display_image","title":"display_image","text":"<pre><code>display_image(\n    idx: int | Tensor,\n    as_seen_by_model: bool = True,\n    verbose: bool = False,\n) -&gt; None\n</code></pre> <p>Display face images with a specific angle.</p> Source code in <code>code/facesim3d/modeling/VGG/prepare_data.py</code> <pre><code>def display_image(self, idx: int | torch.Tensor, as_seen_by_model: bool = True, verbose: bool = False) -&gt; None:\n    \"\"\"Display face images with a specific angle.\"\"\"\n    if torch.is_tensor(idx):\n        idx = idx.tolist().pop()\n\n    face_img, head_nr, angle, _ = self[idx].values()  # _ == idx\n    face = f\"Head{head_nr}\"\n\n    # Display faces\n    title = f\"3d-perspectives | angle: {angle}\u00b0 | as seen by model: {as_seen_by_model} | idx: {idx}\"\n\n    r, c = 10, 8\n    if self.frozen_core:\n        x, y = dims_to_rectangularize(len(face_img))\n        c = round(r / x * y) - 1\n\n    fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True, num=title, figsize=(r, c))\n\n    if as_seen_by_model:\n        if self.frozen_core:\n            img = rectangularize_1d_array(arr=face_img, wide=False)\n            ax.imshow(img, cmap=\"seismic\", vmin=face_img.min(), vmax=face_img.max())\n\n        else:\n            img = face_img.permute(1, 2, 0).to(\"cpu\").numpy()\n            img = (img - img.min()) / (img.max() - img.min()) * 255\n            img = img.astype(np.uint8)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            ax.imshow(Image.fromarray(img))\n    else:\n        ax.imshow(\n            display_face(\n                head_id=face,\n                data_mode=\"3d-perspectives\",\n                angle=angle,\n                interactive=False,\n                show=False,\n                verbose=verbose,\n            )\n        )\n    ax.set_title(face, color=\"black\")\n    ax.axis(\"off\")\n    fig.suptitle(title)\n    fig.tight_layout()\n    fig.show()\n</code></pre>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.get_multi_view_data","title":"get_multi_view_data  <code>cached</code>","text":"<pre><code>get_multi_view_data() -&gt; DataFrame\n</code></pre> <p>Get the multi-view data.</p> Source code in <code>code/facesim3d/modeling/VGG/prepare_data.py</code> <pre><code>@cache\ndef get_multi_view_data() -&gt; pd.DataFrame:\n    \"\"\"Get the multi-view data.\"\"\"\n    mv_tab = pd.DataFrame(columns=[\"head_nr\", \"head_idx\", \"angle\", \"img_path\"])\n    for head_nr in range(1, params.main.n_faces + 1):\n        head_idx = head_nr_to_index(head_id=head_nr)\n        for path_to_image in Path(paths.data.cfd.faceviews).glob(f\"head-{head_idx:03d}_*.png\"):\n            angle = path_to_image.stem.split(\"_\")[1].split(\"angle-\")[-1]\n            mv_tab.loc[len(mv_tab)] = (head_nr, head_idx, angle, path_to_image)\n\n    # Sort table by head_nr and angle\n    def adapt_angle(_angle: str) -&gt; str:\n        \"\"\"Transform a given angle to a sortable str object.\"\"\"\n        if _angle != \"frontal\":\n            _angle = f\"{float(_angle):06.2f}\"\n        return _angle\n\n    mv_tab[\"angle_sortable\"] = mv_tab.angle.map(adapt_angle)\n    mv_tab = mv_tab.sort_values(by=[\"head_nr\", \"angle_sortable\"], ascending=[True, True])\n    return mv_tab.reset_index(drop=True)\n</code></pre>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.load_image_for_model","title":"load_image_for_model","text":"<pre><code>load_image_for_model(\n    image_path: str | Path,\n    dtype: float64,\n    subtract_mean: bool = True,\n) -&gt; Tensor\n</code></pre> <p>Load an image for the <code>VGG</code> model.</p> Source code in <code>code/facesim3d/modeling/VGG/prepare_data.py</code> <pre><code>def load_image_for_model(image_path: str | Path, dtype: torch.float64, subtract_mean: bool = True) -&gt; torch.Tensor:\n    \"\"\"Load an image for the `VGG` model.\"\"\"\n    image = cv2.imread(str(image_path))\n    image = cv2.resize(image, dsize=(224, 224))\n    image = torch.Tensor(image).permute(2, 0, 1).view(1, 3, 224, 224).to(dtype)\n    if subtract_mean:\n        # this subtraction should be the average pixel value of the training set of the original VGGFace\n        image -= torch.Tensor(np.array([129.1863, 104.7624, 93.5940])).to(dtype).view(1, 3, 1, 1)\n    return image\n</code></pre>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.prepare_data_for_human_judgment_model","title":"prepare_data_for_human_judgment_model","text":"<pre><code>prepare_data_for_human_judgment_model(\n    session: str,\n    frozen_core: bool,\n    data_mode: str,\n    last_core_layer: str | None = None,\n    split_ratio: tuple = (0.7, 0.15, 0.15),\n    batch_size: int = 1,\n    shuffle: bool = True,\n    num_workers: int = 0,\n    dtype: dtype = float32,\n    size: int | None = None,\n    exclusive_gender_trials: str | None = None,\n    heads: list[int] | ndarray[int] | int | None = None,\n    **kwargs\n) -&gt; tuple[DataLoader, DataLoader, DataLoader]\n</code></pre> <p>Prepare data for the <code>VGG-Face</code>-model for human similarity judgments.</p> <p>Split the data into a train, validation and test set.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>str</code> <p>'2D' OR '3D'</p> required <code>frozen_core</code> <code>bool</code> <p>prepare data for frozen VGG core or not</p> required <code>data_mode</code> <code>str</code> <p>use \"2d-original\", \"3d-reconstructions\", or \"3d-perspectives\" as input images</p> required <code>last_core_layer</code> <code>str | None</code> <p>must be given if frozen_core is True</p> <code>None</code> <code>split_ratio</code> <code>tuple</code> <p>ratio of train, validation and test set</p> <code>(0.7, 0.15, 0.15)</code> <code>batch_size</code> <code>int</code> <p>batch size for dataloader</p> <code>1</code> <code>shuffle</code> <code>bool</code> <p>shuffle data</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>number of workers for dataloader</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>data type for images</p> <code>float32</code> <code>size</code> <code>int | None</code> <p>optionally define total size of data (which then gets split)</p> <code>None</code> <code>exclusive_gender_trials</code> <code>str | None</code> <p>use exclusive gender trials ['female' OR 'male'], OR None for all samples.</p> <code>None</code> <code>heads</code> <code>list[int] | ndarray[int] | int | None</code> <p>optionally define subset of data, provide a list of head IDs or total number of heads IDs</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader, DataLoader]</code> <p>train_dataloader, validation_dataloader, test_dataloader</p> Source code in <code>code/facesim3d/modeling/VGG/prepare_data.py</code> <pre><code>def prepare_data_for_human_judgment_model(\n    session: str,\n    frozen_core: bool,\n    data_mode: str,\n    last_core_layer: str | None = None,\n    split_ratio: tuple = (0.7, 0.15, 0.15),\n    batch_size: int = 1,\n    shuffle: bool = True,\n    num_workers: int = 0,\n    dtype: torch.dtype = torch.float32,\n    size: int | None = None,\n    exclusive_gender_trials: str | None = None,\n    heads: list[int] | np.ndarray[int] | int | None = None,\n    **kwargs,\n) -&gt; tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"\n    Prepare data for the `VGG-Face`-model for human similarity judgments.\n\n    Split the data into a train, validation and test set.\n\n    :param session: '2D' OR '3D'\n    :param frozen_core: prepare data for frozen VGG core or not\n    :param data_mode: use \"2d-original\", \"3d-reconstructions\", or \"3d-perspectives\" as input images\n    :param last_core_layer: must be given if frozen_core is True\n    :param split_ratio: ratio of train, validation and test set\n    :param batch_size: batch size for dataloader\n    :param shuffle: shuffle data\n    :param num_workers: number of workers for dataloader\n    :param dtype: data type for images\n    :param size: optionally define total size of data (which then gets split)\n    :param exclusive_gender_trials: use exclusive gender trials ['female' OR 'male'], OR None for all samples.\n    :param heads: optionally define subset of data, provide a list of head IDs or total number of heads IDs\n    :return: train_dataloader, validation_dataloader, test_dataloader\n    \"\"\"\n    # Load all data\n    all_data = VGGFaceHumanjudgmentDataset(\n        session=session,\n        frozen_core=frozen_core,\n        data_mode=data_mode,\n        last_core_layer=last_core_layer,\n        dtype=dtype,\n        size=size,\n        exclusive_gender_trials=exclusive_gender_trials,\n        heads=heads,\n        **kwargs,\n    )\n\n    # Split into train, validation and test set\n    if sum(split_ratio) != 1.0:\n        msg = \"Split ratio must sum up to 1.\"\n        raise ValueError(msg)\n    cprint(\n        string=\"Splitting data into {:.0%} training, {:.0%} validation &amp; {:.0%} test set ...\".format(*split_ratio),\n        col=\"b\",\n    )\n    training_size = int(split_ratio[0] * len(all_data))\n    validation_size = int(split_ratio[1] * len(all_data))\n    test_size = len(all_data) - training_size - validation_size\n    train_data, val_data, test_data = random_split(\n        dataset=all_data, lengths=[training_size, validation_size, test_size]\n    )\n\n    # Create dataloaders\n    train_dataloader = (\n        DataLoader(train_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n        if training_size &gt; 0\n        else None\n    )\n    validation_dataloader = (\n        DataLoader(val_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n        if validation_size &gt; 0\n        else None\n    )\n    test_dataloader = (\n        DataLoader(test_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n        if test_size &gt; 0\n        else None\n    )\n\n    return train_dataloader, validation_dataloader, test_dataloader\n</code></pre>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.prepare_data_for_multi_view_model","title":"prepare_data_for_multi_view_model","text":"<pre><code>prepare_data_for_multi_view_model(\n    frozen_core: bool,\n    last_core_layer: str | None = None,\n    split_ratio: tuple = (0.8, 0.2, 0.0),\n    batch_size: int = 1,\n    shuffle: bool = True,\n    num_workers: int = 0,\n    dtype: dtype = float32,\n    heads: list[int] | ndarray[int] | int | None = None,\n    **kwargs\n) -&gt; tuple[DataLoader, DataLoader, DataLoader]\n</code></pre> <p>Prepare data for the multi-view model.</p> <p>Split the data into a train, validation and test set.</p> <p>Parameters:</p> Name Type Description Default <code>frozen_core</code> <code>bool</code> <p>prepare data for frozen VGG core or not</p> required <code>last_core_layer</code> <code>str | None</code> <p>must be given if frozen_core is True</p> <code>None</code> <code>split_ratio</code> <code>tuple</code> <p>ratio of train, validation and test set. The test set always contains the frontal views of faces. If &gt; (..., ..., 0.) take also more views into the test set.</p> <code>(0.8, 0.2, 0.0)</code> <code>batch_size</code> <code>int</code> <p>batch size for dataloader</p> <code>1</code> <code>shuffle</code> <code>bool</code> <p>shuffle data</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>number of workers for the dataloader</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>data type for images</p> <code>float32</code> <code>heads</code> <code>list[int] | ndarray[int] | int | None</code> <p>optionally define subset of data, provide a list of head IDs or total number of heads IDs</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader, DataLoader]</code> <p>train_dataloader, validation_dataloader, test_dataloader</p> Source code in <code>code/facesim3d/modeling/VGG/prepare_data.py</code> <pre><code>def prepare_data_for_multi_view_model(\n    frozen_core: bool,\n    last_core_layer: str | None = None,\n    split_ratio: tuple = (0.8, 0.2, 0.0),\n    batch_size: int = 1,\n    shuffle: bool = True,\n    num_workers: int = 0,\n    dtype: torch.dtype = torch.float32,\n    heads: list[int] | np.ndarray[int] | int | None = None,\n    **kwargs,\n) -&gt; tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"\n    Prepare data for the multi-view model.\n\n    Split the data into a train, validation and test set.\n\n    :param frozen_core: prepare data for frozen VGG core or not\n    :param last_core_layer: must be given if frozen_core is True\n    :param split_ratio: ratio of train, validation and test set.\n                        The test set always contains the frontal views of faces.\n                        If &gt; (..., ..., 0.) take also more views into the test set.\n    :param batch_size: batch size for dataloader\n    :param shuffle: shuffle data\n    :param num_workers: number of workers for the dataloader\n    :param dtype: data type for images\n    :param heads: optionally define subset of data, provide a list of head IDs or total number of heads IDs\n    :return: train_dataloader, validation_dataloader, test_dataloader\n    \"\"\"\n    # Load all data\n    all_data = VGGMultiViewDataset(\n        frozen_core=frozen_core,\n        last_core_layer=last_core_layer,\n        dtype=dtype,\n        heads=heads,\n        **kwargs,\n    )\n\n    # Split into train, validation and test set\n    if not np.allclose(sum(split_ratio), 1):\n        msg = \"Split ratio must sum up to 1.\"\n        raise ValueError(msg)\n\n    if max(split_ratio) &gt; 1.0:\n        msg = \"split_ratio elements must be in in [0., 1].\"\n        raise ValueError(msg)\n\n    cprint(\n        string=\"Splitting data into {:.0%} training, {:.0%} validation &amp; {:.0%} test set ...\".format(*split_ratio),\n        col=\"b\",\n    )\n\n    # Splits are done within face ID's. At least all frontal views are in the test set.\n    n_images_per_face = all_data.multi_view_data.head_nr.value_counts().max()  # == 33 (including frontal view)\n    training_size_per_face = int(split_ratio[0] * (n_images_per_face - 1))  # -1 for the frontal view\n    if split_ratio[2] == 0.0:\n        validation_size_per_face = n_images_per_face - 1 - training_size_per_face\n    else:\n        validation_size_per_face = int(split_ratio[1] * (n_images_per_face - 1))\n    test_size_per_face = n_images_per_face - training_size_per_face - validation_size_per_face\n\n    training_indices = []\n    validation_indices = []\n    test_indices = all_data.multi_view_data[all_data.multi_view_data.angle == \"frontal\"].index.tolist()\n    for _head_nr, data_head_nr in all_data.multi_view_data.groupby(\"head_nr\"):\n        test_indices_for_head = data_head_nr[data_head_nr.angle == \"frontal\"].index.tolist()\n        train_indices_for_head = (\n            data_head_nr[data_head_nr.angle != \"frontal\"].sample(training_size_per_face, replace=False).index.tolist()\n        )\n        training_indices += train_indices_for_head\n        val_indices_for_head = (\n            data_head_nr.drop(index=train_indices_for_head + test_indices_for_head)\n            .sample(validation_size_per_face, replace=False)\n            .index.tolist()\n        )\n        validation_indices += val_indices_for_head\n\n        test_indices_for_head += (\n            data_head_nr.drop(index=train_indices_for_head + val_indices_for_head + test_indices_for_head)\n            .sample(test_size_per_face - 1, replace=False)\n            .index.tolist()\n        )\n        # Add test_indices_for_head to test_indices if not already in there\n        test_indices += [idx for idx in test_indices_for_head if idx not in test_indices]\n\n    assert set(training_indices) &amp; set(validation_indices) &amp; set(test_indices) == set()  # noqa: S101\n    assert len(training_indices) + len(validation_indices) + len(test_indices) == len(all_data)  # noqa: S101\n    assert len(training_indices) + len(validation_indices) + len(test_indices) == len(all_data)  # noqa: S101\n    assert (all_data.multi_view_data.iloc[training_indices + validation_indices].angle != \"frontal\").all()  # noqa: S101\n\n    # Create the subsets\n    train_data, val_data, test_data = (\n        Subset(all_data, indices) for indices in (training_indices, validation_indices, test_indices)\n    )\n\n    # Create dataloaders\n    train_dataloader = (\n        DataLoader(train_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n        if len(training_indices) &gt; 0\n        else None\n    )\n    validation_dataloader = (\n        DataLoader(val_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n        if len(validation_indices) &gt; 0\n        else None\n    )\n    test_dataloader = (\n        DataLoader(test_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n        if len(test_indices) &gt; 0\n        else None\n    )\n\n    return train_dataloader, validation_dataloader, test_dataloader\n</code></pre>"},{"location":"reference/modeling/VGG/prepare_data/#facesim3d.modeling.VGG.prepare_data.revert_model_image","title":"revert_model_image","text":"<pre><code>revert_model_image(\n    image: Tensor, add_mean: bool\n) -&gt; ndarray\n</code></pre> <p>Revert a model-input-image to its original form.</p> Source code in <code>code/facesim3d/modeling/VGG/prepare_data.py</code> <pre><code>def revert_model_image(image: torch.Tensor, add_mean: bool) -&gt; np.ndarray:\n    \"\"\"Revert a model-input-image to its original form.\"\"\"\n    if add_mean:\n        image += torch.Tensor(np.array([129.1863, 104.7624, 93.5940])).to(image.dtype).view(1, 3, 1, 1)\n    return image[0].permute(1, 2, 0).to(\"cpu\").numpy().astype(np.uint8)\n</code></pre>"},{"location":"reference/modeling/VGG/vgg_predict/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> vgg_predict","text":""},{"location":"reference/modeling/VGG/vgg_predict/#facesim3d.modeling.VGG.vgg_predict","title":"vgg_predict","text":"<p>Adapt <code>VGG-Face</code> as the core model to predict human judgments in the face similarity task.</p> <p>Run this script via the command line interface (CLI) to train the <code>VGG-Face</code> model on human judgments.</p> <p>What arguments can be passed in CLI?</p> <pre><code>python -m facesim3d.vgg_predict --help\n</code></pre> <p>Functions:</p> Name Description <code>evaluate_vgg_face_human_judgment_model</code> <p>Evaluate the <code>VGG-Face</code> model for human similarity judgments.</p> <code>main</code> <p>Run the main function of <code>vgg_predict.py</code>.</p> <code>train_vgg_face_human_judgment_model</code> <p>Train the <code>VGG-Face</code> model for human similarity judgments.</p>"},{"location":"reference/modeling/VGG/vgg_predict/#facesim3d.modeling.VGG.vgg_predict.evaluate_vgg_face_human_judgment_model","title":"evaluate_vgg_face_human_judgment_model","text":"<pre><code>evaluate_vgg_face_human_judgment_model(\n    model: VGGFaceHumanjudgment,\n    data: DataLoader,\n    device: str,\n    loss_fn: Module | None = None,\n    writer: SummaryWriter | None = None,\n    global_step: int = 0,\n) -&gt; float\n</code></pre> <p>Evaluate the <code>VGG-Face</code> model for human similarity judgments.</p> Source code in <code>code/facesim3d/modeling/VGG/vgg_predict.py</code> <pre><code>def evaluate_vgg_face_human_judgment_model(\n    model: VGGFaceHumanjudgment,\n    data: DataLoader,\n    device: str,\n    loss_fn: torch.nn.Module | None = None,\n    writer: SummaryWriter | None = None,\n    global_step: int = 0,\n) -&gt; float:\n    \"\"\"Evaluate the `VGG-Face` model for human similarity judgments.\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n\n    n_print = np.maximum(len(data) // 25, 10)  # print 25 times over dataset\n    running_loss = 0.0\n    with torch.no_grad():\n        for i, data_i in tqdm(\n            enumerate(data),\n            desc=f\"Iterate through {'test' if loss_fn is None else 'val'} samples\",\n            total=len(data),\n            position=0 if loss_fn is None else 2,\n            leave=loss_fn is None,\n        ):\n            x1, x2, x3, y, _ = data_i.values()  # _ = idx\n            outputs = model(x1.to(device), x2.to(device), x3.to(device))\n            _, predicted = torch.max(outputs.data, 1)\n\n            total += y.size(0)\n            correct += (predicted == y.to(device)).sum().item()\n\n            if loss_fn:\n                loss = loss_fn(outputs, y.to(device))\n                running_loss += loss.item()\n            if (i % n_print) == (n_print - 1):\n                if loss_fn:\n                    msg = f\"Step: {i + 1:6d}, Loss: {loss.item():.5f} | running loss: {running_loss / n_print:.5f}\"\n                    print(msg)\n                if writer is not None:\n                    if loss_fn:  # currently only for validation set\n                        writer.add_scalar(\n                            tag=f\"loss/{'test' if loss_fn is None else 'val'}\",\n                            scalar_value=loss.item(),\n                            global_step=i + global_step,\n                        )\n                    writer.add_scalar(\n                        tag=f\"running_acc/{'test' if loss_fn is None else 'val'}\",\n                        scalar_value=correct / total,\n                        global_step=i + global_step,\n                    )\n\n    acc = correct / total\n    msg = f\"Accuracy of the network on the {'test' if loss_fn is None else 'validation'} set (n={total}): {acc:.2%}\"\n    print(msg)\n    logger.info(msg)\n\n    return acc\n</code></pre>"},{"location":"reference/modeling/VGG/vgg_predict/#facesim3d.modeling.VGG.vgg_predict.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Run the main function of <code>vgg_predict.py</code>.</p> Source code in <code>code/facesim3d/modeling/VGG/vgg_predict.py</code> <pre><code>def main():\n    \"\"\"Run the main function of `vgg_predict.py`.\"\"\"\n    # Set device\n    if FLAGS.device is None:\n        device = str(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n    else:\n        device = FLAGS.device\n    cprint(string=f\"\\nUsing {device = }\\n\", col=\"y\", ts=True)\n    logger.info(\"Using device: %s\", device)\n\n    # Set seed\n    if FLAGS.seed is not None:\n        torch.manual_seed(FLAGS.seed)\n        np.random.seed(FLAGS.seed)\n\n        if device.startswith(\"cuda\"):\n            torch.cuda.manual_seed(FLAGS.seed)\n            torch.cuda.manual_seed_all(FLAGS.seed)\n            torch.backends.cudnn.benchmark = True  # look for optimal algorithms for given train config\n        logger.info(\"Set seed to %s.\", FLAGS.seed)\n\n    # Init model\n    if FLAGS.freeze_weights:\n        # This uses pre-computed activation maps of the VGGFace model\n        vgg_hum = (\n            VGGFaceHumanjudgmentFrozenCore(\n                decision_block=FLAGS.decision_block,\n                last_core_layer=FLAGS.last_core_layer,\n                parallel_bridge=FLAGS.parallel_bridge,\n                session=FLAGS.session,\n            )\n            .to(device)\n            .float()\n        )\n    else:\n        # Here we train end-to-end from image space to human judgments\n        vgg_hum = (\n            VGGFaceHumanjudgment(\n                decision_block=FLAGS.decision_block,\n                freeze_vgg_core=FLAGS.freeze_weights,\n                last_core_layer=FLAGS.last_core_layer,\n                parallel_bridge=FLAGS.parallel_bridge,\n                session=FLAGS.session,\n            )\n            .to(device)\n            .float()\n        )\n\n    # Train model to predict human judgment\n    cprint(\n        string=f\"\\nTraining &amp; testing ({pd.Timestamp.now().ceil(freq='s')})\\n\"\n        f\"\\t{vgg_hum.__class__.__name__}:\\n\"\n        f\"\\t\\t\u25b8 '{FLAGS.session}' session\\n\"\n        f\"\\t\\t\u25b8 '{FLAGS.decision_block}' decision block\\n\"\n        f\"\\t\\t\u25b8 {'frozen' if FLAGS.freeze_weights else 'unfrozen'} VGG core\\n\"\n        f\"\\t\\t\u25b8 last core layer: '{FLAGS.last_core_layer}'\\n\"\n        f\"\\t\\t\u25b8 parallel bridge: {FLAGS.parallel_bridge}\\n\"\n        f\"\\t\\t\u25b8 learning rate: {FLAGS.learning_rate}\\n\"\n        f\"\\t\\t\u25b8 exclusive gender trials : {FLAGS.exclusive_gender_trials}\\n\",\n        col=\"b\",\n        fm=\"ul\",\n    )\n\n    # Prepare data\n    train_dl, val_dl, test_dl = prepare_data_for_human_judgment_model(\n        session=FLAGS.session,\n        frozen_core=FLAGS.freeze_weights,\n        data_mode=FLAGS.data_mode,\n        last_core_layer=FLAGS.last_core_layer,\n        split_ratio=(0.7, 0.15, 0.15),  # keep training set large for testing function\n        batch_size=FLAGS.batch_size,\n        shuffle=FLAGS.shuffle,\n        num_workers=FLAGS.num_workers,\n        dtype=torch.float32,\n        heads=FLAGS.heads,\n        size=FLAGS.n_samples,\n        exclusive_gender_trials=FLAGS.exclusive_gender_trials,\n    )  # keep size small for testing of implementation\n    logger.info(\"Data is prepared for '%s'\", vgg_hum.__class__.__name__)\n\n    # Train &amp; test model\n    trained_vgg_hum = train_vgg_face_human_judgment_model(\n        model=vgg_hum,\n        session=FLAGS.session,\n        data_mode=FLAGS.data_mode,\n        exclusive_gender_trials=FLAGS.exclusive_gender_trials,\n        train_data=train_dl,\n        val_data=val_dl,\n        test_data=test_dl,\n        epochs=FLAGS.epochs,\n        learning_rate=FLAGS.learning_rate,\n        device=device,\n        send_message=FLAGS.notification,\n        seed=FLAGS.seed,\n    )\n    logger.info(\"Finished training of '%s'.\", {trained_vgg_hum.__class__.__name__})\n    cprint(string=f\"\\nI, '{trained_vgg_hum.__class__.__name__}', am trained!\\n\", col=\"g\", fm=\"bo\")\n</code></pre>"},{"location":"reference/modeling/VGG/vgg_predict/#facesim3d.modeling.VGG.vgg_predict.train_vgg_face_human_judgment_model","title":"train_vgg_face_human_judgment_model","text":"<pre><code>train_vgg_face_human_judgment_model(\n    model: (\n        VGGFaceHumanjudgment\n        | VGGFaceHumanjudgmentFrozenCore\n    ),\n    session: str,\n    data_mode: str,\n    exclusive_gender_trials: str | None,\n    train_data: DataLoader,\n    val_data: DataLoader,\n    test_data: DataLoader,\n    epochs: int,\n    device: str,\n    learning_rate: float,\n    send_message: bool = False,\n    seed: int | None = None,\n) -&gt; VGGFaceHumanjudgment\n</code></pre> <p>Train the <code>VGG-Face</code> model for human similarity judgments.</p> Source code in <code>code/facesim3d/modeling/VGG/vgg_predict.py</code> <pre><code>def train_vgg_face_human_judgment_model(\n    model: VGGFaceHumanjudgment | VGGFaceHumanjudgmentFrozenCore,\n    session: str,\n    data_mode: str,\n    exclusive_gender_trials: str | None,\n    train_data: DataLoader,\n    val_data: DataLoader,\n    test_data: DataLoader,\n    epochs: int,\n    device: str,\n    learning_rate: float,\n    send_message: bool = False,\n    seed: int | None = None,\n) -&gt; VGGFaceHumanjudgment:\n    \"\"\"Train the `VGG-Face` model for human similarity judgments.\"\"\"\n    # Check arguments\n    exclusive_gender_trials = check_exclusive_gender_trials(exclusive_gender_trials=exclusive_gender_trials)\n    # TODO: turn \"\" into e.g. \"full_sample\", after running all models, &amp; move previous dirs  # noqa: FIX002\n    p_fix = \"\" if exclusive_gender_trials is None else f\"{exclusive_gender_trials}_only_trials\"  # path_fix\n\n    # Prepare model\n    model_name = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M')}_{model.__class__.__name__}\"\n    model.name = model_name\n    save_path = Path(paths.data.models.vggbehave, p_fix, session, f\"{model_name}_final.pth\")\n    save_path_best = Path(str(save_path).replace(\"_final.pth\", \"_best.pth\"))\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n\n    model.train()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    writer = SummaryWriter(log_dir=str(Path(paths.data.models.vggbehave, p_fix, \"runs\", session, model_name)))\n\n    # Write input example and graph to tensorboard\n    # writer.add_graph(model=model, input_to_model=(img1.to(device), img2.to(device), img3.to(device)),\n    #                  verbose=True)\n\n    n_print = np.maximum(len(train_data) // 5, 2)  # print 5 times per epoch\n    val_freq = np.maximum(epochs // 10, 2)\n    cprint(string=f\"\\nStart training of '{model_name}' ...\\n\", col=\"b\", fm=\"ul\")\n    best_acc = 0.0  # for validation set, init\n    start_time = pd.Timestamp.now()\n    epoch_acc = 0.0  # init\n    logger.info(\"Start training of '%s' ...\", model_name)\n    for epoch in tqdm(range(epochs), desc=\"Epochs\", total=epochs, position=0):\n        running_loss = 0.0  # reset for each epoch\n        running_corrects = 0\n        for i, data in tqdm(\n            enumerate(train_data),\n            desc=\"Iterate through training samples\",\n            total=len(train_data),\n            position=1,\n            leave=False,\n        ):\n            x1, x2, x3, y, _ = data.values()  # _ = idx\n            optimizer.zero_grad()\n            outputs = model(x1.to(device), x2.to(device), x3.to(device))\n            _, predictions = torch.max(outputs, 1)\n            loss = criterion(outputs, y.to(device))\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * x1.size(0)  # x1.size(0) == train_data.batch_size\n            # multiply by batch size to get correct loss since average is taken over batch for x-entropy\n            running_corrects += torch.sum(predictions == y.data.to(device))  # OR just y\n            if (i % n_print) == (n_print - 1):\n                # Save (running) loss to file / use writer\n                msg = (\n                    f\"Epoch: {epoch + 1} | Step: {i + 1:6d} | Current loss: {loss.item():.5f} | \"\n                    f\"Running loss: {running_loss / ((i + 1) * train_data.batch_size):.5f}\"\n                )\n                print(msg, end=\"\\r\")\n                logger.info(msg)\n                writer.add_scalar(tag=\"loss/train\", scalar_value=loss.item(), global_step=i)\n                writer.add_scalar(\n                    tag=\"running_acc/train\",\n                    scalar_value=running_corrects.double() / ((i + 1) * train_data.batch_size),\n                    global_step=i * (epoch + 1),\n                )\n\n        epoch_loss = running_loss / (len(train_data) * train_data.batch_size)\n        # == train_data.sampler.num_samples\n        epoch_acc = running_corrects.double() / (len(train_data) * train_data.batch_size)\n        cprint(\n            string=f\"After epoch {epoch + 1}: Training Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2%}\", col=\"g\", ts=True\n        )\n\n        if (epoch % val_freq) == (val_freq - 1):\n            val_acc = evaluate_vgg_face_human_judgment_model(\n                model=model,\n                data=val_data,\n                device=device,\n                loss_fn=criterion,\n                writer=writer,\n                global_step=i * (epoch + 1),\n            )\n            model.train()  # switch back to train mode\n\n            if val_acc &gt; best_acc:\n                best_acc = val_acc\n                torch.save(model.state_dict(), save_path_best)\n                cprint(\n                    string=f\"Saved model with current best val accuracy ({best_acc:.1%}) to '{save_path_best}'\",\n                    col=\"b\",\n                    ts=True,\n                )\n\n    cprint(\"\\n\" + 2 * \"*****_\" + \" FINISHED TRAINING \" + \"_*****\" * 3 + \"\\n\", col=\"g\", ts=True)\n\n    # Last evaluation of model on training and validation set\n    accs = {\n        \"train\": epoch_acc.cpu().item(),  # use last epoch's accuracy\n        \"val\": evaluate_vgg_face_human_judgment_model(\n            model=model, data=val_data, device=device, loss_fn=criterion, writer=writer\n        ),\n        \"test\": evaluate_vgg_face_human_judgment_model(\n            model=model, data=test_data, device=device, loss_fn=None, writer=writer\n        ),\n    }\n    for set_name, acc in accs.items():\n        msg = f\"Final accuracy of the network on the {set_name} set: {acc:.2%}\"\n        cprint(string=msg, col=\"g\", fm=\"bo\")\n        logger.info(msg)\n\n    # Save final model\n    if accs[\"val\"] &gt;= best_acc:\n        msg = f\"Final model has the best val accuracy ({accs['val']:.1%}) and is saved to '{save_path}'\"\n        torch.save(model.state_dict(), save_path)\n        if save_path_best.exists():\n            save_path_best.unlink()\n    else:\n        msg = (\n            f\"Final model has not the best val accuracy ({accs['val']:.1%}), hence we keep previous best model \"\n            f\"only and rename it to '{save_path}'\"\n        )\n        if save_path_best.exists():\n            save_path_best.rename(save_path)\n    cprint(string=msg, col=\"b\", ts=True)\n    logger.info(msg)\n\n    # Save model hyperparameters to table\n    hp_tab = get_vgg_performance_table(hp_search=False, exclusive_gender_trials=exclusive_gender_trials)\n\n    n_heads = len(np.unique(train_data.dataset.dataset.session_data.to_numpy().flatten()))\n    # Fill in hyperparameters &amp; accuracies\n    hp_tab.loc[len(hp_tab), :] = [\n        model_name,\n        session,\n        data_mode.lower(),\n        model.freeze_vgg_core,\n        model.last_core_layer,\n        model.parallel_bridge,\n        model.decision_block_mode,\n        train_data.batch_size,\n        epochs,\n        learning_rate,\n        seed,\n        device,\n        n_heads,\n        len(train_data),\n        len(val_data),\n        (pd.Timestamp.now() - start_time).round(freq=\"s\"),\n        accs[\"train\"],\n        accs[\"val\"],\n        accs[\"test\"],\n    ]\n    # Convert columns to correct types\n    hp_tab.time_taken = hp_tab.time_taken.astype(str)  # writer (below) cannot handle timedelta64\n    acc_cols = [c for c in hp_tab.columns if \"_acc\" in c]\n    col_convert = [\"bs\", \"epochs\", \"seed\", \"n_train\", \"n_val\"]\n    hp_tab[col_convert] = hp_tab[col_convert].astype(int)\n    hp_tab[acc_cols] = hp_tab[acc_cols].astype(float).round(3)\n\n    # Also save hyperparameters &amp; accuracies to tensorboard\n    writer.add_hparams(\n        hparam_dict=hp_tab.loc[len(hp_tab) - 1, hp_tab.columns[:-3]].to_dict(),\n        metric_dict=hp_tab.loc[len(hp_tab) - 1, hp_tab.columns[-3:]].to_dict(),\n    )\n\n    # Save hp table\n    p2_save = (\n        paths.data.models.behave.hp_table\n        if exclusive_gender_trials is None\n        else paths.data.models.behave.hp_table_gender.format(gender=exclusive_gender_trials)\n    )\n    Path(p2_save).parent.mkdir(parents=True, exist_ok=True)\n    hp_tab.to_csv(p2_save, index=False)\n    logger.info(\"Saved hyperparameters &amp; accuracies to '%s'.\", p2_save)\n\n    # Close tensorboard writer\n    writer.close()\n\n    # Send notification\n    if send_message:\n        response = send_to_mattermost(\n            text=MODEL_MESSAGE.format(\n                model_name=model_name, hp=hp_tab.loc[len(hp_tab) - 1].to_markdown()\n            ),  # long narrow table (better for Mattermost)\n            username=config.PROJECT_NAME,\n            incoming_webhook=config.minerva.webhook_in,\n            icon_url=config.PROJECT_ICON_URL2,\n        )\n\n        if not response.ok:\n            msg = f\"Could not send message to Mattermost: {response.text}\"\n            cprint(string=msg, col=\"r\", ts=True)\n            logger.error(msg)\n    return model\n</code></pre>"}]}