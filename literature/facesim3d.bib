% Encoding: UTF-8

% FaceSim3D
% List of literature relevant to the whole project
% Additional *.bib files could contain publication specific references

% pubman genre = conference-paper
@inproceedings{Hofmann2022,
    title = {{Testing the effect of depth on the perception of faces in an online study}},
    author = {Hofmann, Simon and Koushik, Abhay and Klotzsche, Felix and Nikulin, Vadim V. and Villringer, Arno and Gaebler, Michael},
    doi = {10.32470/CCN.2022.1254-0},
    year = {2022},
    abstract = {{Faces are socially relevant stimuli that can be distinguished by the spatial arrangements of their visual features. However, face perception has been mostly investigated with static 2D images, which differs from everyday life experience. In an online study, we investigate face perception in two viewing conditions (2D {\&} 3D). We compare the cognitive face space for these conditions, by modeling the acquired human similarity ratings with similarity matrices computed from physical face attributes and feature maps of deep learning-based face recognition models. Lastly, we fit these models to the human similarity judgements to explore relevant facial features between the viewing conditions. Unveiling differences between 2D and 3D perception of faces will further our understanding on the role of stimulus presentation on face processing.}},
    booktitle = {{Proceedings of the 2022 Conference on Cognitive Computational Neuroscience}},
    address = {San Francisco},
    note = {Conference on Cognitive Computational Neuroscience (CCN)},
}

% pubman genre = preprint
@misc{Hofmann2024,
    title = {Human-Aligned Deep and Sparse Encoding Models of Dynamic {{3D}} Face Similarity Perception},
    author = {Hofmann, Simon M. and Ciston, Anthony and Koushik, Abhay and Klotzsche, Felix and Hebart, Martin N and M{\"u}ller, Klaus-Robert and Villringer, Arno and Scherf, Nico and Hilsmann, Anna and Nikulin, Vadim V and Gaebler, Michael},
    doi = {10.31234/osf.io/f62pw},
    year = {2024},
    month = sep,
    publisher = {PsyArXiv},
    abstract = {Human face perception happens dynamically over time and primarily in three-dimensional space. Perceived face similarity, including identity, should ideally remain invariant to changes along these dimensions. Surprisingly, much of our knowledge about face representations stems from static presentations of 2D images, which might not sufficiently capture real-world dynamic face processing in 3D. To test the effect of space and time on face similarity judgements, we conducted a pre-registered (https://osf.io/678uh) online experiment using a triplet odd-one-out task in a static 2D and a dynamic 3D condition. In the task, participants were asked to choose the most dissimilar face out of three. We then trained sparse and deep computational encoding models of human face similarity judgements to investigate the latent representations that underlie their predictions. Aggregated over all faces, we found a strong correlation between both viewing conditions, indicating a consistent processing of face similarity between 2D and 3D. Despite these similarities, our encoding models revealed subtle differences between viewing conditions, where a small set of face features, such as distance between chin and cheeks, eye size, nose shape, and particularly in the 3D condition, face-width-height ratio, explained much of the variance in human judgements. Our openly available data and encoding models lay the groundwork for understanding face similarity judgements, which are crucial for our ability to recognise and identify faces in a dynamically changing real world.},
    address = {Leipzig, Germany},
}
