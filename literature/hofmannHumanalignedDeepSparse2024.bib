% Encoding: UTF-8
% PUBLICATION: Human-Aligned Deep and Sparse Encoding Models of Dynamic 3D Face Similarity Perception, PsyArXiv, 2024
% List of literature appearing in the publication

@article{abudarhamFaceRecognitionDepends2021,
    title = {Face {{Recognition Depends}} on {{Specialized Mechanisms Tuned}} to {{View}}-{{Invariant Facial Features}}: {{Insights}} from {{Deep Neural Networks Optimized}} for {{Face}} or {{Object Recognition}}},
    shorttitle = {Face {{Recognition Depends}} on {{Specialized Mechanisms Tuned}} to {{View}}-{{Invariant Facial Features}}},
    author = {Abudarham, Naphtali and Grosbard, Idan and Yovel, Galit},
    year = {2021},
    month = sep,
    journal = {Cognitive Science},
    volume = {45},
    number = {9},
    pages = {e13031},
    issn = {0364-0213, 1551-6709},
    doi = {10.1111/cogs.13031},
    urldate = {2023-08-31},
    abstract = {Abstract Face recognition is a computationally challenging classification task. Deep convolutional neural networks (DCNNs) are brain-inspired algorithms that have recently reached human-level performance in face and object recognition. However, it is not clear to what extent DCNNs generate a human-like representation of face identity. We have recently revealed a subset of facial features that are used by humans for face recognition. This enables us now to ask whether DCNNs rely on the same facial information and whether this human-like representation depends on a system that is optimized for face identification. In the current study, we examined the representation of DCNNs of faces that differ in features that are critical or non-critical for human face recognition. Our findings show that DCNNs optimized for face identification are tuned to the same facial features used by humans for face recognition. Sensitivity to these features was highly correlated with performance of the DCNN on a benchmark face recognition task. Moreover, sensitivity to these features and a view-invariant face representation emerged at higher layers of a DCNN optimized for face recognition but not for object recognition. This finding parallels the division to a face and an object system in high-level visual cortex. Taken together, these findings validate human perceptual models of face recognition, enable us to use DCNNs to test predictions about human face and object recognition as well as contribute to the interpretability of DCNNs.},
    langid = {english},
}

@article{anzellottiDecodingRepresentationsFace2014,
    title = {Decoding {{Representations}} of {{Face Identity That}} Are {{Tolerant}} to {{Rotation}}},
    author = {Anzellotti, Stefano and Fairhall, Scott L. and Caramazza, Alfonso},
    year = {2014},
    month = aug,
    journal = {Cerebral Cortex},
    volume = {24},
    number = {8},
    pages = {1988--1995},
    issn = {1460-2199, 1047-3211},
    doi = {10.1093/cercor/bht046},
    urldate = {2024-09-10},
    langid = {english},
}

@article{buriganaInvariantsKoffkaTheory2017,
    title = {``{{Invariants}}'' in {{Koffka}}'s {{Theory}} of {{Constancies}} in {{Vision}}: {{Highlighting Their Logical Structure}} and {{Lasting Value}}},
    shorttitle = {``{{Invariants}}'' in {{Koffka}}'s {{Theory}} of {{Constancies}} in {{Vision}}},
    author = {Burigana, Luigi and Vicovaro, Michele},
    year = {2017},
    month = mar,
    journal = {Gestalt Theory},
    volume = {39},
    number = {1},
    pages = {6--29},
    issn = {2519-5808},
    doi = {10.1515/gth-2017-0004},
    urldate = {2024-07-18},
    abstract = {Summary By introducing the concept of ``invariants'', Koffka (1935) endowed perceptual psychology with a flexible theoretical tool, which is suitable for representing vision situations in which a definite part of the stimulus pattern is relevant but not sufficient to determine a corresponding part of the perceived scene. He characterised his ``invariance principle'' as a principle conclusively breaking free from the ``old constancy hypothesis'', which rigidly surmised point-to-point relations between stimulus and perceptual properties. In this paper, we explain the basic terms and assumptions implicit in Koffka's concept, by representing them in a set-theoretic framework. Then, we highlight various aspects and implications of the concept in terms of answers to six separate questions: forms of invariants, heuristic paths to them, what is invariant in an invariant, roots of conditional indeterminacy, variability vs. indeterminacy, and overcoming of the indeterminacy. Lastly, we illustrate the lasting value and theoretical power of the concept, by showing that Koffka's insights relating to it do occur in modern perceptual psychology and by highlighting its role in a model of perceptual transparency.},
    copyright = {http://creativecommons.org/licenses/by-nc-nd/3.0},
    langid = {english},
}

@article{burt4DSpaceTimeDimensions2020,
    title = {The {{4D Space-Time Dimensions}} of {{Facial Perception}}},
    author = {Burt, Adelaide L. and Crewther, David P.},
    year = {2020},
    month = jul,
    journal = {Frontiers in Psychology},
    volume = {11},
    pages = {1842},
    issn = {1664-1078},
    doi = {10.3389/fpsyg.2020.01842},
    urldate = {2022-03-09},
    langid = {english},
}

@article{childsIndividualDifferencesFace2021,
    title = {Do Individual Differences in Face Recognition Ability Moderate the Other Ethnicity Effect?},
    author = {Childs, Michael Jeanne and Jones, Alex and Thwaites, Peter and Zdravkovi{\'c}, Sun{\v c}ica and Thorley, Craig and Suzuki, Atsunobu and Shen, Rachel and Ding, Qi and Burns, Edwin and Xu, Hong and Tree, Jeremy J.},
    year = {2021},
    month = jul,
    journal = {Journal of Experimental Psychology: Human Perception and Performance},
    volume = {47},
    number = {7},
    pages = {893--907},
    issn = {1939-1277, 0096-1523},
    doi = {10.1037/xhp0000762},
    urldate = {2024-07-08},
    langid = {english},
}

@article{cootesActiveAppearanceModels2001,
    title = {Active Appearance Models},
    author = {Cootes, T.F. and Edwards, G.J. and Taylor, C.J.},
    year = {2001},
    month = jun,
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    volume = {23},
    number = {6},
    pages = {681--685},
    issn = {01628828},
    doi = {10.1109/34.927467},
    urldate = {2023-08-28},
}

@misc{dapelloAligningModelMacaque2022,
    title = {Aligning {{Model}} and {{Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment}} and {{Adversarial Robustness}}},
    author = {Dapello, Joel and Kar, Kohitij and Schrimpf, Martin and Geary, Robert and Ferguson, Michael and Cox, David D. and DiCarlo, James J.},
    year = {2022},
    month = jul,
    doi = {10.1101/2022.07.01.498495},
    urldate = {2024-04-08},
    abstract = {While some state-of-the-art artificial neural network systems in computer vision are strikingly accurate models of the corresponding primate visual processing, there are still many discrepancies between these models and the behavior of primates on object recognition tasks. Many current models suffer from extreme sensitivity to adversarial attacks and often do not align well with the image-by-image behavioral error patterns observed in humans. Previous research has provided strong evidence that primate object recognition behavior can be very accurately predicted by neural population activity in the inferior temporal (IT) cortex, a brain area in the late stages of the visual processing hierarchy. Therefore, here we directly test whether making the late stage representations of models more similar to that of macaque IT produces new models that exhibit more robust, primate-like behavior. We collected a dataset of chronic, large-scale multi-electrode recordings across the IT cortex in six non-human primates (rhesus macaques). We then use these data to fine-tune (end-to-end) the model "IT" representations such that they are more aligned with the biological IT representations, while preserving accuracy on object recognition tasks. We generate a cohort of models with a range of IT similarity scores validated on held-out animals across two image sets with distinct statistics. Across a battery of optimization conditions, we observed a strong correlation between the models' ITlikeness and alignment with human behavior, as well as an increase in its adversarial robustness. We further assessed the limitations of this approach and find that the improvements in behavioral alignment and adversarial robustness generalize across different image statistics, but not to object categories outside of those covered in our IT training set. Taken together, our results demonstrate that building models that are more aligned with the primate brain leads to more robust and human-like behavior, and call for larger neural data-sets to further augment these gains. Code, models, and data are available at https://github.com/dapello/braintree.},
    langid = {english},
}

@article{daubeGroundingDeepNeural2021,
    title = {Grounding Deep Neural Network Predictions of Human Categorization Behavior in Understandable Functional Features: {{The}} Case of Face Identity},
    shorttitle = {Grounding Deep Neural Network Predictions of Human Categorization Behavior in Understandable Functional Features},
    author = {Daube, Christoph and Xu, Tian and Zhan, Jiayu and Webb, Andrew and Ince, Robin A.A. and Garrod, Oliver G.B. and Schyns, Philippe G.},
    year = {2021},
    month = oct,
    journal = {Patterns},
    volume = {2},
    number = {10},
    pages = {100348},
    issn = {26663899},
    doi = {10.1016/j.patter.2021.100348},
    urldate = {2023-05-22},
    abstract = {Deep neural networks (DNNs) can resolve real-world categorization tasks with apparent human-level performance. However, true equivalence of behavioral performance between humans and their DNN models requires that their internal mechanisms process equivalent features of the stimulus. To develop such feature equivalence, our methodology leveraged an interpretable and experimentally controlled generative model of the stimuli (realistic three-dimensional textured faces). Humans rated the similarity of randomly generated faces to four familiar identities. We predicted these similarity ratings from the activations of five DNNs trained with different optimization objectives. Using information theoretic redundancy, reverse correlation, and the testing of generalization gradients, we show that DNN predictions of human behavior improve because their shape and texture features overlap with those that subsume human behavior. Thus, we must equate the functional features that subsume the behavioral performances of the brain and its models before comparing where, when, and how these features are processed.},
    langid = {english},
}

@article{degutisWhatPrevalenceDevelopmental2023,
    title = {What Is the Prevalence of Developmental Prosopagnosia? {{An}} Empirical Assessment of Different Diagnostic Cutoffs},
    shorttitle = {What Is the Prevalence of Developmental Prosopagnosia?},
    author = {DeGutis, Joseph and Bahierathan, Kanisha and Barahona, Katherine and Lee, EunMyoung and Evans, Travis C. and Shin, Hye Min and Mishra, Maruti and Likitlersuang, Jirapat and Wilmer, Jeremy B.},
    year = {2023},
    month = apr,
    journal = {Cortex},
    volume = {161},
    pages = {51--64},
    issn = {00109452},
    doi = {10.1016/j.cortex.2022.12.014},
    urldate = {2023-07-10},
    langid = {english}
}

@article{dehmoobadsharifabadiAreFaceRepresentations2016,
    title = {Are Face Representations Depth Cue Invariant?},
    author = {Dehmoobadsharifabadi, Armita and Farivar, Reza},
    year = {2016},
    month = jun,
    journal = {Journal of Vision},
    volume = {16},
    number = {8},
    pages = {6},
    issn = {1534-7362},
    doi = {10.1167/16.8.6},
    urldate = {2023-07-17},
    langid = {english},
}

@article{dengLightweightDeepLearning2023,
    title = {A Lightweight Deep Learning Model for Real-time Face Recognition},
    author = {Deng, Zong-Yue and Chiang, Hsin-Han and Kang, Li-Wei and Li, Hsiao-Chi},
    year = {2023},
    month = nov,
    journal = {IET Image Processing},
    volume = {17},
    number = {13},
    pages = {3869--3883},
    issn = {1751-9659, 1751-9667},
    doi = {10.1049/ipr2.12903},
    urldate = {2024-07-24},
    abstract = {Abstract Lightweight deep learning models for face recognition are becoming increasingly crucial for deployment on resource-constrained devices such as embedded systems or mobile devices. This paper presents a highly efficient and compact deep learning (DL) model that achieves state-of-the-art performance on various face recognition benchmarks. The developed DL model employs one- or few-shot learning to obtain effective feature embeddings and draws inspiration from FaceNet with significant refinements to achieve a memory size of only 3.5~MB---about 30 times smaller than FaceNet---while maintaining high accuracy and real-time performance. The study demonstrates the model's effectiveness through extensive experiments, which include testing on public datasets and the model's ability to recognize occluded faces in uncontrolled environments using grayscale input images. Compared to the state-of-the-art lightweight models, the proposed model requires fewer FLOPs (0.06G), has a smaller number of parameters (1.2~M), and occupies a smaller model size (3.5~MB) while achieving a competitive level of recognition accuracy and real-time performance. The results show that the model is well-suited for deployment in embedded domains, including live entrance security checks, driver authorization, and in-class attendance systems. The entire code of FN8 is available on GitHub .},
    langid = {english},
}

@article{dinataleUncannyValleyEffect2023,
    title = {Uncanny Valley Effect: {{A}} Qualitative Synthesis of Empirical Research to Assess the Suitability of Using Virtual Faces in Psychological Research},
    shorttitle = {Uncanny Valley Effect},
    author = {Di Natale, Anna Flavia and Simonetti, Matilde Ellen and La Rocca, Stefania and Bricolo, Emanuela},
    year = {2023},
    month = may,
    journal = {Computers in Human Behavior Reports},
    volume = {10},
    pages = {100288},
    issn = {24519588},
    doi = {10.1016/j.chbr.2023.100288},
    urldate = {2024-07-18},
    langid = {english},
}

@article{eng3DFacesAre2017,
    title = {{{3D}} Faces Are Recognized More Accurately and Faster than {{2D}} Faces, but with Similar Inversion Effects},
    author = {Eng, Z.H.D. and Yick, Y.Y. and Guo, Y and Xu, H. and Reiner, M. and Cham, T.J. and Chen, S.H.A.},
    year = {2017},
    month = sep,
    journal = {Vision Research},
    volume = {138},
    pages = {78--85},
    issn = {00426989},
    doi = {10.1016/j.visres.2017.06.004},
    urldate = {2022-03-09},
    abstract = {Recognition of faces typically occurs via holistic processing where individual features are combined to provide an overall facial representation. However, when faces are inverted, there is greater reliance on featural processing where faces are recognized based on their individual features. These findings are based on a substantial number of studies using 2-dimensional (2D) faces and it is unknown whether these results can be extended to 3-dimensional (3D) faces, which have more depth information that is absent in the typical 2D stimuli used in face recognition literature. The current study used the face inversion paradigm as a means to investigate how holistic and featural processing are differentially influenced by 2D and 3D faces. Twenty-five participants completed a delayed face-matching task consisting of upright and inverted faces that were presented as both 2D and 3D stereoscopic images. Recognition accuracy was significantly higher for 3D upright faces compared to 2D upright faces, providing support that the enriched visual information in 3D stereoscopic images facilitates holistic processing that is essential for the recognition of upright faces. Typical face inversion effects were also obtained, regardless of whether the faces were presented in 2D or 3D. Moreover, recognition performances for 2D inverted and 3D inverted faces did not differ. Taken together, these results demonstrated that 3D stereoscopic effects influence face recognition during holistic processing but not during featural processing. Our findings therefore provide a novel perspective that furthers our understanding of face recognition mechanisms, shedding light on how the integration of stereoscopic information in 3D faces influences face recognition processes.},
    langid = {english},
}

@misc{FaceSim_Manuscripta,
    title = {{FaceSim\_Manuscript}},
    journal = {Google Docs},
    urldate = {2024-02-16},
    howpublished = {https://docs.google.com/document/d/13xQDP4Mwp-sahIBc4sqjPZWbX8p\_WxtywJDvslxcX0I/edit?pli=1\&usp=embed\_facebook},
    langid = {ngerman},
}

@article{fengLearningAnimatableDetailed2021,
    title = {Learning an Animatable Detailed {{3D}} Face Model from In-the-Wild Images},
    author = {Feng, Yao and Feng, Haiwen and Black, Michael J. and Bolkart, Timo},
    year = {2021},
    month = jul,
    journal = {ACM Transactions on Graphics},
    volume = {40},
    number = {4},
    pages = {88:1--88:13},
    issn = {0730-0301},
    doi = {10.1145/3450626.3459936},
    urldate = {2022-03-04},
    abstract = {While current monocular 3D face reconstruction methods can recover fine geometric details, they suffer several limitations. Some methods produce faces that cannot be realistically animated because they do not model how wrinkles vary with expression. Other methods are trained on high-quality face scans and do not generalize well to in-the-wild images. We present the first approach that regresses 3D face shape and animatable details that are specific to an individual but change with expression. Our model, DECA (Detailed Expression Capture and Animation), is trained to robustly produce a UV displacement map from a low-dimensional latent representation that consists of person-specific detail parameters and generic expression parameters, while a regressor is trained to predict detail, shape, albedo, expression, pose and illumination parameters from a single image. To enable this, we introduce a novel detail-consistency loss that disentangles person-specific details from expression-dependent wrinkles. This disentanglement allows us to synthesize realistic person-specific wrinkles by controlling expression parameters while keeping person-specific details unchanged. DECA is learned from in-the-wild images with no paired 3D supervision and achieves state-of-the-art shape reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild data demonstrate DECA's robustness and its ability to disentangle identity- and expression-dependent details enabling animation of reconstructed faces. The model and code are publicly available at https://deca.is.tue.mpg.de.},
}

@article{freiwaldFaceProcessingSystems2016,
    title = {Face {{Processing Systems}}: {{From Neurons}} to {{Real-World Social Perception}}},
    shorttitle = {Face {{Processing Systems}}},
    author = {Freiwald, Winrich and Duchaine, Bradley and Yovel, Galit},
    year = {2016},
    month = jul,
    journal = {Annual Review of Neuroscience},
    volume = {39},
    number = {1},
    pages = {325--346},
    issn = {0147-006X, 1545-4126},
    doi = {10.1146/annurev-neuro-070815-013934},
    urldate = {2021-09-06},
    abstract = {Primate face processing depends on a distributed network of interlinked face-selective areas composed of face-selective neurons. In both humans and macaques, the network is divided into a ventral stream and a dorsal stream, and the functional similarities of the areas in humans and macaques indicate they are homologous. Neural correlates for face detection, holistic processing, face space, and other key properties of human face processing have been identified at the single neuron level, and studies providing causal evidence have firmly established that face-selective brain areas are central to face processing. These mechanisms give rise to our highly accurate familiar face recognition but also to our error prone performance with unfamiliar faces. This limitation of the face system has important implications for consequential situations such as eyewitness identification and policing.},
    langid = {english},
}

@article{furmanskiPerceptualLearningObject2000,
    title = {Perceptual Learning in Object Recognition: Object Specificity and Size Invariance},
    shorttitle = {Perceptual Learning in Object Recognition},
    author = {Furmanski, Christopher S. and Engel, Stephen A.},
    year = {2000},
    month = mar,
    journal = {Vision Research},
    volume = {40},
    number = {5},
    pages = {473--484},
    issn = {00426989},
    doi = {10.1016/S0042-6989(99)00134-0},
    urldate = {2023-07-17},
    langid = {english},
}

@article{geirhosImageNettrainedCNNsAre2018,
    title = {{{ImageNet-trained CNNs}} Are Biased towards Texture; Increasing Shape Bias Improves Accuracy and Robustness},
    author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
    year = {2018},
    publisher = {[object Object]},
    doi = {10.48550/ARXIV.1811.12231},
    urldate = {2024-04-08},
    abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on "Stylized-ImageNet", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
    copyright = {arXiv.org perpetual, non-exclusive license},
}

@article{goyalInductiveBiasesDeep2022,
    title = {Inductive Biases for Deep Learning of Higher-Level Cognition},
    author = {Goyal, Anirudh and Bengio, Yoshua},
    year = {2022},
    month = oct,
    journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
    volume = {478},
    number = {2266},
    pages = {20210068},
    issn = {1364-5021, 1471-2946},
    doi = {10.1098/rspa.2021.0068},
    urldate = {2024-05-08},
    abstract = {A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans' abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.},
    langid = {english},
}

@article{grossmanConvergentEvolutionFace2019a,
    title = {Convergent Evolution of Face Spaces across Human Face-Selective Neuronal Groups and Deep Convolutional Networks},
    author = {Grossman, Shany and Gaziv, Guy and Yeagle, Erin M. and Harel, Michal and M{\'e}gevand, Pierre and Groppe, David M. and Khuvis, Simon and Herrero, Jose L. and Irani, Michal and Mehta, Ashesh D. and Malach, Rafael},
    year = {2019},
    month = oct,
    journal = {Nature Communications},
    volume = {10},
    number = {1},
    pages = {4934},
    issn = {2041-1723},
    doi = {10.1038/s41467-019-12623-6},
    urldate = {2024-08-13},
    abstract = {Abstract The discovery that deep convolutional neural networks (DCNNs) achieve human performance in realistic tasks offers fresh opportunities for linking neuronal tuning properties to such tasks. Here we show that the face-space geometry, revealed through pair-wise activation similarities of face-selective neuronal~groups recorded intracranially in 33 patients, significantly matches that of a DCNN having human-level face recognition capabilities. This convergent evolution of pattern similarities across biological and artificial networks highlights the significance of face-space geometry in face perception. Furthermore, the nature of the neuronal to DCNN match suggests a role of human face areas in pictorial aspects of face perception. First, the match was confined to intermediate DCNN layers. Second, presenting identity-preserving image manipulations to the DCNN abolished its correlation to neuronal responses. Finally, DCNN units matching human neuronal~group tuning displayed view-point selective receptive fields. Our results demonstrate the importance of face-space geometry in the pictorial aspects of human face perception.},
    langid = {english},
}

@article{gucluDeepNeuralNetworks2015,
    title = {Deep {{Neural Networks Reveal}} a {{Gradient}} in the {{Complexity}} of {{Neural Representations}} across the {{Ventral Stream}}},
    author = {G{\"u}ç̧l{\"u}, Umut and van Gerven, Marcel A. J.},
    year = {2015},
    month = jul,
    journal = {Journal of Neuroscience},
    volume = {35},
    number = {27},
    pages = {10005--10014},
    publisher = {Society for Neuroscience},
    doi = {10.1523/jneurosci.5023-14.2015}
}

@article{harrisWhatFaceHow2016,
    title = {What's in a {{Face}}? {{How Face Gender}} and {{Current Affect Influence Perceived Emotion}}},
    shorttitle = {What's in a {{Face}}?},
    author = {Harris, Daniel A. and {Hayes-Skelton}, Sarah A. and Ciaramitaro, Vivian M.},
    year = {2016},
    month = sep,
    journal = {Frontiers in Psychology},
    volume = {7},
    issn = {1664-1078},
    doi = {10.3389/fpsyg.2016.01468},
    urldate = {2024-04-09},
    abstract = {Faces drive our social interactions. A vast literature suggests an interaction between gender and emotional face perception, with studies using different methodologies demonstrating that the gender of a face can affect how emotions are processed. However, how different is our perception of affective male and female faces? Furthermore, how does our current affective state when viewing faces influence our perceptual biases? We presented participants with a series of faces morphed along an emotional continuum from happy to angry. Participants judged each face morph as either happy or angry. We determined each participant's unique emotional `neutral' point, defined as the face morph judged to be perceived equally happy and angry, separately for male and female faces. We also assessed how current state affect influenced these perceptual neutral points. Our results indicate that, for both male and female participants, the emotional neutral point for male faces is perceptually biased to be happier than for female faces. This bias suggests that more happiness is required to perceive a male face as emotionally neutral, i.e., we are biased to perceive a male face as more negative. Interestingly, we also find that perceptual biases in perceiving female faces are correlated with current mood, such that positive state affect correlates with perceiving female faces as happier, while we find no significant correlation between negative state affect and the perception of facial emotion. Furthermore, we find reaction time biases, with slower responses for angry male faces compared to angry female faces.},
    langid = {english},
}

@article{haxbyDistributedHumanNeural2000,
    title = {The Distributed Human Neural System for Face Perception},
    author = {Haxby, James V. and Hoffman, Elizabeth A. and Gobbini, M.Ida},
    year = {2000},
    month = jun,
    journal = {Trends in Cognitive Sciences},
    volume = {4},
    number = {6},
    pages = {223--233},
    issn = {13646613},
    doi = {10.1016/S1364-6613(00)01482-0},
    urldate = {2023-07-17},
    langid = {english},
}

@article{hebartRevealingMultidimensionalMental2020,
    title = {Revealing the Multidimensional Mental Representations of Natural Objects Underlying Human Similarity Judgements},
    author = {Hebart, Martin N. and Zheng, Charles Y. and Pereira, Francisco and Baker, Chris I.},
    year = {2020},
    month = nov,
    journal = {Nature Human Behaviour},
    volume = {4},
    number = {11},
    pages = {1173--1185},
    issn = {2397-3374},
    doi = {10.1038/s41562-020-00951-3},
    urldate = {2021-01-07},
    langid = {english},
}

@inproceedings{hermannOriginsPrevalenceTexture2020,
    title = {The {{Origins}} and {{Prevalence}} of {{Texture Bias}} in {{Convolutional Neural Networks}}},
    booktitle = {34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2020)},
    author = {Hermann, Katherine L and Chen, Ting and Kornblith, Simon},
    year = {2020},
    address = {Vancouver, Canada},
    abstract = {Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to classify images by texture rather than by shape. How pervasive is this bias, and where does it come from? We find that, when trained on datasets of images with conflicting shape and texture, CNNs learn to classify by shape at least as easily as by texture. What factors, then, produce the texture bias in CNNs trained on ImageNet? Different unsupervised training objectives and different architectures have small but significant and largely independent effects on the level of texture bias. However, all objectives and architectures still lead to models that make texturebased classification decisions a majority of the time, even if shape information is decodable from their hidden representations. The effect of data augmentation is much larger. By taking less aggressive random crops at training time and applying simple, naturalistic augmentation (color distortion, noise, and blur), we train models that classify ambiguous images by shape a majority of the time, and outperform baselines on out-of-distribution test sets. Our results indicate that apparent differences in the way humans and ImageNet-trained CNNs process images may arise not primarily from differences in their internal workings, but from differences in the data that they see.},
    langid = {english},
}

@article{hofmannFaceSimilarity3D2022,
    title = {Face {{Similarity 3D}}},
    author = {Hofmann, Simon and Ciston, Anthony and Koushik, Abhay and Klotzsche, Felix and Villringer, Arno and Nikulin, Vadim and Gaebler, Michael},
    year = {2022},
    month = nov,
    publisher = {Open Science Framework},
    doi = {10.17605/OSF.IO/678UH},
    urldate = {2023-09-20},
    abstract = {Faces can be distinguished by the 3D spatial arrangements of their features. Human face perception has been mostly investigated with static images in 2D, which differs from face perception in everyday life. In our online study, we will investigate face perception in two viewing conditions (static 2D and 3D). Based on behavioral similarity ratings, we will map and compare the cognitive face space for 2D and 3D stimuli to advance the understanding on the role of stimulus presentation on face processing.},
    collaborator = {{Open Science Framework}},
    copyright = {MIT License},
}

@article{jandovaSexualDimorphismHuman2018,
    title = {Sexual Dimorphism in Human Facial Expressions by {{3D}} Surface Processing},
    author = {Jandov{\'a}, M. and Urbanov{\'a}, P.},
    year = {2018},
    month = may,
    journal = {HOMO},
    volume = {69},
    number = {3},
    pages = {98--109},
    issn = {0018442X},
    doi = {10.1016/j.jchb.2018.06.002},
    urldate = {2024-07-18},
    abstract = {Human face is a dynamic system where facial expressions can rapidly modify geometry of facial features. Facial expressions are believed to be universal across world populations, but only a few studies have explored whether grimacing is sexually dimorphic and if so to what extent. The present paper explores inter- and intra-individual variation of human facial expressions with respect to individual's sex based on a set of neutral and expression-varying 3D facial scans. The study sample composed of 20 individuals (10 males and 10 females) for whom 120 scans featuring grimaces associated with disgust, surprise, ``u'' sound, smile and wide smile were collected by an optical scanner Vectra XT. In order to quantify the dissimilarity among 3D images, surface comparison approach based on aligned 3D meshes and closest point-to-point distances was carried out in Fidentis Analyst application.},
    langid = {english},
}

@article{jozwikFaceDissimilarityJudgments2022,
    title = {Face Dissimilarity Judgments Are Predicted by Representational Distance in Morphable and Image-Computable Models},
    author = {Jozwik, Kamila M and O'Keeffe, Jonathan and Storrs, Katherine R and Guo, Wenxuan and Golan, Tal and Kriegeskorte, Nikolaus},
    year = {2022},
    volume = {119},
    number = {27},
    pages = {11},
    langid = {english},
}

@article{kahlbaumSinnesdelirien1866,
    title = {{Die Sinnesdelirien}},
    author = {Kahlbaum, Karl Ludwig},
    year = {1866},
    journal = {Allgemeine Zeitschrift f{\"u}r Psychiatrie und psychisch-gerichtliche Medizin},
    volume = {23},
    pages = {1--86},
    langid = {german}
}

@article{kanwisherUsingArtificialNeural2023,
    title = {Using Artificial Neural Networks to Ask `Why' Questions of Minds and Brains},
    author = {Kanwisher, Nancy and Khosla, Meenakshi and Dobs, Katharina},
    year = {2023},
    month = mar,
    journal = {Trends in Neurosciences},
    volume = {46},
    number = {3},
    pages = {240--254},
    issn = {01662236},
    doi = {10.1016/j.tins.2022.12.008},
    urldate = {2023-08-28},
    langid = {english},
}

@article{khaligh-razaviDeepSupervisedNot2014,
    title = {Deep {{Supervised}}, but {{Not Unsupervised}}, {{Models May Explain IT Cortical Representation}}},
    author = {{Khaligh-Razavi}, Seyed-Mahdi and Kriegeskorte, Nikolaus},
    editor = {Diedrichsen, J{\"o}rn},
    year = {2014},
    month = nov,
    journal = {PLoS Computational Biology},
    volume = {10},
    number = {11},
    pages = {e1003915},
    issn = {1553-7358},
    doi = {10.1371/journal.pcbi.1003915},
    urldate = {2024-11-25},
    langid = {english},
}

@inproceedings{kingmaAdamMethodStochastic2015,
    title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
    booktitle = {{{ICLR}}},
    author = {Kingma, Diederik P and Ba, Jimmy Lei},
    year = {2015},
    pages = {1--15},
    abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo- ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre- tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con- vergence properties of the algorithm and provide a regret bound on the conver- gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
}

@book{koffkaPrinciplesGestaltPsychology1935,
    title = {Principles {{Of Gestalt Psychology}}},
    author = {Koffka, Kurt},
    year = {1935},
    publisher = {{Harcourt, Brace and Company}},
    address = {New York, NY},
    langid = {english},
}

@article{kriegeskorteRepresentationalSimilarityAnalysis2008,
    title = {Representational Similarity Analysis -- Connecting the Branches of Systems Neuroscience},
    author = {Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter},
    year = {2008},
    journal = {Frontiers in Systems Neuroscience},
    issn = {16625137},
    doi = {10.3389/neuro.06.004.2008},
    urldate = {2022-04-19},
    abstract = {A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities.The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.},
    langid = {english},
}

@article{landauImportanceShapeEarly1988,
    title = {The Importance of Shape in Early Lexical Learning},
    author = {Landau, Barbara and Smith, Linda B. and Jones, Susan S.},
    year = {1988},
    month = jul,
    journal = {Cognitive Development},
    volume = {3},
    number = {3},
    pages = {299--321},
    issn = {08852014},
    doi = {10.1016/0885-2014(88)90014-7},
    urldate = {2024-04-08},
    copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
    langid = {english},
}

@inproceedings{laubInducingMetricViolations2006,
    title = {Inducing {{Metric Violations}} in {{Human Similarity Judgements}}},
    booktitle = {Advances in {{Neural Information Processing Systems}}},
    author = {Laub, Julian and M{\"u}ller, Klaus-Robert and Wichmann, Felix A. and Macke, Jakob H},
    editor = {Sch{\"o}lkopf, B. and Platt, J. and Hoffman, T.},
    year = {2006},
    volume = {19},
    publisher = {MIT Press},
}

@article{lecunDeepLearning2015,
    title = {Deep Learning},
    author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
    year = {2015},
    month = may,
    journal = {Nature},
    volume = {521},
    number = {7553},
    pages = {436--444},
    publisher = {Springer Nature},
    doi = {10.1038/nature14539},
}

@article{leopoldComparativeViewFace2010,
    title = {A Comparative View of Face Perception.},
    author = {Leopold, David A. and Rhodes, Gillian},
    year = {2010},
    journal = {Journal of Comparative Psychology},
    volume = {124},
    number = {3},
    pages = {233--251},
    issn = {1939-2087, 0735-7036},
    doi = {10.1037/a0019460},
    urldate = {2023-07-17},
    langid = {english},
}

@article{liLearningModelFacial2017,
    title = {Learning a Model of Facial Shape and Expression from {{4D}} Scans},
    author = {Li, Tianye and Bolkart, Timo and Black, Michael J. and Li, Hao and Romero, Javier},
    year = {2017},
    month = nov,
    journal = {ACM Transactions on Graphics},
    volume = {36},
    number = {6},
    pages = {1--17},
    issn = {0730-0301, 1557-7368},
    doi = {10.1145/3130800.3130813},
    urldate = {2022-09-15},
    abstract = {The field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression blendshapes. The pose and expression dependent articulations are learned from 4D face sequences in the D3DFACS dataset along with additional 4D sequences. We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).},
    langid = {english},
}

@article{liuDoesStereopsisImprove2020,
    title = {Does Stereopsis Improve Face Identification? {{A}} Study Using a Virtual Reality Display with Integrated Eye-Tracking and Pupillometry},
    shorttitle = {Does Stereopsis Improve Face Identification?},
    author = {Liu, Hang and Laeng, Bruno and Czajkowski, Nikolai Olavi},
    year = {2020},
    month = oct,
    journal = {Acta Psychologica},
    volume = {210},
    pages = {103142},
    issn = {00016918},
    doi = {10.1016/j.actpsy.2020.103142},
    urldate = {2024-07-18},
    langid = {english},
}

@article{loTransformNotTransform2015,
    title = {To Transform or Not to Transform: Using Generalized Linear Mixed Models to Analyse Reaction Time Data},
    shorttitle = {To Transform or Not to Transform},
    author = {Lo, Steson and Andrews, Sally},
    year = {2015},
    month = aug,
    journal = {Frontiers in Psychology},
    volume = {6},
    issn = {1664-1078},
    doi = {10.3389/fpsyg.2015.01171},
    urldate = {2024-07-22},
}

@article{maChicagoFaceDatabase2015,
    title = {The {{Chicago}} Face Database: {{A}} Free Stimulus Set of Faces and Norming Data},
    shorttitle = {The {{Chicago}} Face Database},
    author = {Ma, Debbie S. and Correll, Joshua and Wittenbrink, Bernd},
    year = {2015},
    month = dec,
    journal = {Behavior Research Methods},
    volume = {47},
    number = {4},
    pages = {1122--1135},
    issn = {1554-3528},
    doi = {10.3758/s13428-014-0532-5},
    urldate = {2022-03-04},
    abstract = {Researchers studying a range of psychological phenomena (e.g., theory of mind, emotion, stereotyping and prejudice, interpersonal attraction, etc.) sometimes employ photographs of people as stimuli. In this paper, we introduce the Chicago Face Database, a free resource consisting of 158 high-resolution, standardized photographs of Black and White males and females between the ages of 18 and 40~years and extensive data about these targets. In Study 1, we report pre-testing of these faces, which includes both subjective norming data and objective physical measurements of the images included in the database. In Study 2 we surveyed psychology researchers to assess the suitability of these targets for research purposes and explored factors that were associated with researchers' judgments of suitability. Instructions are outlined for those interested in obtaining access to the stimulus set and accompanying ratings and measures.},
    langid = {english},
}

@article{malpassRecognitionFacesOwn1969,
    title = {Recognition for Faces of Own and Other Race.},
    author = {Malpass, Roy S. and Kravitz, Jerome},
    year = {1969},
    month = dec,
    journal = {Journal of Personality and Social Psychology},
    volume = {13},
    number = {4},
    pages = {330--334},
    issn = {1939-1315, 0022-3514},
    doi = {10.1037/h0028434},
    urldate = {2024-07-08},
    langid = {english}
}

@book{marrVisionComputationalInvestigation1982,
    title = {Vision: {{A Computational Investigation}} into the {{Human Representation}} and {{Processing}} of {{Visual Information}}},
    shorttitle = {Vision},
    author = {Marr, David},
    year = {1982},
    volume = {2},
    publisher = {MIT Press},
    address = {Cambridge, Mass.},
    isbn = {978-0-262-51462-0},
    langid = {english},
}

@misc{muttenthalerVICEVariationalInterpretable2022,
    title = {{{VICE}}: {{Variational Interpretable Concept Embeddings}}},
    shorttitle = {{{VICE}}},
    author = {Muttenthaler, Lukas and Zheng, Charles Y. and McClure, Patrick and Vandermeulen, Robert A. and Hebart, Martin N. and Pereira, Francisco},
    year = {2022},
    month = may,
    number = {arXiv:2205.00756},
    eprint = {2205.00756},
    primaryclass = {cs, stat},
    publisher = {arXiv},
    urldate = {2022-08-12},
    abstract = {A central goal in the cognitive sciences is the development of numerical models for mental representations of object concepts. This paper introduces Variational Interpretable Concept Embeddings (VICE), an approximate Bayesian method for embedding object concepts in a vector space using data collected from humans in an odd-one-out triplet task. VICE uses variational inference to obtain sparse, non-negative representations of object concepts with uncertainty estimates for the embedding values. These estimates are used to automatically select the dimensions that best explain the data. We derive a PAC learning bound for VICE that can be used to estimate generalization performance or determine sufficient sample size in experimental design. VICE rivals or outperforms its predecessor, SPoSE, at predicting human behavior in the odd-one-out triplet task. Furthermore, VICE's object representations are more reproducible and consistent across random initializations.},
    archiveprefix = {arXiv},
    langid = {english},
}

@incollection{olshausenPerceptionInferenceProblem2014,
    title = {Perception as an {{Inference Problem}}},
    booktitle = {The {{Cognitive Neurosciences}}},
    author = {Olshausen, Bruno A.},
    editor = {Gazzaniga, Michael S. and Mangun, George R.},
    year = {2014},
    month = oct,
    edition = {5},
    publisher = {The MIT Press},
    doi = {10.7551/mitpress/9504.003.0037},
    urldate = {2024-08-12},
    isbn = {978-0-262-31936-2},
    langid = {english}
}

@inproceedings{parkhiDeepFaceRecognition2015,
    title = {Deep {{Face Recognition}}},
    booktitle = {Proceedings of the {{British Machine Vision Conference}} 2015},
    author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew},
    year = {2015},
    pages = {41.1-41.12},
    publisher = {British Machine Vision Association},
    address = {Swansea},
    doi = {10.5244/C.29.41},
    urldate = {2021-10-05},
    abstract = {The goal of this paper is face recognition -- from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets.},
    isbn = {978-1-901725-53-7},
    langid = {english},
}

@inproceedings{paysan3DFaceModel2009,
    title = {A {{3D Face Model}} for {{Pose}} and {{Illumination Invariant Face Recognition}}},
    booktitle = {2009 {{Sixth IEEE International Conference}} on {{Advanced Video}} and {{Signal Based Surveillance}}},
    author = {Paysan, Pascal and Knothe, Reinhard and Amberg, Brian and Romdhani, Sami and Vetter, Thomas},
    year = {2009},
    month = sep,
    pages = {296--301},
    publisher = {IEEE},
    address = {Genova, Italy},
    doi = {10.1109/AVSS.2009.58},
    urldate = {2021-10-05},
    abstract = {Generative 3D face models are a powerful tool in computer vision. They provide pose and illumination invariance by modeling the space of 3D faces and the imaging process. The power of these models comes at the cost of an expensive and tedious construction process, which has led the community to focus on more easily constructed but less powerful models. With this paper we publish a generative 3D shape and texture model, the Basel Face Model (BFM), and demonstrate its application to several face recognition task. We improve on previous models by offering higher shape and texture accuracy due to a better scanning device and less correspondence artifacts due to an improved registration algorithm.},
    isbn = {978-1-4244-4755-8},
    langid = {english},
}

@article{peerTurkAlternativePlatforms2017,
    title = {Beyond the {{Turk}}: {{Alternative}} Platforms for Crowdsourcing Behavioral Research},
    shorttitle = {Beyond the {{Turk}}},
    author = {Peer, Eyal and Brandimarte, Laura and Samat, Sonam and Acquisti, Alessandro},
    year = {2017},
    month = may,
    journal = {Journal of Experimental Social Psychology},
    volume = {70},
    pages = {153--163},
    issn = {00221031},
    doi = {10.1016/j.jesp.2017.01.006},
    urldate = {2022-03-11},
    abstract = {The success of Amazon Mechanical Turk (MTurk) as an online research platform has come at a price: MTurk has suffered from slowing rates of population replenishment, and growing participant non-naivety. Recently, a number of alternative platforms have emerged, offering capabilities similar to MTurk but providing access to new and more na{\"i}ve populations. After surveying several options, we empirically examined two such platforms, CrowdFlower (CF) and Prolific Academic (ProA). In two studies, we found that participants on both platforms were more na{\"i}ve and less dishonest compared to MTurk participants. Across the three platforms, CF provided the best response rate, but CF participants failed more attention-check questions and did not reproduce known effects replicated on ProA and MTurk. Moreover, ProA participants produced data quality that was higher than CF's and comparable to MTurk's. ProA and CF participants were also much more diverse than participants from MTurk.},
    langid = {english},
}

@book{rhodesOxfordHandbookFace2011,
    title = {Oxford {{Handbook}} of {{Face Perception}}},
    editor = {Rhodes, Gillian and Calder, Andy and Johnson, Mark and Haxby, James V.},
    year = {2011},
    month = jul,
    edition = {1},
    publisher = {Oxford University Press},
    doi = {10.1093/oxfordhb/9780199559053.001.0001},
    urldate = {2023-07-17},
    isbn = {978-0-19-955905-3},
    langid = {english}
}

@article{roadsEnrichingImageNetHuman2020,
    title = {Enriching {{ImageNet}} with {{Human Similarity Judgments}} and {{Psychological Embeddings}}},
    author = {Roads, Brett D. and Love, Bradley C.},
    year = {2020},
    month = nov,
    journal = {arXiv:2011.11015 [cs]},
    eprint = {2011.11015},
    primaryclass = {cs},
    urldate = {2021-10-22},
    abstract = {Advances in object recognition flourished in part because of the availability of high-quality datasets and associated benchmarks. However, these benchmarks---such as ILSVRC---are relatively task-specific, focusing predominately on predicting class labels. We introduce a publiclyavailable dataset that embodies the task-general capabilities of human perception and reasoning. The Human Similarity Judgments extension to ImageNet (ImageNet-HSJ) is composed of human similarity judgments that supplement the ILSVRC validation set. The new dataset supports a range of task and performance metrics, including the evaluation of unsupervised learning algorithms. We demonstrate two methods of assessment: using the similarity judgments directly and using a psychological embedding trained on the similarity judgments. This embedding space contains an order of magnitude more points (i.e., images) than previous efforts based on human judgments. Scaling to the full 50,000 image set was made possible through a selective sampling process that used variational Bayesian inference and model ensembles to sample aspects of the embedding space that were most uncertain. This methodological innovation not only enables scaling, but should also improve the quality of solutions by focusing sampling where it is needed. To demonstrate the utility of ImageNet-HSJ, we used the similarity ratings and the embedding space to evaluate how well several popular models conform to human similarity judgments. One finding is that more complex models that perform better on task-specific benchmarks do not better conform to human semantic judgments. In addition to the human similarity judgments, pre-trained psychological embeddings and code for inferring variational embeddings are made publicly available. Collectively, ImageNet-HSJ assets support the appraisal of internal representations and the development of more human-like models.},
    archiveprefix = {arXiv},
    langid = {english},
}

@article{roadsObtainingPsychologicalEmbeddings2019,
    title = {Obtaining Psychological Embeddings through Joint Kernel and Metric Learning},
    author = {Roads, Brett D. and Mozer, Michael C.},
    year = {2019},
    month = oct,
    journal = {Behavior Research Methods},
    volume = {51},
    number = {5},
    pages = {2180--2193},
    issn = {1554-3528},
    doi = {10.3758/s13428-019-01285-3},
    urldate = {2021-10-22},
    abstract = {Psychological embeddings provide a powerful formalism for characterizing human-perceived similarity among members of a stimulus set. Obtaining high-quality embeddings can be costly due to algorithm design, software deployment, and participant compensation. This work aims to advance state-of-the-art embedding techniques and provide a comprehensive software package that makes obtaining high-quality psychological embeddings both easy and relatively efficient. Contributions are made on four fronts. First, the embedding procedure allows multiple trial configurations (e.g., triplets) to be used for collecting similarity judgments from participants. For example, trials can be configured to collect triplet comparisons or to sort items into groups. Second, a likelihood model is provided for three classes of similarity kernels allowing users to easily infer the parameters of their preferred model using gradient descent. Third, an active selection algorithm is provided that makes data collection more efficient by proposing comparisons that provide the strongest constraints on the embedding. Fourth, the likelihood model allows the specification of group-specific attention weight parameters. A series of experiments are included to highlight each of these contributions and their impact on converging to a high-quality embedding. Collectively, these incremental improvements provide a powerful and complete set of tools for inferring psychological embeddings. The relevant tools are available as the Python package PsiZ, which can be cloned from GitHub (https://github.com/roads/psiz).},
    langid = {english},
}

@article{roelfsemaSolvingBindingProblem2023,
    title = {Solving the Binding Problem: {{Assemblies}} Form When Neurons Enhance Their Firing Rate---They Don't Need to Oscillate or Synchronize},
    shorttitle = {Solving the Binding Problem},
    author = {Roelfsema, Pieter R.},
    year = {2023},
    month = apr,
    journal = {Neuron},
    volume = {111},
    number = {7},
    pages = {1003--1019},
    issn = {08966273},
    doi = {10.1016/j.neuron.2023.03.016},
    urldate = {2023-06-11},
    abstract = {When we look at an image, its features are represented in our visual system in a highly distributed manner, calling for a mechanism that binds them into coherent object representations. There have been different proposals for the neuronal mechanisms that can mediate binding. One hypothesis is that binding is achieved by oscillations that synchronize neurons representing features of the same perceptual object. This view allows separate communication channels between different brain areas. Another hypothesis is that binding of features that are represented in different brain regions occurs when the neurons in these areas that respond to the same object simultaneously enhance their firing rate, which would correspond to directing object-based attention to these features. This review summarizes evidence in favor of and against these two hypotheses, examining the neuronal correlates of binding and assessing the time course of perceptual grouping. I conclude that enhanced neuronal firing rates bind features into coherent object representations, whereas oscillations and synchrony are unrelated to binding.},
    langid = {english},
}

@article{rustSelectivityToleranceInvariance2010,
    title = {Selectivity and {{Tolerance}} (``{{Invariance}}'') {{Both Increase}} as {{Visual Information Propagates}} from {{Cortical Area V4}} to {{IT}}},
    author = {Rust, Nicole C. and DiCarlo, James J.},
    year = {2010},
    month = sep,
    journal = {The Journal of Neuroscience},
    volume = {30},
    number = {39},
    pages = {12978--12995},
    issn = {0270-6474, 1529-2401},
    doi = {10.1523/JNEUROSCI.0179-10.2010},
    urldate = {2024-09-10},
    abstract = {Our ability to recognize objects despite large changes in position, size, and context is achieved through computations that are thought to increase both the shape selectivity and the tolerance (``invariance'') of the visual representation at successive stages of the ventral pathway [visual cortical areas V1, V2, and V4 and inferior temporal cortex (IT)]. However, these ideas have proven difficult to test. Here, we consider how well population activity patterns at two stages of the ventral stream (V4 and IT) discriminate between, and generalize across, different images. We found that both V4 and IT encode natural images with similar fidelity, whereas the IT population is much more sensitive to controlled, statistical scrambling of those images. Scrambling sensitivity was proportional to receptive field (RF) size in both V4 and IT, suggesting that, on average, the number of visual feature conjunctions implemented by a V4 or IT neuron is directly related to its RF size. We also found that the IT population could better discriminate between objects across changes in position, scale, and context, thus directly demonstrating a V4-to-IT gain in tolerance. This tolerance gain could be accounted for by both a decrease in single-unit sensitivity to identity-preserving transformations (e.g., an increase in RF size) and an increase in the maintenance of rank-order object selectivity within the RF. These results demonstrate that, as visual information travels from V4 to IT, the population representation is reformatted to become more selective for feature conjunctions and more tolerant to identity preserving transformations, and they reveal the single-unit response properties that underlie that reformatting.},
    copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
    langid = {english},
}

@article{schmidhuberDeepLearningNeural2015,
    title = {Deep Learning in Neural Networks: {{An}} Overview},
    author = {Schmidhuber, J{\"u}rgen},
    year = {2015},
    journal = {Neural Networks},
    volume = {61},
    pages = {85--117},
    publisher = {Elsevier Ltd},
    issn = {0893-6080},
    doi = {10.1016/j.neunet.2014.09.003},
    abstract = {In recent years, deep artificial neural networks (including recurrent ones) havewonnumerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
}

@article{shepardAnalysisProximitiesMultidimensional1962,
    title = {The Analysis of Proximities: {{Multidimensional}} Scaling with an Unknown Distance Function. {{I}}.},
    shorttitle = {The Analysis of Proximities},
    author = {Shepard, Roger N.},
    year = {1962},
    month = jun,
    journal = {Psychometrika},
    volume = {27},
    number = {2},
    pages = {125--140},
    issn = {0033-3123, 1860-0980},
    doi = {10.1007/BF02289630},
    urldate = {2024-08-05},
    copyright = {http://www.springer.com/tdm},
    langid = {english},
}

@article{sigalaVisualCategorizationShapes2002,
    title = {Visual Categorization Shapes Feature Selectivity in the Primate Temporal Cortex},
    author = {Sigala, Natasha and Logothetis, Nikos K.},
    year = {2002},
    month = jan,
    journal = {Nature},
    volume = {415},
    number = {6869},
    pages = {318--320},
    issn = {0028-0836, 1476-4687},
    doi = {10.1038/415318a},
    urldate = {2024-04-08},
    copyright = {http://www.springer.com/tdm},
    langid = {english},
}

@article{stringerLearningTransformInvariant2008,
    title = {Learning Transform Invariant Object Recognition in the Visual System with Multiple Stimuli Present during Training},
    author = {Stringer, S.M. and Rolls, E.T.},
    year = {2008},
    month = sep,
    journal = {Neural Networks},
    volume = {21},
    number = {7},
    pages = {888--903},
    issn = {08936080},
    doi = {10.1016/j.neunet.2007.11.004},
    urldate = {2023-07-17},
    langid = {english}
}

@article{tverskySimilaritySeparabilityTriangle1982,
    title = {Similarity, {{Separability}}, and the {{Triangle Inequality}}},
    author = {Tversky, Amos and Gati, Itamar},
    year = {1982},
    langid = {english},
}

@incollection{ugailDeepFaceRecognition2022,
    title = {Deep Face Recognition Using Full and Partial Face Images},
    booktitle = {Advanced {{Methods}} and {{Deep Learning}} in {{Computer Vision}}},
    author = {Ugail, Hassan},
    year = {2022},
    pages = {221--241},
    publisher = {Elsevier},
    doi = {10.1016/B978-0-12-822109-9.00015-1},
    urldate = {2024-07-24},
    copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
    isbn = {978-0-12-822109-9},
    langid = {english}
}

@article{vandyckModelingBiologicalFace2023,
    title = {Modeling Biological Face Recognition with Deep Convolutional Neural Networks},
    author = {{van Dyck}, Leonard E. and Gruber, Walter R.},
    year = {2023},
    month = aug,
    journal = {Journal of Cognitive Neuroscience},
    eprint = {2208.06681},
    primaryclass = {cs},
    pages = {1--17},
    issn = {0898-929X, 1530-8898},
    doi = {10.1162/jocn_a_02040},
    urldate = {2023-08-28},
    abstract = {Deep convolutional neural networks (DCNNs) have become the state-of-the-art computational models of biological object recognition. Their remarkable success has helped vision science break new ground and recent efforts have started to transfer this achievement to research on biological face recognition. In this regard, face detection can be investigated by comparing face-selective biological neurons and brain areas to artificial neurons and model layers. Similarly, face identification can be examined by comparing in vivo and in silico multidimensional "face spaces". In this review, we summarize the first studies that use DCNNs to model biological face recognition. On the basis of a broad spectrum of behavioral and computational evidence, we conclude that DCNNs are useful models that closely resemble the general hierarchical organization of face recognition in the ventral visual pathway and the core face network. In two exemplary spotlights, we emphasize the unique scientific contributions of these models. First, studies on face detection in DCNNs indicate that elementary face selectivity emerges automatically through feedforward processing even in the absence of visual experience. Second, studies on face identification in DCNNs suggest that identity-specific experience and generative mechanisms facilitate this particular challenge. Taken together, as this novel modeling approach enables close control of predisposition (i.e., architecture) and experience (i.e., training data), it may be suited to inform long-standing debates on the substrates of biological face recognition.},
    archiveprefix = {arXiv},
}

@inproceedings{wahSimilarityComparisonsInteractive2014,
    title = {Similarity {{Comparisons}} for {{Interactive Fine-Grained Categorization}}},
    booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
    author = {Wah, Catherine and Van Horn, Grant and Branson, Steve and Maji, Subhransu and Perona, Pietro and Belongie, Serge},
    year = {2014},
    month = jun,
    pages = {859--866},
    publisher = {IEEE},
    address = {Columbus, OH},
    doi = {10.1109/CVPR.2014.115},
    urldate = {2022-02-01},
    abstract = {Current human-in-the-loop fine-grained visual categorization systems depend on a predefined vocabulary of attributes and parts, usually determined by experts. In this work, we move away from that expert-driven and attributecentric paradigm and present a novel interactive classification system that incorporates computer vision and perceptual similarity metrics in a unified framework. At test time, users are asked to judge relative similarity between a query image and various sets of images; these general queries do not require expert-defined terminology and are applicable to other domains and basic-level categories, enabling a flexible, efficient, and scalable system for finegrained categorization with humans in the loop. Our system outperforms existing state-of-the-art systems for relevance feedback-based image retrieval as well as interactive classification, resulting in a reduction of up to 43\% in the average number of questions needed to correctly classify an image.},
    isbn = {978-1-4799-5118-5},
    langid = {english},
}

@article{welchmanHumanBrainDepth2016,
    title = {The {{Human Brain}} in {{Depth}}: {{How We See}} in {{3D}}},
    shorttitle = {The {{Human Brain}} in {{Depth}}},
    author = {Welchman, Andrew E.},
    year = {2016},
    month = oct,
    journal = {Annual Review of Vision Science},
    volume = {2},
    number = {1},
    pages = {345--376},
    issn = {2374-4642, 2374-4650},
    doi = {10.1146/annurev-vision-111815-114605},
    urldate = {2024-05-08},
    abstract = {Human perception is remarkably flexible: We experience vivid threedimensional (3D) structure under diverse conditions, from the seemingly random magic-eye stereograms to the aesthetically beautiful, but obviously flat, canvases of the Old Masters. How does the brain achieve this apparently effortless robustness? Using brain imaging we are beginning to discover how different parts of the visual cortex support 3D perception by tracing different computations in the dorsal and ventral pathways. This review concentrates on studies of binocular disparity and its combination with other depth cues. This work suggests that the dorsal visual cortex is strongly engaged by 3D information and is involved in integrating signals to represent the structure of viewed surfaces. The ventral cortex may store representations of object configurations and the features required for task performance. These differences can be broadly understood in terms of the different computational demands of reducing estimator variance versus increasing the separation between exemplars.},
    langid = {english},
}

@article{wertheimerUntersuchungenZurLehre1922,
    title = {{Untersuchungen zur Lehre von der Gestalt}},
    author = {Wertheimer, Max},
    year = {1922},
    journal = {Gestalt Theory},
    volume = {39},
    number = {1},
    pages = {79--89},
    issn = {2519-5808},
    doi = {10.1515/gth-2017-0007},
    urldate = {2024-07-18},
    copyright = {http://creativecommons.org/licenses/by-nc-nd/3.0},
    langid = {ngerman},
}

@article{wheatstoneContributionsPhysiologyVision1838,
    title = {Contributions to the Physiology of Vision. ---{{Part}} the First. {{On}} Some Remarkable, and Hitherto Unobserved, Phenomena of Binocular Vision},
    author = {Wheatstone, Charles},
    year = {1838},
    journal = {Philosophical Transactions of the Royal Society of London},
    volume = {128},
    pages = {371--394},
    issn = {0261-0523, 2053-9223},
    doi = {10.1098/rstl.1838.0019},
    urldate = {2022-03-10},
    abstract = {When an object is viewed at so great a distance that the optic axes of both eyes are sensibly parallel when directed towards it, the perspective projections of it, seen by each eye separately, are similar, and the appearance to the two eyes is precisely the same as when the object is seen by one eye only. There is, in such case, no difference between the visual appearance of an object in relief and its perspective projection on a plane surface; and hence pictorial representations of distant objects, when those circumstances which would prevent or disturb the illusion are carefully excluded, may be rendered such perfect resemblances of the objects they are intended to represent as to be mistaken for them; the Diorama is an instance of this. But this similarity no longer exists when the object is placed so near the eyes that to view it the optic axes must converge; under these conditions a different perspective projection of it is seen by each eye, and these perspectives are more dissimilar as the convergence of the optic axes becomes greater. This fact may be easily verified by placing any figure of three dimensions, an outline cube for instance, at a moderate distance before the eyes, and while the head is kept perfectly steady, viewing it with each eye successively while the other is closed. Plate XI. fig. 13. represents the two perspective projections of a cube; b is that seen by the right eye, and a that presented to the left eye; the figure being supposed to be placed about seven inches immediately before the spectator. The appearances, which are by this simple experiment rendered so obvious, may be easily inferred from the established laws of perspective; for the same object in relief is, when viewed by a different eye, seen from two points of sight at a distance from each other equal to the line joining the two eyes. Yet they seem to have escaped the attention of every philosopher and artist who has treated of the subjects of vision and perspective. I can ascribe this inattention to a phenomenon leading to the important and curious consequences, which will form the subject of the present communication, only to this circumstance; that the results being contrary to a principle which was very generally maintained by optical writers, viz. that objects can be seen single only when their images fall on corresponding points of the two retin{\ae}, an hypothesis which will be hereafter discussed, if the consideration ever arose in their minds, it was hastily discarded under the conviction, that if the pictures presented to the two eyes are under certain circumstances dissimilar, their differences must be so small that they need not be taken into account.},
    langid = {english}
}

@article{xuUnderstandingTransformationTolerant2022,
    title = {Understanding Transformation Tolerant Visual Object Representations in the Human Brain and Convolutional Neural Networks},
    author = {Xu, Yaoda and {Vaziri-Pashkam}, Maryam},
    year = {2022},
    month = nov,
    journal = {NeuroImage},
    volume = {263},
    pages = {119635},
    issn = {10538119},
    doi = {10.1016/j.neuroimage.2022.119635},
    urldate = {2024-09-10},
    langid = {english},
}

@article{yaminsPerformanceoptimizedHierarchicalModels2014,
    title = {Performance-Optimized Hierarchical Models Predict Neural Responses in Higher Visual Cortex},
    author = {Yamins, Daniel L. K. and Hong, Ha and Cadieu, Charles F. and Solomon, Ethan A. and Seibert, Darren and DiCarlo, James J.},
    year = {2014},
    month = jun,
    journal = {Proceedings of the National Academy of Sciences},
    volume = {111},
    number = {23},
    pages = {8619--8624},
    issn = {0027-8424, 1091-6490},
    doi = {10.1073/pnas.1403112111},
    urldate = {2023-08-28},
    abstract = {Significance Humans and monkeys easily recognize objects in scenes. This ability is known to be supported by a network of hierarchically interconnected brain areas. However, understanding neurons in higher levels of this hierarchy has long remained a major challenge in visual systems neuroscience. We use computational techniques to identify a neural network model that matches human performance on challenging object categorization tasks. Although not explicitly constrained to match neural data, this model turns out to be highly predictive of neural responses in both the V4 and inferior temporal cortex, the top two layers of the ventral visual hierarchy. In addition to yielding greatly improved models of visual cortex, these results suggest that a process of biological performance optimization directly shaped neural mechanisms. , The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model's categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model's intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization---applied in a biologically appropriate model class---can be used to build quantitative predictive models of neural processing.},
    langid = {english},
}

@article{yildirimEfficientInverseGraphics2020,
    title = {Efficient Inverse Graphics in Biological Face Processing},
    author = {Yildirim, Ilker and Belledonne, Mario and Freiwald, Winrich and Tenenbaum, Josh},
    year = {2020},
    month = mar,
    journal = {Science Advances},
    volume = {6},
    number = {10},
    pages = {eaax5979},
    issn = {2375-2548},
    doi = {10.1126/sciadv.aax5979},
    urldate = {2022-08-17},
    abstract = {Neural networks in the primate brain may invert a graphics style model of how 3D object shapes and textures cause observed images. , Vision not only detects and recognizes objects, but performs rich inferences about the underlying scene structure that causes the patterns of light we see. Inverting generative models, or ``analysis-by-synthesis'', presents a possible solution, but its mechanistic implementations have typically been too slow for online perception, and their mapping to neural circuits remains unclear. Here we present a neurally plausible efficient inverse graphics model and test it in the domain of face recognition. The model is based on a deep neural network that learns to invert a three-dimensional face graphics program in a single fast feedforward pass. It explains human behavior qualitatively and quantitatively, including the classic ``hollow face'' illusion, and it maps directly onto a specialized face-processing circuit in the primate brain. The model fits both behavioral and neural data better than state-of-the-art computer vision models, and suggests an interpretable reverse-engineering account of how the brain transforms images into percepts.},
    langid = {english},
}

@article{yilmazPerceptualInvariancePsychophysical1967,
    title = {Perceptual Invariance and the Psychophysical Law},
    author = {Yilmaz, H{\"u}seyin},
    year = {1967},
    month = nov,
    journal = {Perception \& Psychophysics},
    volume = {2},
    number = {11},
    pages = {533--538},
    issn = {0031-5117, 1532-5962},
    doi = {10.3758/BF03210261},
    urldate = {2023-07-17},
    langid = {english},
}

@inproceedings{zeilerVisualizingUnderstandingConvolutional2014,
    title = {Visualizing and {{Understanding Convolutional Networks}}},
    booktitle = {Proc. of {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
    author = {Zeiler, Matthew D and Fergus, Rob},
    year = {2014},
    pages = {818--833},
    publisher = {Springer},
}
